{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1= pd.read_csv(r\"C:\\Users\\i\\Downloads\\joy-ratings-0to1.train.txt\", delimiter='\\t', header=None)\n",
    "df1.columns = ['Id', 'tweet', 'emotion', 'score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Id                                              tweet emotion  score\n",
      "0    30000  Just got back from seeing @GaryDelaney in Burs...     joy  0.980\n",
      "1    30001  Oh dear an evening of absolute hilarity I don'...     joy  0.958\n",
      "2    30002  Been waiting all week for this game ‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è #ch...     joy  0.940\n",
      "3    30003  @gardiner_love : Thank you so much, Gloria! Yo...     joy  0.938\n",
      "4    30004  I feel so blessed to work with the family that...     joy  0.938\n",
      "..     ...                                                ...     ...    ...\n",
      "818  30818  It's just the lack of company and liveliness o...     joy  0.058\n",
      "819  30819             Quinn's short hair makes me sad. #glee     joy  0.040\n",
      "820  30820  hate overthinking e v e r y t h i n g like i j...     joy  0.040\n",
      "821  30821  People who cheer for sports teams completely o...     joy  0.020\n",
      "822  30822  @DamnPatriot You're a POS for rejoicing in som...     joy  0.019\n",
      "\n",
      "[823 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import emoji\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", tweet)\n",
    "    \n",
    "    return tweet\n",
    "df1['tweet'] = df1['tweet'].apply(preprocess_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_mentions(tweet):\n",
    "    return re.sub(r'@\\w+', '', tweet)\n",
    "\n",
    "df1['tweet'] = df1['tweet'].apply(remove_mentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s#@]', '', text) \n",
    "    text = re.sub(r'\\d+', '', text)  \n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "df1['tweet'] = df1['tweet'].apply(clean_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>emotion</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30000</td>\n",
       "      <td>just got back from seeing in burslem amazing f...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30001</td>\n",
       "      <td>oh dear an evening of absolute hilarity i dont...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30002</td>\n",
       "      <td>been waiting all week for this game #cheer #fr...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30003</td>\n",
       "      <td>thank you so much gloria youre so sweet and th...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30004</td>\n",
       "      <td>i feel so blessed to work with the family that...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818</th>\n",
       "      <td>30818</td>\n",
       "      <td>its just the lack of company and liveliness ou...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>30819</td>\n",
       "      <td>quinns short hair makes me sad #glee</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>30820</td>\n",
       "      <td>hate overthinking e v e r y t h i n g like i j...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>30821</td>\n",
       "      <td>people who cheer for sports teams completely o...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>30822</td>\n",
       "      <td>youre a pos for rejoicing in someones death</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>823 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id                                              tweet emotion  score\n",
       "0    30000  just got back from seeing in burslem amazing f...     joy  0.980\n",
       "1    30001  oh dear an evening of absolute hilarity i dont...     joy  0.958\n",
       "2    30002  been waiting all week for this game #cheer #fr...     joy  0.940\n",
       "3    30003  thank you so much gloria youre so sweet and th...     joy  0.938\n",
       "4    30004  i feel so blessed to work with the family that...     joy  0.938\n",
       "..     ...                                                ...     ...    ...\n",
       "818  30818  its just the lack of company and liveliness ou...     joy  0.058\n",
       "819  30819               quinns short hair makes me sad #glee     joy  0.040\n",
       "820  30820  hate overthinking e v e r y t h i n g like i j...     joy  0.040\n",
       "821  30821  people who cheer for sports teams completely o...     joy  0.020\n",
       "822  30822        youre a pos for rejoicing in someones death     joy  0.019\n",
       "\n",
       "[823 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_emoji(tweet):\n",
    "    text = emoji.demojize(tweet)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['tweet'] = df1['tweet'].apply(convert_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_tweets(tweet):\n",
    "    tokenizer = TweetTokenizer()\n",
    "    return tokenizer.tokenize(tweet)\n",
    "\n",
    "df1['tweet'] = df1['tweet'].apply(tokenize_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>emotion</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30000</td>\n",
       "      <td>[just, got, back, from, seeing, in, burslem, a...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30001</td>\n",
       "      <td>[oh, dear, an, evening, of, absolute, hilarity...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30002</td>\n",
       "      <td>[been, waiting, all, week, for, this, game, #c...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30003</td>\n",
       "      <td>[thank, you, so, much, gloria, youre, so, swee...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30004</td>\n",
       "      <td>[i, feel, so, blessed, to, work, with, the, fa...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818</th>\n",
       "      <td>30818</td>\n",
       "      <td>[its, just, the, lack, of, company, and, livel...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>30819</td>\n",
       "      <td>[quinns, short, hair, makes, me, sad, #glee]</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>30820</td>\n",
       "      <td>[hate, overthinking, e, v, e, r, y, t, h, i, n...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>30821</td>\n",
       "      <td>[people, who, cheer, for, sports, teams, compl...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>30822</td>\n",
       "      <td>[youre, a, pos, for, rejoicing, in, someones, ...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>823 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id                                              tweet emotion  score\n",
       "0    30000  [just, got, back, from, seeing, in, burslem, a...     joy  0.980\n",
       "1    30001  [oh, dear, an, evening, of, absolute, hilarity...     joy  0.958\n",
       "2    30002  [been, waiting, all, week, for, this, game, #c...     joy  0.940\n",
       "3    30003  [thank, you, so, much, gloria, youre, so, swee...     joy  0.938\n",
       "4    30004  [i, feel, so, blessed, to, work, with, the, fa...     joy  0.938\n",
       "..     ...                                                ...     ...    ...\n",
       "818  30818  [its, just, the, lack, of, company, and, livel...     joy  0.058\n",
       "819  30819       [quinns, short, hair, makes, me, sad, #glee]     joy  0.040\n",
       "820  30820  [hate, overthinking, e, v, e, r, y, t, h, i, n...     joy  0.040\n",
       "821  30821  [people, who, cheer, for, sports, teams, compl...     joy  0.020\n",
       "822  30822  [youre, a, pos, for, rejoicing, in, someones, ...     joy  0.019\n",
       "\n",
       "[823 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\n",
    "    'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from', 'has', 'he', 'in', 'is', 'it', 'its',\n",
    "    'of', 'on', 'that', 'the', 'to', 'was', 'were', 'will', 'with', 'i', 'you', 'your', 'so', 'all',\n",
    "    'about', 'above', 'after', 'again', 'against', 'ain', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\",\n",
    "    'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can',\n",
    "    'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\",\n",
    "    'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\",\n",
    "    'have', 'haven', \"haven't\", 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how',\n",
    "    'i', 'if', 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
    "    'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no',\n",
    "    'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves',\n",
    "    'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she's\", 'should', \"should've\", 'shouldn',\n",
    "    \"shouldn't\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them',\n",
    "    'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until',\n",
    "    'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', 'were', 'weren', \"weren't\", 'what', 'when', 'where',\n",
    "    'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you',\n",
    "    \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n",
    "def remove_stopwords(tokens):\n",
    "    filtered_tokens = [token for token in tokens if len(token) > 1 and token.lower() not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "df1['tweet'] = df1['tweet'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>emotion</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30000</td>\n",
       "      <td>[got, back, seeing, burslem, amazing, face, st...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30001</td>\n",
       "      <td>[oh, dear, evening, absolute, hilarity, dont, ...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30002</td>\n",
       "      <td>[waiting, week, game, #cheer, #friday]</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30003</td>\n",
       "      <td>[thank, much, gloria, youre, sweet, thoughtful...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30004</td>\n",
       "      <td>[feel, blessed, work, family, nanny, nothing, ...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818</th>\n",
       "      <td>30818</td>\n",
       "      <td>[lack, company, liveliness, makes, bored]</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>30819</td>\n",
       "      <td>[quinns, short, hair, makes, sad, #glee]</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>30820</td>\n",
       "      <td>[hate, overthinking, like, jus, wanna, happy, ...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>30821</td>\n",
       "      <td>[people, cheer, sports, teams, completely, out...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>30822</td>\n",
       "      <td>[youre, pos, rejoicing, someones, death]</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>823 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id                                              tweet emotion  score\n",
       "0    30000  [got, back, seeing, burslem, amazing, face, st...     joy  0.980\n",
       "1    30001  [oh, dear, evening, absolute, hilarity, dont, ...     joy  0.958\n",
       "2    30002             [waiting, week, game, #cheer, #friday]     joy  0.940\n",
       "3    30003  [thank, much, gloria, youre, sweet, thoughtful...     joy  0.938\n",
       "4    30004  [feel, blessed, work, family, nanny, nothing, ...     joy  0.938\n",
       "..     ...                                                ...     ...    ...\n",
       "818  30818          [lack, company, liveliness, makes, bored]     joy  0.058\n",
       "819  30819           [quinns, short, hair, makes, sad, #glee]     joy  0.040\n",
       "820  30820  [hate, overthinking, like, jus, wanna, happy, ...     joy  0.040\n",
       "821  30821  [people, cheer, sports, teams, completely, out...     joy  0.020\n",
       "822  30822           [youre, pos, rejoicing, someones, death]     joy  0.019\n",
       "\n",
       "[823 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "def extract_features(tweet):\n",
    "    tokenized_text = ' '.join(tweet)\n",
    "    input_ids = torch.tensor(tokenizer.encode(tokenized_text, add_special_tokens=True)).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    features = last_hidden_states.squeeze(0).numpy()\n",
    "    \n",
    "    return features\n",
    "\n",
    "df1['features'] = df1['tweet'].apply(extract_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>emotion</th>\n",
       "      <th>score</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>30217</td>\n",
       "      <td>[yeah, playing, well, #optimism]</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.625</td>\n",
       "      <td>[[-0.09087066, 0.20415401, 0.38680488, 0.03166...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>30769</td>\n",
       "      <td>[would, like, know, source, presidents, optimi...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.180</td>\n",
       "      <td>[[-0.10396489, -0.044955045, -0.051590685, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>30371</td>\n",
       "      <td>[man, know, juggle, didnt, balls, #funny, #pun...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.519</td>\n",
       "      <td>[[-0.10058107, 0.23081657, -0.1354249, -0.0073...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711</th>\n",
       "      <td>30711</td>\n",
       "      <td>[god, facebooks, design, started, remind, mysp...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.250</td>\n",
       "      <td>[[0.13354683, -0.04225903, -0.053082053, -0.25...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>30414</td>\n",
       "      <td>[hes, brilliant, lost, joyous, plot, us, year,...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.480</td>\n",
       "      <td>[[-0.4616325, -0.026620476, 0.21807232, 0.0884...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>30812</td>\n",
       "      <td>[quinns, short, hair, makes, sad]</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.083</td>\n",
       "      <td>[[-0.011215197, 0.08818388, -0.063707456, 0.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>30340</td>\n",
       "      <td>[watched, django, unchained, people, may, frow...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.536</td>\n",
       "      <td>[[0.15952902, 0.13285297, 0.2045169, 0.0801301...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>30449</td>\n",
       "      <td>[watch, amazing, lively, broadcast, #musically]</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.462</td>\n",
       "      <td>[[-0.027367242, -0.017335942, 0.23272076, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>30594</td>\n",
       "      <td>[yell, bye, garbage, cheerfully]</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.360</td>\n",
       "      <td>[[0.1024229, 0.5135636, -0.030556034, -0.06082...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>30506</td>\n",
       "      <td>[twitter, font, endless, hilarity]</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.420</td>\n",
       "      <td>[[-0.29139048, -0.34074506, 0.07822999, -0.054...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>823 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id                                              tweet emotion  score  \\\n",
       "217  30217                   [yeah, playing, well, #optimism]     joy  0.625   \n",
       "769  30769  [would, like, know, source, presidents, optimi...     joy  0.180   \n",
       "371  30371  [man, know, juggle, didnt, balls, #funny, #pun...     joy  0.519   \n",
       "711  30711  [god, facebooks, design, started, remind, mysp...     joy  0.250   \n",
       "414  30414  [hes, brilliant, lost, joyous, plot, us, year,...     joy  0.480   \n",
       "..     ...                                                ...     ...    ...   \n",
       "812  30812                  [quinns, short, hair, makes, sad]     joy  0.083   \n",
       "340  30340  [watched, django, unchained, people, may, frow...     joy  0.536   \n",
       "449  30449    [watch, amazing, lively, broadcast, #musically]     joy  0.462   \n",
       "594  30594                   [yell, bye, garbage, cheerfully]     joy  0.360   \n",
       "506  30506                 [twitter, font, endless, hilarity]     joy  0.420   \n",
       "\n",
       "                                              features  \n",
       "217  [[-0.09087066, 0.20415401, 0.38680488, 0.03166...  \n",
       "769  [[-0.10396489, -0.044955045, -0.051590685, 0.0...  \n",
       "371  [[-0.10058107, 0.23081657, -0.1354249, -0.0073...  \n",
       "711  [[0.13354683, -0.04225903, -0.053082053, -0.25...  \n",
       "414  [[-0.4616325, -0.026620476, 0.21807232, 0.0884...  \n",
       "..                                                 ...  \n",
       "812  [[-0.011215197, 0.08818388, -0.063707456, 0.01...  \n",
       "340  [[0.15952902, 0.13285297, 0.2045169, 0.0801301...  \n",
       "449  [[-0.027367242, -0.017335942, 0.23272076, -0.0...  \n",
       "594  [[0.1024229, 0.5135636, -0.030556034, -0.06082...  \n",
       "506  [[-0.29139048, -0.34074506, 0.07822999, -0.054...  \n",
       "\n",
       "[823 rows x 5 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "df1 = shuffle(df1)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 768)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.features[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>emotion</th>\n",
       "      <th>score</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>30217</td>\n",
       "      <td>[yeah, playing, well, #optimism]</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.625</td>\n",
       "      <td>[[-0.09087066, 0.20415401, 0.38680488, 0.03166...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>30769</td>\n",
       "      <td>[would, like, know, source, presidents, optimi...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.180</td>\n",
       "      <td>[[-0.10396489, -0.044955045, -0.051590685, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>30371</td>\n",
       "      <td>[man, know, juggle, didnt, balls, #funny, #pun...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.519</td>\n",
       "      <td>[[-0.10058107, 0.23081657, -0.1354249, -0.0073...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711</th>\n",
       "      <td>30711</td>\n",
       "      <td>[god, facebooks, design, started, remind, mysp...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.250</td>\n",
       "      <td>[[0.13354683, -0.04225903, -0.053082053, -0.25...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>30414</td>\n",
       "      <td>[hes, brilliant, lost, joyous, plot, us, year,...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.480</td>\n",
       "      <td>[[-0.4616325, -0.026620476, 0.21807232, 0.0884...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770</th>\n",
       "      <td>30770</td>\n",
       "      <td>[unixware, skydrivecokie, hearty, exactment, b...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.180</td>\n",
       "      <td>[[-0.24502039, 0.23167601, -0.18355255, -0.050...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>30232</td>\n",
       "      <td>[god, replaced, sadness, laughter, cant, go, w...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.615</td>\n",
       "      <td>[[-0.14044082, 0.38514432, 0.18663892, -0.0345...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>30779</td>\n",
       "      <td>[apparently, money, world, cant, buy, single, ...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.160</td>\n",
       "      <td>[[-0.19860527, 0.02035996, -0.018056665, 0.059...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>30527</td>\n",
       "      <td>[intermit, animatedncaa, pittsburgh, vs, missi...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.406</td>\n",
       "      <td>[[-0.64525056, -0.26255015, -0.029385332, -0.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>30223</td>\n",
       "      <td>[man, know, juggle, didnt, balls, itn, #funny,...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.625</td>\n",
       "      <td>[[-0.004961823, 0.425498, -0.117140874, 0.0105...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id                                              tweet emotion  score  \\\n",
       "217  30217                   [yeah, playing, well, #optimism]     joy  0.625   \n",
       "769  30769  [would, like, know, source, presidents, optimi...     joy  0.180   \n",
       "371  30371  [man, know, juggle, didnt, balls, #funny, #pun...     joy  0.519   \n",
       "711  30711  [god, facebooks, design, started, remind, mysp...     joy  0.250   \n",
       "414  30414  [hes, brilliant, lost, joyous, plot, us, year,...     joy  0.480   \n",
       "770  30770  [unixware, skydrivecokie, hearty, exactment, b...     joy  0.180   \n",
       "232  30232  [god, replaced, sadness, laughter, cant, go, w...     joy  0.615   \n",
       "779  30779  [apparently, money, world, cant, buy, single, ...     joy  0.160   \n",
       "527  30527  [intermit, animatedncaa, pittsburgh, vs, missi...     joy  0.406   \n",
       "223  30223  [man, know, juggle, didnt, balls, itn, #funny,...     joy  0.625   \n",
       "\n",
       "                                              features  \n",
       "217  [[-0.09087066, 0.20415401, 0.38680488, 0.03166...  \n",
       "769  [[-0.10396489, -0.044955045, -0.051590685, 0.0...  \n",
       "371  [[-0.10058107, 0.23081657, -0.1354249, -0.0073...  \n",
       "711  [[0.13354683, -0.04225903, -0.053082053, -0.25...  \n",
       "414  [[-0.4616325, -0.026620476, 0.21807232, 0.0884...  \n",
       "770  [[-0.24502039, 0.23167601, -0.18355255, -0.050...  \n",
       "232  [[-0.14044082, 0.38514432, 0.18663892, -0.0345...  \n",
       "779  [[-0.19860527, 0.02035996, -0.018056665, 0.059...  \n",
       "527  [[-0.64525056, -0.26255015, -0.029385332, -0.2...  \n",
       "223  [[-0.004961823, 0.425498, -0.117140874, 0.0105...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "features = df1['features'].tolist()\n",
    "padded_features = pad_sequences(features, padding='post')\n",
    "padded_df = df1.copy()\n",
    "padded_df['features'] = padded_features.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.stack(padded_df['features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (823, 41, 768)\n",
      "Output shape: (823,)\n"
     ]
    }
   ],
   "source": [
    "y = np.array(padded_df['score'])    \n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.reshape(x, (823, 41 * 768))  \n",
    "y = np.reshape(y, (823,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.625, 0.18 , 0.519, 0.25 , 0.48 , 0.18 , 0.615, 0.16 , 0.406,\n",
       "       0.625, 0.438, 0.417, 0.562, 0.24 , 0.483, 0.407, 0.104, 0.75 ,\n",
       "       0.958, 0.48 , 0.179, 0.38 , 0.66 , 0.229, 0.458, 0.333, 0.3  ,\n",
       "       0.34 , 0.173, 0.28 , 0.417, 0.312, 0.479, 0.74 , 0.04 , 0.292,\n",
       "       0.44 , 0.3  , 0.864, 0.395, 0.815, 0.46 , 0.667, 0.708, 0.438,\n",
       "       0.5  , 0.6  , 0.396, 0.36 , 0.312, 0.562, 0.438, 0.708, 0.48 ,\n",
       "       0.688, 0.64 , 0.542, 0.729, 0.288, 0.32 , 0.4  , 0.237, 0.602,\n",
       "       0.458, 0.06 , 0.327, 0.479, 0.75 , 0.56 , 0.353, 0.46 , 0.56 ,\n",
       "       0.312, 0.833, 0.646, 0.481, 0.462, 0.578, 0.558, 0.312, 0.833,\n",
       "       0.86 , 0.48 , 0.245, 0.667, 0.708, 0.625, 0.562, 0.24 , 0.68 ,\n",
       "       0.36 , 0.94 , 0.84 , 0.1  , 0.417, 0.542, 0.68 , 0.812, 0.6  ,\n",
       "       0.68 , 0.34 , 0.562, 0.5  , 0.771, 0.52 , 0.229, 0.28 , 0.729,\n",
       "       0.167, 0.46 , 0.611, 0.72 , 0.646, 0.66 , 0.78 , 0.312, 0.18 ,\n",
       "       0.521, 0.926, 0.292, 0.562, 0.78 , 0.521, 0.25 , 0.14 , 0.558,\n",
       "       0.48 , 0.625, 0.558, 0.28 , 0.88 , 0.846, 0.375, 0.729, 0.646,\n",
       "       0.5  , 0.673, 0.688, 0.5  , 0.537, 0.75 , 0.792, 0.84 , 0.4  ,\n",
       "       0.459, 0.542, 0.312, 0.24 , 0.438, 0.72 , 0.262, 0.125, 0.646,\n",
       "       0.6  , 0.68 , 0.5  , 0.271, 0.812, 0.28 , 0.54 , 0.52 , 0.312,\n",
       "       0.708, 0.75 , 0.4  , 0.02 , 0.25 , 0.154, 0.812, 0.139, 0.646,\n",
       "       0.542, 0.42 , 0.375, 0.708, 0.84 , 0.64 , 0.383, 0.84 , 0.292,\n",
       "       0.16 , 0.646, 0.189, 0.533, 0.482, 0.58 , 0.688, 0.58 , 0.625,\n",
       "       0.396, 0.688, 0.36 , 0.583, 0.308, 0.72 , 0.76 , 0.231, 0.521,\n",
       "       0.44 , 0.146, 0.204, 0.7  , 0.479, 0.54 , 0.231, 0.56 , 0.312,\n",
       "       0.833, 0.654, 0.494, 0.654, 0.875, 0.827, 0.577, 0.235, 0.38 ,\n",
       "       0.34 , 0.84 , 0.625, 0.854, 0.4  , 0.604, 0.5  , 0.771, 0.28 ,\n",
       "       0.458, 0.479, 0.593, 0.46 , 0.56 , 0.304, 0.44 , 0.292, 0.868,\n",
       "       0.521, 0.544, 0.542, 0.208, 0.32 , 0.44 , 0.558, 0.212, 0.479,\n",
       "       0.938, 0.5  , 0.333, 0.646, 0.48 , 0.596, 0.417, 0.646, 0.542,\n",
       "       0.604, 0.827, 0.521, 0.792, 0.583, 0.438, 0.562, 0.462, 0.58 ,\n",
       "       0.854, 0.854, 0.396, 0.938, 0.1  , 0.462, 0.396, 0.38 , 0.82 ,\n",
       "       0.562, 0.2  , 0.229, 0.146, 0.292, 0.562, 0.375, 0.212, 0.058,\n",
       "       0.442, 0.917, 0.292, 0.104, 0.5  , 0.354, 0.625, 0.479, 0.333,\n",
       "       0.22 , 0.604, 0.385, 0.521, 0.82 , 0.38 , 0.417, 0.354, 0.792,\n",
       "       0.345, 0.349, 0.32 , 0.729, 0.146, 0.404, 0.354, 0.14 , 0.667,\n",
       "       0.408, 0.125, 0.625, 0.667, 0.417, 0.583, 0.52 , 0.479, 0.646,\n",
       "       0.417, 0.75 , 0.667, 0.44 , 0.521, 0.407, 0.375, 0.567, 0.449,\n",
       "       0.46 , 0.279, 0.125, 0.417, 0.52 , 0.5  , 0.6  , 0.188, 0.5  ,\n",
       "       0.771, 0.833, 0.5  , 0.812, 0.354, 0.438, 0.521, 0.5  , 0.854,\n",
       "       0.62 , 0.333, 0.86 , 0.613, 0.32 , 0.48 , 0.604, 0.333, 0.434,\n",
       "       0.25 , 0.25 , 0.812, 0.5  , 0.52 , 0.56 , 0.562, 0.48 , 0.354,\n",
       "       0.438, 0.36 , 0.521, 0.417, 0.167, 0.771, 0.292, 0.76 , 0.354,\n",
       "       0.694, 0.452, 0.444, 0.271, 0.25 , 0.731, 0.5  , 0.404, 0.292,\n",
       "       0.161, 0.352, 0.354, 0.62 , 0.66 , 0.462, 0.2  , 0.188, 0.269,\n",
       "       0.72 , 0.534, 0.274, 0.708, 0.42 , 0.56 , 0.417, 0.82 , 0.54 ,\n",
       "       0.42 , 0.36 , 0.396, 0.26 , 0.521, 0.792, 0.769, 0.396, 0.227,\n",
       "       0.327, 0.417, 0.479, 0.417, 0.56 , 0.646, 0.354, 0.74 , 0.729,\n",
       "       0.872, 0.294, 0.36 , 0.729, 0.66 , 0.74 , 0.44 , 0.52 , 0.667,\n",
       "       0.212, 0.48 , 0.396, 0.5  , 0.096, 0.6  , 0.5  , 0.32 , 0.708,\n",
       "       0.833, 0.625, 0.62 , 0.519, 0.56 , 0.208, 0.5  , 0.44 , 0.396,\n",
       "       0.375, 0.34 , 0.481, 0.583, 0.34 , 0.917, 0.384, 0.771, 0.917,\n",
       "       0.863, 0.32 , 0.263, 0.521, 0.667, 0.879, 0.36 , 0.458, 0.271,\n",
       "       0.68 , 0.5  , 0.54 , 0.48 , 0.5  , 0.562, 0.66 , 0.688, 0.48 ,\n",
       "       0.34 , 0.333, 0.656, 0.64 , 0.396, 0.25 , 0.208, 0.433, 0.04 ,\n",
       "       0.44 , 0.68 , 0.479, 0.144, 0.52 , 0.135, 0.66 , 0.729, 0.22 ,\n",
       "       0.877, 0.36 , 0.34 , 0.458, 0.32 , 0.38 , 0.771, 0.48 , 0.788,\n",
       "       0.34 , 0.771, 0.48 , 0.229, 0.52 , 0.292, 0.562, 0.479, 0.521,\n",
       "       0.4  , 0.875, 0.854, 0.25 , 0.32 , 0.625, 0.346, 0.924, 0.346,\n",
       "       0.5  , 0.604, 0.875, 0.25 , 0.646, 0.104, 0.458, 0.12 , 0.16 ,\n",
       "       0.562, 0.4  , 0.729, 0.327, 0.62 , 0.922, 0.292, 0.688, 0.792,\n",
       "       0.208, 0.479, 0.64 , 0.5  , 0.417, 0.583, 0.562, 0.18 , 0.58 ,\n",
       "       0.729, 0.36 , 0.479, 0.458, 0.34 , 0.5  , 0.25 , 0.521, 0.48 ,\n",
       "       0.4  , 0.654, 0.655, 0.667, 0.74 , 0.34 , 0.188, 0.12 , 0.231,\n",
       "       0.708, 0.583, 0.792, 0.615, 0.604, 0.34 , 0.292, 0.667, 0.26 ,\n",
       "       0.704, 0.083, 0.271, 0.849, 0.76 , 0.383, 0.583, 0.271, 0.38 ,\n",
       "       0.473, 0.385, 0.22 , 0.646, 0.6  , 0.7  , 0.458, 0.667, 0.58 ,\n",
       "       0.56 , 0.271, 0.519, 0.52 , 0.562, 0.604, 0.1  , 0.36 , 0.396,\n",
       "       0.42 , 0.36 , 0.76 , 0.48 , 0.542, 0.32 , 0.311, 0.146, 0.796,\n",
       "       0.583, 0.521, 0.229, 0.917, 0.44 , 0.56 , 0.4  , 0.208, 0.312,\n",
       "       0.646, 0.5  , 0.771, 0.192, 0.771, 0.146, 0.521, 0.32 , 0.657,\n",
       "       0.458, 0.375, 0.1  , 0.9  , 0.833, 0.396, 0.229, 0.536, 0.438,\n",
       "       0.521, 0.583, 0.708, 0.854, 0.76 , 0.98 , 0.78 , 0.48 , 0.396,\n",
       "       0.333, 0.288, 0.896, 0.4  , 0.51 , 0.375, 0.812, 0.729, 0.75 ,\n",
       "       0.656, 0.16 , 0.875, 0.76 , 0.62 , 0.5  , 0.56 , 0.327, 0.86 ,\n",
       "       0.56 , 0.438, 0.5  , 0.812, 0.396, 0.48 , 0.538, 0.225, 0.375,\n",
       "       0.792, 0.6  , 0.729, 0.375, 0.5  , 0.62 , 0.646, 0.917, 0.2  ,\n",
       "       0.745, 0.104, 0.56 , 0.34 , 0.019, 0.365, 0.58 , 0.462, 0.32 ,\n",
       "       0.583, 0.625, 0.542, 0.259, 0.42 , 0.396, 0.404, 0.188, 0.808,\n",
       "       0.667, 0.36 , 0.509, 0.25 , 0.38 , 0.396, 0.667, 0.769, 0.5  ,\n",
       "       0.231, 0.646, 0.704, 0.396, 0.854, 0.458, 0.271, 0.646, 0.062,\n",
       "       0.167, 0.42 , 0.075, 0.68 , 0.792, 0.458, 0.55 , 0.34 , 0.48 ,\n",
       "       0.654, 0.468, 0.72 , 0.333, 0.271, 0.74 , 0.827, 0.24 , 0.519,\n",
       "       0.146, 0.417, 0.76 , 0.44 , 0.438, 0.58 , 0.604, 0.458, 0.625,\n",
       "       0.483, 0.792, 0.479, 0.812, 0.062, 0.56 , 0.417, 0.56 , 0.26 ,\n",
       "       0.562, 0.92 , 0.66 , 0.54 , 0.66 , 0.48 , 0.229, 0.48 , 0.547,\n",
       "       0.375, 0.438, 0.5  , 0.167, 0.519, 0.333, 0.58 , 0.542, 0.312,\n",
       "       0.62 , 0.365, 0.7  , 0.208, 0.44 , 0.604, 0.583, 0.34 , 0.48 ,\n",
       "       0.207, 0.896, 0.46 , 0.32 , 0.375, 0.6  , 0.48 , 0.28 , 0.68 ,\n",
       "       0.271, 0.32 , 0.25 , 0.4  , 0.562, 0.771, 0.75 , 0.577, 0.352,\n",
       "       0.74 , 0.562, 0.56 , 0.3  , 0.688, 0.6  , 0.558, 0.708, 0.229,\n",
       "       0.104, 0.148, 0.131, 0.208, 0.808, 0.375, 0.375, 0.208, 0.604,\n",
       "       0.64 , 0.327, 0.522, 0.438, 0.607, 0.292, 0.604, 0.208, 0.083,\n",
       "       0.536, 0.462, 0.36 , 0.42 ])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=x.copy()  \n",
    "Y=y.copy()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=1500)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "alpha = 2000\n",
    "model = Ridge(alpha=alpha)\n",
    "model.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.02252365870889028\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X)\n",
    "mse = mean_squared_error(Y, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.1218327461509717\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mae = mean_absolute_error(Y, y_pred)\n",
    "\n",
    "print(\"Mean Absolute Error:\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['regression_model.pkl']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "save_path = 'regression_model.pkl'\n",
    "joblib.dump(model, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2= pd.read_csv(r\"C:\\Users\\i\\Downloads\\joy-ratings-0to1.dev.gold.txt\", delimiter='\\t', header=None)\n",
    "df2.columns = ['Id', 'tweet', 'emotion', 'score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>emotion</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30823</td>\n",
       "      <td>@theclobra lol I thought maybe, couldn't decid...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30824</td>\n",
       "      <td>Nawaz Sharif is getting more funnier than @kap...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30825</td>\n",
       "      <td>Nawaz Sharif is getting more funnier than @kap...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30826</td>\n",
       "      <td>@tomderivan73 üòÅ...I'll just people watch and e...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30827</td>\n",
       "      <td>I love my family so much #lucky #grateful #sma...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>30897</td>\n",
       "      <td>It feels good to get outside for a minute and ...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>30898</td>\n",
       "      <td>@r0Ils ppl get triggered over u smiling they'r...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>30899</td>\n",
       "      <td>@GigaFag @pipertownsend_ snapchat new would be...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>30900</td>\n",
       "      <td>@GigaFag @pipertownsend_ snapchat new would be...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>30901</td>\n",
       "      <td>A hearty Jonza! to all my friends and follower.</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.704</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>79 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id                                              tweet emotion  score\n",
       "0   30823  @theclobra lol I thought maybe, couldn't decid...     joy  0.312\n",
       "1   30824  Nawaz Sharif is getting more funnier than @kap...     joy  0.700\n",
       "2   30825  Nawaz Sharif is getting more funnier than @kap...     joy  0.580\n",
       "3   30826  @tomderivan73 üòÅ...I'll just people watch and e...     joy  0.438\n",
       "4   30827  I love my family so much #lucky #grateful #sma...     joy  0.936\n",
       "..    ...                                                ...     ...    ...\n",
       "74  30897  It feels good to get outside for a minute and ...     joy  0.580\n",
       "75  30898  @r0Ils ppl get triggered over u smiling they'r...     joy  0.170\n",
       "76  30899  @GigaFag @pipertownsend_ snapchat new would be...     joy  0.396\n",
       "77  30900  @GigaFag @pipertownsend_ snapchat new would be...     joy  0.156\n",
       "78  30901    A hearty Jonza! to all my friends and follower.     joy  0.704\n",
       "\n",
       "[79 rows x 4 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", tweet)\n",
    "    \n",
    "    return tweet\n",
    "df2['tweet'] = df2['tweet'].apply(preprocess_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_mentions(tweet):\n",
    "    return re.sub(r'@\\w+', '', tweet)\n",
    "\n",
    "df2['tweet'] = df2['tweet'].apply(remove_mentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s#@]', '', text)  \n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "df2['tweet'] = df2['tweet'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_emoji(tweet):\n",
    "    text = emoji.demojize(tweet)\n",
    "    return text\n",
    "df2['tweet'] = df2['tweet'].apply(convert_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_tweets(tweet):\n",
    "    tokenizer = TweetTokenizer()\n",
    "    return tokenizer.tokenize(tweet)\n",
    "\n",
    "df2['tweet'] = df2['tweet'].apply(tokenize_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\n",
    "    'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from', 'has', 'he', 'in', 'is', 'it', 'its',\n",
    "    'of', 'on', 'that', 'the', 'to', 'was', 'were', 'will', 'with', 'i', 'you', 'your', 'so', 'all',\n",
    "    'about', 'above', 'after', 'again', 'against', 'ain', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\",\n",
    "    'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can',\n",
    "    'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\",\n",
    "    'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\",\n",
    "    'have', 'haven', \"haven't\", 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how',\n",
    "    'i', 'if', 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
    "    'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no',\n",
    "    'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves',\n",
    "    'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she's\", 'should', \"should've\", 'shouldn',\n",
    "    \"shouldn't\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them',\n",
    "    'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until',\n",
    "    'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', 'were', 'weren', \"weren't\", 'what', 'when', 'where',\n",
    "    'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you',\n",
    "    \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n",
    "\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    filtered_tokens = [token for token in tokens if len(token) > 1 and token.lower() not in stop_words]\n",
    "    return filtered_tokens\n",
    "df2['tweet'] = df2['tweet'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "def extract_features(tweet):\n",
    "    \n",
    "    tokenized_text = ' '.join(tweet)\n",
    "    input_ids = torch.tensor(tokenizer.encode(tokenized_text, add_special_tokens=True)).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    features = last_hidden_states.squeeze(0).numpy()\n",
    "    \n",
    "    return features\n",
    "df2['features'] = df2['tweet'].apply(extract_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "features2 = df2['features'].tolist()\n",
    "padded_features2 = pad_sequences(features2, padding='post')\n",
    "padded_df2 = df2.copy()\n",
    "padded_df2['features'] = padded_features2.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input feature shape: (79, 30, 768)\n"
     ]
    }
   ],
   "source": [
    "X = np.stack(padded_df2['features'])\n",
    "print('Input feature shape:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input feature shape: (79, 30, 768)\n"
     ]
    }
   ],
   "source": [
    "print('Input feature shape:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "max_sequence_length=41\n",
    "X_=pad_sequences(X,maxlen=max_sequence_length,padding=\"post\",truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = np.reshape(X_, (79, 41 * 768))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_path = 'regression_model.pkl'\n",
    "loaded_model = joblib.load(saved_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.51067087, 0.56594957, 0.53886009, 0.42587583, 0.67197353,\n",
       "       0.62279419, 0.42493034, 0.38395827, 0.43890411, 0.46397063,\n",
       "       0.44422341, 0.44984654, 0.49561138, 0.51144366, 0.54812439,\n",
       "       0.41797186, 0.5506133 , 0.51982122, 0.50336815, 0.49420508,\n",
       "       0.49448108, 0.50812564, 0.43621751, 0.50508995, 0.46945534,\n",
       "       0.49814222, 0.51606673, 0.43800663, 0.4931089 , 0.5102105 ,\n",
       "       0.56497422, 0.53859552, 0.57704407, 0.56196774, 0.44054848,\n",
       "       0.51594501, 0.38864627, 0.42835729, 0.42044819, 0.5535203 ,\n",
       "       0.46597232, 0.43993218, 0.54864473, 0.72180125, 0.49060816,\n",
       "       0.64108801, 0.47002779, 0.5060718 , 0.50455816, 0.4811201 ,\n",
       "       0.45205043, 0.46731664, 0.44736971, 0.45089417, 0.41492245,\n",
       "       0.40951056, 0.51635703, 0.44873437, 0.36714014, 0.47626151,\n",
       "       0.50994082, 0.44523467, 0.46325495, 0.47538883, 0.46217677,\n",
       "       0.46498087, 0.43780302, 0.44721916, 0.47382236, 0.48220281,\n",
       "       0.51320674, 0.49671024, 0.45515152, 0.43341137, 0.47332716,\n",
       "       0.42342857, 0.52780437, 0.42106089, 0.50412459])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = loaded_model.predict(X_)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['predict']=pd.DataFrame(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_square_error: 0.0359444478046534\n"
     ]
    }
   ],
   "source": [
    "mse=mean_squared_error(df2['score'],df2['predict'])\n",
    "print(\"mean_square_error:\",mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.16298924407690876\n"
     ]
    }
   ],
   "source": [
    "mae = mean_absolute_error(df2['score'],df2['predict'])\n",
    "print(\"Mean Absolute Error:\", mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>emotion</th>\n",
       "      <th>score</th>\n",
       "      <th>features</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30823</td>\n",
       "      <td>[lol, thought, maybe, couldnt, decide, levity]</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.312</td>\n",
       "      <td>[[-0.26824236, 0.33785567, -0.06891699, 0.1938...</td>\n",
       "      <td>0.510671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30824</td>\n",
       "      <td>[nawaz, sharif, getting, funnier, day, day, #l...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.700</td>\n",
       "      <td>[[-0.7382417, -0.14251125, 0.09192327, -0.2908...</td>\n",
       "      <td>0.565950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30825</td>\n",
       "      <td>[nawaz, sharif, getting, funnier, day, day, #c...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.580</td>\n",
       "      <td>[[-0.77927727, -0.30715537, 0.123740554, -0.08...</td>\n",
       "      <td>0.538860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30826</td>\n",
       "      <td>[ill, people, watch, enjoy, rare, show, optimism]</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.438</td>\n",
       "      <td>[[0.06172254, 0.24399748, 0.23058335, 0.041538...</td>\n",
       "      <td>0.425876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30827</td>\n",
       "      <td>[love, family, much, #lucky, #grateful, #smart...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.936</td>\n",
       "      <td>[[-0.115673274, 0.18784252, 0.052415516, 0.005...</td>\n",
       "      <td>0.671974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>30897</td>\n",
       "      <td>[feels, good, get, outside, minute, get, fresh...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.580</td>\n",
       "      <td>[[0.23290278, 0.111647494, 0.13098769, -0.2015...</td>\n",
       "      <td>0.473327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>30898</td>\n",
       "      <td>[ppl, get, triggered, smiling, theyre, irrelev...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.170</td>\n",
       "      <td>[[-0.34056798, 0.09875966, 0.037416108, -0.053...</td>\n",
       "      <td>0.423429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>30899</td>\n",
       "      <td>[snapchat, new, would, beg, differ, #optimism]</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.396</td>\n",
       "      <td>[[-0.0088849235, -0.0068749096, -0.09531322, 0...</td>\n",
       "      <td>0.527804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>30900</td>\n",
       "      <td>[snapchat, new, would, beg, differ]</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.156</td>\n",
       "      <td>[[-0.0039193854, -0.053918257, -0.09985137, 0....</td>\n",
       "      <td>0.421061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>30901</td>\n",
       "      <td>[hearty, jonza, friends, follower]</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.704</td>\n",
       "      <td>[[-0.37420592, -0.028574115, 0.04579575, -0.14...</td>\n",
       "      <td>0.504125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>79 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id                                              tweet emotion  score  \\\n",
       "0   30823     [lol, thought, maybe, couldnt, decide, levity]     joy  0.312   \n",
       "1   30824  [nawaz, sharif, getting, funnier, day, day, #l...     joy  0.700   \n",
       "2   30825  [nawaz, sharif, getting, funnier, day, day, #c...     joy  0.580   \n",
       "3   30826  [ill, people, watch, enjoy, rare, show, optimism]     joy  0.438   \n",
       "4   30827  [love, family, much, #lucky, #grateful, #smart...     joy  0.936   \n",
       "..    ...                                                ...     ...    ...   \n",
       "74  30897  [feels, good, get, outside, minute, get, fresh...     joy  0.580   \n",
       "75  30898  [ppl, get, triggered, smiling, theyre, irrelev...     joy  0.170   \n",
       "76  30899     [snapchat, new, would, beg, differ, #optimism]     joy  0.396   \n",
       "77  30900                [snapchat, new, would, beg, differ]     joy  0.156   \n",
       "78  30901                 [hearty, jonza, friends, follower]     joy  0.704   \n",
       "\n",
       "                                             features   predict  \n",
       "0   [[-0.26824236, 0.33785567, -0.06891699, 0.1938...  0.510671  \n",
       "1   [[-0.7382417, -0.14251125, 0.09192327, -0.2908...  0.565950  \n",
       "2   [[-0.77927727, -0.30715537, 0.123740554, -0.08...  0.538860  \n",
       "3   [[0.06172254, 0.24399748, 0.23058335, 0.041538...  0.425876  \n",
       "4   [[-0.115673274, 0.18784252, 0.052415516, 0.005...  0.671974  \n",
       "..                                                ...       ...  \n",
       "74  [[0.23290278, 0.111647494, 0.13098769, -0.2015...  0.473327  \n",
       "75  [[-0.34056798, 0.09875966, 0.037416108, -0.053...  0.423429  \n",
       "76  [[-0.0088849235, -0.0068749096, -0.09531322, 0...  0.527804  \n",
       "77  [[-0.0039193854, -0.053918257, -0.09985137, 0....  0.421061  \n",
       "78  [[-0.37420592, -0.028574115, 0.04579575, -0.14...  0.504125  \n",
       "\n",
       "[79 rows x 6 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3= pd.read_csv(r\"C:\\Users\\i\\Downloads\\joy-ratings-0to1.test.target.txt\", delimiter='\\t', header=None)\n",
    "df3.columns = ['Id', 'tweet', 'emotion', 'score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", tweet)\n",
    "    \n",
    "    return tweet\n",
    "df3['tweet'] = df3['tweet'].apply(preprocess_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_mentions(tweet):\n",
    "    return re.sub(r'@\\w+', '', tweet)\n",
    "\n",
    "df3['tweet'] = df3['tweet'].apply(remove_mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s#@]', '', text)  \n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "df3['tweet'] = df3['tweet'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_emoji(tweet):\n",
    "    text = emoji.demojize(tweet)\n",
    "    return text\n",
    "df3['tweet'] = df3['tweet'].apply(convert_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_tweets(tweet):\n",
    "    tokenizer = TweetTokenizer()\n",
    "    return tokenizer.tokenize(tweet)\n",
    "\n",
    "df3['tweet'] = df3['tweet'].apply(tokenize_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\n",
    "    'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from', 'has', 'he', 'in', 'is', 'it', 'its',\n",
    "    'of', 'on', 'that', 'the', 'to', 'was', 'were', 'will', 'with', 'i', 'you', 'your', 'so', 'all',\n",
    "    'about', 'above', 'after', 'again', 'against', 'ain', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\",\n",
    "    'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can',\n",
    "    'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\",\n",
    "    'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\",\n",
    "    'have', 'haven', \"haven't\", 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how',\n",
    "    'i', 'if', 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
    "    'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no',\n",
    "    'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves',\n",
    "    'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she's\", 'should', \"should've\", 'shouldn',\n",
    "    \"shouldn't\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them',\n",
    "    'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until',\n",
    "    'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', 'were', 'weren', \"weren't\", 'what', 'when', 'where',\n",
    "    'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you',\n",
    "    \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n",
    "\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    filtered_tokens = [token for token in tokens if len(token) > 1 and token.lower() not in stop_words]\n",
    "    return filtered_tokens\n",
    "df3['tweet'] = df3['tweet'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "def extract_features(tweet):\n",
    "    \n",
    "    tokenized_text = ' '.join(tweet)\n",
    "    input_ids = torch.tensor(tokenizer.encode(tokenized_text, add_special_tokens=True)).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    features = last_hidden_states.squeeze(0).numpy()\n",
    "    \n",
    "    return features\n",
    "df3['features'] = df3['tweet'].apply(extract_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 768)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3['features'][713].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "features3 = df3['features'].tolist()\n",
    "padded_features3 = pad_sequences(features3, padding='post')\n",
    "padded_df3 = df3.copy()\n",
    "padded_df3['features'] = padded_features3.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input feature shape: (714, 50, 768)\n"
     ]
    }
   ],
   "source": [
    "X = np.stack(padded_df3['features'])\n",
    "print('Input feature shape:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "max_sequence_length=41\n",
    "X_=pad_sequences(X,maxlen=max_sequence_length,padding=\"post\",truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = np.reshape(X_, (714, 41 * 768))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_path = 'regression_model.pkl'\n",
    "loaded_model = joblib.load(saved_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.49188611, 0.48464327, 0.51110206, 0.48194501, 0.49575611,\n",
       "       0.49194217, 0.42417449, 0.59917032, 0.60081146, 0.51608042,\n",
       "       0.46363387, 0.46636145, 0.57816595, 0.5052366 , 0.50554963,\n",
       "       0.45725766, 0.4754244 , 0.57698626, 0.52600815, 0.45380398,\n",
       "       0.44849393, 0.53103794, 0.48943426, 0.58125414, 0.48446023,\n",
       "       0.48433892, 0.48679562, 0.45633936, 0.47808832, 0.4382326 ,\n",
       "       0.46888897, 0.465714  , 0.4959723 , 0.49959235, 0.52799674,\n",
       "       0.46330732, 0.42185666, 0.46310266, 0.54037418, 0.44102055,\n",
       "       0.50585491, 0.46329422, 0.44017728, 0.47596729, 0.42232573,\n",
       "       0.51357054, 0.52901234, 0.44842178, 0.49648592, 0.45034227,\n",
       "       0.50658957, 0.57603754, 0.59534921, 0.54607246, 0.54132354,\n",
       "       0.44128895, 0.5774718 , 0.5050549 , 0.46108631, 0.44969707,\n",
       "       0.45743111, 0.63194011, 0.54401419, 0.4504699 , 0.43032981,\n",
       "       0.4900119 , 0.45572501, 0.52789591, 0.51433159, 0.46233172,\n",
       "       0.48925761, 0.45152229, 0.55285052, 0.54628877, 0.41455249,\n",
       "       0.55219056, 0.49950114, 0.46678675, 0.43675643, 0.43328982,\n",
       "       0.44881902, 0.58406534, 0.45239112, 0.43732365, 0.52156509,\n",
       "       0.42467732, 0.47013465, 0.46848938, 0.4271673 , 0.41628156,\n",
       "       0.48689986, 0.52276265, 0.33561666, 0.51666289, 0.44781751,\n",
       "       0.51572197, 0.47829899, 0.37444033, 0.40863125, 0.49788046,\n",
       "       0.45610741, 0.52284898, 0.54411404, 0.48269935, 0.56361067,\n",
       "       0.48293382, 0.39639884, 0.48761738, 0.50793515, 0.51674409,\n",
       "       0.48263883, 0.43543966, 0.50306614, 0.4837585 , 0.55247429,\n",
       "       0.41462807, 0.50975424, 0.4990122 , 0.48481645, 0.41480218,\n",
       "       0.45779235, 0.44927648, 0.4953458 , 0.43991549, 0.4379265 ,\n",
       "       0.4235923 , 0.48421721, 0.48524842, 0.5718874 , 0.46845214,\n",
       "       0.32967336, 0.49979237, 0.43758284, 0.51015154, 0.49008691,\n",
       "       0.46096116, 0.57655228, 0.52020505, 0.51069257, 0.42490486,\n",
       "       0.4903164 , 0.48339584, 0.51707563, 0.47468021, 0.46909088,\n",
       "       0.45927179, 0.50842174, 0.46852132, 0.42943239, 0.49279453,\n",
       "       0.49911751, 0.50810958, 0.55530743, 0.56718289, 0.58163794,\n",
       "       0.38346072, 0.41191008, 0.5123245 , 0.53409039, 0.49525601,\n",
       "       0.45399173, 0.45117237, 0.56148191, 0.41359794, 0.47380438,\n",
       "       0.44009025, 0.47362219, 0.52984984, 0.46589832, 0.4974383 ,\n",
       "       0.47677926, 0.44455778, 0.47498518, 0.44387485, 0.49217591,\n",
       "       0.50568282, 0.54628544, 0.40446607, 0.49734272, 0.51495105,\n",
       "       0.42987239, 0.45537577, 0.42989172, 0.58876573, 0.49664008,\n",
       "       0.52122696, 0.41319336, 0.49176496, 0.51230208, 0.57600598,\n",
       "       0.55720691, 0.49656669, 0.35715904, 0.51678614, 0.47689099,\n",
       "       0.53821707, 0.50595177, 0.53947225, 0.56303911, 0.54064787,\n",
       "       0.54376506, 0.54533504, 0.49326794, 0.44363585, 0.45099269,\n",
       "       0.55286239, 0.55303082, 0.59239055, 0.47517485, 0.53243   ,\n",
       "       0.42776927, 0.52002683, 0.45991709, 0.56043816, 0.58820211,\n",
       "       0.481037  , 0.45815716, 0.44999983, 0.49675083, 0.5243723 ,\n",
       "       0.58150633, 0.56912328, 0.46524312, 0.54045787, 0.49043393,\n",
       "       0.56801445, 0.55954403, 0.56856454, 0.51769416, 0.46507185,\n",
       "       0.61190832, 0.54695125, 0.59432044, 0.58964806, 0.57175346,\n",
       "       0.45421983, 0.54891084, 0.6013065 , 0.54681777, 0.4568148 ,\n",
       "       0.50877477, 0.51231401, 0.52545128, 0.43357277, 0.52287859,\n",
       "       0.50309413, 0.53831904, 0.50036271, 0.4638822 , 0.48104052,\n",
       "       0.48355564, 0.46760957, 0.4859243 , 0.5407984 , 0.50991913,\n",
       "       0.54316203, 0.47609241, 0.47676583, 0.44737266, 0.40714898,\n",
       "       0.44831681, 0.52106628, 0.47090208, 0.47440005, 0.44989284,\n",
       "       0.41885686, 0.56542693, 0.50552717, 0.50529262, 0.60568583,\n",
       "       0.60123856, 0.57526993, 0.50229594, 0.45187335, 0.42499222,\n",
       "       0.47567474, 0.47429534, 0.56086896, 0.44146849, 0.35952852,\n",
       "       0.48449031, 0.45668676, 0.50935861, 0.51269478, 0.46312717,\n",
       "       0.4604571 , 0.50639939, 0.49665445, 0.49042689, 0.55340065,\n",
       "       0.55874139, 0.58958001, 0.57414574, 0.49477357, 0.57167207,\n",
       "       0.44612678, 0.52130322, 0.49486081, 0.47760265, 0.57451718,\n",
       "       0.52789907, 0.46308055, 0.51252537, 0.53910619, 0.39198913,\n",
       "       0.4883045 , 0.42231736, 0.41800562, 0.49423413, 0.47009835,\n",
       "       0.50411007, 0.54912256, 0.48754614, 0.48750421, 0.46436389,\n",
       "       0.43109426, 0.57225844, 0.52698057, 0.50249794, 0.50842975,\n",
       "       0.46367346, 0.4982506 , 0.50784722, 0.50712298, 0.50001422,\n",
       "       0.48433318, 0.50085721, 0.49757383, 0.52863846, 0.539555  ,\n",
       "       0.55082343, 0.59624045, 0.47777356, 0.50299221, 0.53303597,\n",
       "       0.50355509, 0.53168198, 0.39890459, 0.59755461, 0.58231603,\n",
       "       0.46236468, 0.44115674, 0.49201612, 0.50700608, 0.44467099,\n",
       "       0.55708478, 0.40492778, 0.51134792, 0.50587249, 0.47415018,\n",
       "       0.58194813, 0.5290839 , 0.51141685, 0.4490403 , 0.50396943,\n",
       "       0.45870147, 0.43325006, 0.515264  , 0.45520772, 0.52246334,\n",
       "       0.4707356 , 0.4612088 , 0.49659777, 0.4283371 , 0.46061524,\n",
       "       0.40861229, 0.50976627, 0.52884088, 0.46574986, 0.48823185,\n",
       "       0.42877369, 0.47085287, 0.48539168, 0.49840054, 0.55049946,\n",
       "       0.47328464, 0.49307112, 0.51539533, 0.48202467, 0.47901263,\n",
       "       0.42754435, 0.45931363, 0.49106538, 0.54305955, 0.42142499,\n",
       "       0.48136228, 0.48368177, 0.46073457, 0.41337771, 0.46172885,\n",
       "       0.43926694, 0.45850522, 0.46482071, 0.47536289, 0.46904041,\n",
       "       0.44034357, 0.50122159, 0.609241  , 0.44373472, 0.48363352,\n",
       "       0.41412171, 0.44709188, 0.4118559 , 0.36932792, 0.4495275 ,\n",
       "       0.48177672, 0.5562086 , 0.45912939, 0.47858031, 0.4199248 ,\n",
       "       0.44676533, 0.48016082, 0.45979699, 0.38799442, 0.52155955,\n",
       "       0.56798085, 0.56036081, 0.57916147, 0.51498698, 0.45269964,\n",
       "       0.60098497, 0.49717126, 0.41766372, 0.47080897, 0.49510592,\n",
       "       0.4069841 , 0.49753528, 0.47339914, 0.41642271, 0.49077128,\n",
       "       0.44629427, 0.43719549, 0.47625397, 0.59457081, 0.42039493,\n",
       "       0.47866673, 0.41999479, 0.4515252 , 0.42685592, 0.47058671,\n",
       "       0.62887048, 0.4583076 , 0.54905634, 0.56759404, 0.5321306 ,\n",
       "       0.43721564, 0.4413091 , 0.50011542, 0.56152172, 0.44943061,\n",
       "       0.47399461, 0.47160098, 0.51346671, 0.49156803, 0.46534377,\n",
       "       0.47477278, 0.45241824, 0.40673014, 0.49002851, 0.57059385,\n",
       "       0.49989467, 0.45452877, 0.45467447, 0.49731712, 0.50638135,\n",
       "       0.39032798, 0.46186397, 0.5528412 , 0.52734822, 0.52343064,\n",
       "       0.51768946, 0.52197214, 0.48158931, 0.45395613, 0.49007743,\n",
       "       0.45566426, 0.49791068, 0.56493619, 0.54361282, 0.43485584,\n",
       "       0.42540054, 0.50654293, 0.40393572, 0.45708832, 0.69242916,\n",
       "       0.67414731, 0.51340109, 0.46229801, 0.47592311, 0.50638669,\n",
       "       0.4948495 , 0.43329658, 0.49677078, 0.35748246, 0.49835029,\n",
       "       0.47755453, 0.53507457, 0.50917969, 0.42200701, 0.39431711,\n",
       "       0.4338618 , 0.45575983, 0.52666643, 0.46537726, 0.48679462,\n",
       "       0.52228683, 0.47859095, 0.34586075, 0.47853282, 0.4584199 ,\n",
       "       0.41329383, 0.3781261 , 0.45245645, 0.49160712, 0.49012167,\n",
       "       0.44203113, 0.4776755 , 0.45833535, 0.48912128, 0.45088275,\n",
       "       0.46754918, 0.4358294 , 0.49975206, 0.44078017, 0.4909028 ,\n",
       "       0.5787038 , 0.46892082, 0.5212989 , 0.46564673, 0.55275936,\n",
       "       0.5474808 , 0.60957267, 0.38846491, 0.49604189, 0.47697467,\n",
       "       0.47268518, 0.50083498, 0.42592745, 0.52783434, 0.44504442,\n",
       "       0.44169097, 0.4649305 , 0.60866022, 0.65639008, 0.51784485,\n",
       "       0.51784485, 0.57402054, 0.34793783, 0.39021831, 0.49050649,\n",
       "       0.42457077, 0.46325412, 0.47261732, 0.45371023, 0.42269053,\n",
       "       0.58039781, 0.4257579 , 0.48310393, 0.39318057, 0.54361282,\n",
       "       0.49948853, 0.54407468, 0.44067118, 0.46126895, 0.47639689,\n",
       "       0.4290366 , 0.44791881, 0.44019275, 0.45632997, 0.5096661 ,\n",
       "       0.50421862, 0.4374484 , 0.50787215, 0.53608728, 0.55579123,\n",
       "       0.46632861, 0.55098972, 0.55462329, 0.45840748, 0.4627243 ,\n",
       "       0.47384308, 0.55111597, 0.41723932, 0.52726553, 0.56900178,\n",
       "       0.52317669, 0.58576423, 0.64217988, 0.43811363, 0.4647816 ,\n",
       "       0.44090142, 0.51969067, 0.51429509, 0.4723161 , 0.48230646,\n",
       "       0.45393571, 0.50230088, 0.43992834, 0.43808189, 0.47585541,\n",
       "       0.46218128, 0.38158912, 0.50515679, 0.3891808 , 0.6540296 ,\n",
       "       0.50927394, 0.52391871, 0.51584328, 0.46874954, 0.4775913 ,\n",
       "       0.42963365, 0.43896721, 0.45075163, 0.43676962, 0.39933702,\n",
       "       0.52111763, 0.55511124, 0.55674484, 0.47394366, 0.49512336,\n",
       "       0.46376258, 0.50611038, 0.43422299, 0.43660849, 0.45223429,\n",
       "       0.48407198, 0.50580027, 0.4899777 , 0.46076933, 0.41396811,\n",
       "       0.45170435, 0.48429281, 0.49234962, 0.47585525, 0.45144788,\n",
       "       0.53873828, 0.44513513, 0.53139413, 0.55102094, 0.47538649,\n",
       "       0.51899433, 0.50931258, 0.51664747, 0.48472617, 0.41771291,\n",
       "       0.5260922 , 0.34998706, 0.44897339, 0.48823083, 0.43797856,\n",
       "       0.51411286, 0.50339198, 0.50344344, 0.50281362, 0.45289829,\n",
       "       0.51896942, 0.43135847, 0.44884027, 0.46708034, 0.4250228 ,\n",
       "       0.5658382 , 0.45876815, 0.44869849, 0.51561914, 0.46596821,\n",
       "       0.47162766, 0.63490861, 0.64272254, 0.45474128, 0.49839432,\n",
       "       0.51795777, 0.49858686, 0.45695574, 0.44208035, 0.51560019,\n",
       "       0.46153661, 0.57027444, 0.45121044, 0.49266752, 0.50754559,\n",
       "       0.44878442, 0.47170043, 0.48466691, 0.655459  , 0.42229958,\n",
       "       0.45597571, 0.46721896, 0.43228563, 0.56349364, 0.41557181,\n",
       "       0.39377756, 0.45459704, 0.48750104, 0.56131064, 0.40182957,\n",
       "       0.50423732, 0.47078988, 0.38591931, 0.52335951, 0.42433561,\n",
       "       0.48910896, 0.45274163, 0.50391446, 0.48147648, 0.49351649,\n",
       "       0.50224109, 0.56412108, 0.53790651, 0.55476262, 0.52151487,\n",
       "       0.4997021 , 0.44544305, 0.48245551, 0.45194606])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicttest = loaded_model.predict(X_)\n",
    "predicttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['predicttest']=pd.DataFrame(predicttest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=df3.drop(['score','features'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>emotion</th>\n",
       "      <th>predicttest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30902</td>\n",
       "      <td>[must, knowing, #blithe, means, adj, happy, ch...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.491886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30903</td>\n",
       "      <td>[old, saying, #smile, shared, one, gained, ano...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.484643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30904</td>\n",
       "      <td>[bridget, jones, baby, bloody, hilarious, #bri...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.511102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30905</td>\n",
       "      <td>[sparkling, water, makes, life, sparkly]</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.481945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30906</td>\n",
       "      <td>[im, tired, everybody, telling, chill, everyth...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.495756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>31611</td>\n",
       "      <td>[tired, body, mind, sparkling, teeth, say, fol...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.521515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>31612</td>\n",
       "      <td>[refuse, chirp, chirp, girl]</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.499702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711</th>\n",
       "      <td>31613</td>\n",
       "      <td>[hard, stifle, laughter, overheard, comment, r...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.445443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>712</th>\n",
       "      <td>31614</td>\n",
       "      <td>[walking, little, boy, red, shirt, years, age,...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.482456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713</th>\n",
       "      <td>31615</td>\n",
       "      <td>[asked, one, thing, guys, tonight, got, #happy...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.451946</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>714 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id                                              tweet emotion  \\\n",
       "0    30902  [must, knowing, #blithe, means, adj, happy, ch...     joy   \n",
       "1    30903  [old, saying, #smile, shared, one, gained, ano...     joy   \n",
       "2    30904  [bridget, jones, baby, bloody, hilarious, #bri...     joy   \n",
       "3    30905           [sparkling, water, makes, life, sparkly]     joy   \n",
       "4    30906  [im, tired, everybody, telling, chill, everyth...     joy   \n",
       "..     ...                                                ...     ...   \n",
       "709  31611  [tired, body, mind, sparkling, teeth, say, fol...     joy   \n",
       "710  31612                       [refuse, chirp, chirp, girl]     joy   \n",
       "711  31613  [hard, stifle, laughter, overheard, comment, r...     joy   \n",
       "712  31614  [walking, little, boy, red, shirt, years, age,...     joy   \n",
       "713  31615  [asked, one, thing, guys, tonight, got, #happy...     joy   \n",
       "\n",
       "     predicttest  \n",
       "0       0.491886  \n",
       "1       0.484643  \n",
       "2       0.511102  \n",
       "3       0.481945  \n",
       "4       0.495756  \n",
       "..           ...  \n",
       "709     0.521515  \n",
       "710     0.499702  \n",
       "711     0.445443  \n",
       "712     0.482456  \n",
       "713     0.451946  \n",
       "\n",
       "[714 rows x 4 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
