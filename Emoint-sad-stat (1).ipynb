{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1= pd.read_csv(r\"C:\\Users\\i\\Downloads\\sadness-ratings-0to1.train.txt\", delimiter='\\t', header=None)\n",
    "df1.columns = ['Id', 'tweet', 'emotion', 'score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Id                                              tweet  emotion  score\n",
      "0    40000                      Depression sucks! #depression  sadness  0.958\n",
      "1    40001            Feeling worthless as always #depression  sadness  0.958\n",
      "2    40002                       Feeling worthless as always   sadness  0.958\n",
      "3    40003  My #Fibromyalgia has been really bad lately wh...  sadness  0.946\n",
      "4    40004  Im think ima lay in bed all day and sulk. Life...  sadness  0.934\n",
      "..     ...                                                ...      ...    ...\n",
      "781  40781  @VivienLloyd Thank you so much! Just home - st...  sadness  0.104\n",
      "782  40782              Just put the winter duvet on ‚òÉÔ∏è‚ùÑÔ∏èüå¨‚òîÔ∏è   sadness  0.104\n",
      "783  40783  @SilkInSide @TommyJoeRatliff that's so pretty!...  sadness  0.088\n",
      "784  40784  @BluesfestByron second artist announcement loo...  sadness  0.083\n",
      "785  40785  I can literally eat creamy pesto pasta topped ...  sadness  0.083\n",
      "\n",
      "[786 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import emoji\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", tweet)\n",
    "    \n",
    "    return tweet\n",
    "df1['tweet'] = df1['tweet'].apply(preprocess_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_mentions(tweet):\n",
    "    return re.sub(r'@\\w+', '', tweet)\n",
    "\n",
    "df1['tweet'] = df1['tweet'].apply(remove_mentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s#@]', '', text) \n",
    "    text = re.sub(r'\\d+', '', text)  \n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "df1['tweet'] = df1['tweet'].apply(clean_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>emotion</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40000</td>\n",
       "      <td>depression sucks #depression</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40001</td>\n",
       "      <td>feeling worthless as always #depression</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40002</td>\n",
       "      <td>feeling worthless as always</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40003</td>\n",
       "      <td>my #fibromyalgia has been really bad lately wh...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40004</td>\n",
       "      <td>im think ima lay in bed all day and sulk life ...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>40781</td>\n",
       "      <td>thank you so much just home stunned but so hap...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>40782</td>\n",
       "      <td>just put the winter duvet on</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783</th>\n",
       "      <td>40783</td>\n",
       "      <td>thats so pretty i love the sky in the backgrou...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>784</th>\n",
       "      <td>40784</td>\n",
       "      <td>second artist announcement looking good #blues...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785</th>\n",
       "      <td>40785</td>\n",
       "      <td>i can literally eat creamy pesto pasta topped ...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>786 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id                                              tweet  emotion  score\n",
       "0    40000                       depression sucks #depression  sadness  0.958\n",
       "1    40001            feeling worthless as always #depression  sadness  0.958\n",
       "2    40002                        feeling worthless as always  sadness  0.958\n",
       "3    40003  my #fibromyalgia has been really bad lately wh...  sadness  0.946\n",
       "4    40004  im think ima lay in bed all day and sulk life ...  sadness  0.934\n",
       "..     ...                                                ...      ...    ...\n",
       "781  40781  thank you so much just home stunned but so hap...  sadness  0.104\n",
       "782  40782                       just put the winter duvet on  sadness  0.104\n",
       "783  40783  thats so pretty i love the sky in the backgrou...  sadness  0.088\n",
       "784  40784  second artist announcement looking good #blues...  sadness  0.083\n",
       "785  40785  i can literally eat creamy pesto pasta topped ...  sadness  0.083\n",
       "\n",
       "[786 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_emoji(tweet):\n",
    "    text = emoji.demojize(tweet)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['tweet'] = df1['tweet'].apply(convert_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_tweets(tweet):\n",
    "    tokenizer = TweetTokenizer()\n",
    "    return tokenizer.tokenize(tweet)\n",
    "\n",
    "df1['tweet'] = df1['tweet'].apply(tokenize_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>emotion</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40000</td>\n",
       "      <td>[depression, sucks, #depression]</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40001</td>\n",
       "      <td>[feeling, worthless, as, always, #depression]</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40002</td>\n",
       "      <td>[feeling, worthless, as, always]</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40003</td>\n",
       "      <td>[my, #fibromyalgia, has, been, really, bad, la...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40004</td>\n",
       "      <td>[im, think, ima, lay, in, bed, all, day, and, ...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>40781</td>\n",
       "      <td>[thank, you, so, much, just, home, stunned, bu...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>40782</td>\n",
       "      <td>[just, put, the, winter, duvet, on]</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783</th>\n",
       "      <td>40783</td>\n",
       "      <td>[thats, so, pretty, i, love, the, sky, in, the...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>784</th>\n",
       "      <td>40784</td>\n",
       "      <td>[second, artist, announcement, looking, good, ...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785</th>\n",
       "      <td>40785</td>\n",
       "      <td>[i, can, literally, eat, creamy, pesto, pasta,...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>786 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id                                              tweet  emotion  score\n",
       "0    40000                   [depression, sucks, #depression]  sadness  0.958\n",
       "1    40001      [feeling, worthless, as, always, #depression]  sadness  0.958\n",
       "2    40002                   [feeling, worthless, as, always]  sadness  0.958\n",
       "3    40003  [my, #fibromyalgia, has, been, really, bad, la...  sadness  0.946\n",
       "4    40004  [im, think, ima, lay, in, bed, all, day, and, ...  sadness  0.934\n",
       "..     ...                                                ...      ...    ...\n",
       "781  40781  [thank, you, so, much, just, home, stunned, bu...  sadness  0.104\n",
       "782  40782                [just, put, the, winter, duvet, on]  sadness  0.104\n",
       "783  40783  [thats, so, pretty, i, love, the, sky, in, the...  sadness  0.088\n",
       "784  40784  [second, artist, announcement, looking, good, ...  sadness  0.083\n",
       "785  40785  [i, can, literally, eat, creamy, pesto, pasta,...  sadness  0.083\n",
       "\n",
       "[786 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\n",
    "    'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from', 'has', 'he', 'in', 'is', 'it', 'its',\n",
    "    'of', 'on', 'that', 'the', 'to', 'was', 'were', 'will', 'with', 'i', 'you', 'your', 'so', 'all',\n",
    "    'about', 'above', 'after', 'again', 'against', 'ain', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\",\n",
    "    'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can',\n",
    "    'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\",\n",
    "    'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\",\n",
    "    'have', 'haven', \"haven't\", 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how',\n",
    "    'i', 'if', 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
    "    'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no',\n",
    "    'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves',\n",
    "    'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she's\", 'should', \"should've\", 'shouldn',\n",
    "    \"shouldn't\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them',\n",
    "    'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until',\n",
    "    'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', 'were', 'weren', \"weren't\", 'what', 'when', 'where',\n",
    "    'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you',\n",
    "    \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n",
    "def remove_stopwords(tokens):\n",
    "    filtered_tokens = [token for token in tokens if len(token) > 1 and token.lower() not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "df1['tweet'] = df1['tweet'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>emotion</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40000</td>\n",
       "      <td>[depression, sucks, #depression]</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40001</td>\n",
       "      <td>[feeling, worthless, always, #depression]</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40002</td>\n",
       "      <td>[feeling, worthless, always]</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40003</td>\n",
       "      <td>[#fibromyalgia, really, bad, lately, good, men...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40004</td>\n",
       "      <td>[im, think, ima, lay, bed, day, sulk, life, hi...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>40781</td>\n",
       "      <td>[thank, much, home, stunned, happy, dont, thin...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>40782</td>\n",
       "      <td>[put, winter, duvet]</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783</th>\n",
       "      <td>40783</td>\n",
       "      <td>[thats, pretty, love, sky, background, purple,...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>784</th>\n",
       "      <td>40784</td>\n",
       "      <td>[second, artist, announcement, looking, good, ...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785</th>\n",
       "      <td>40785</td>\n",
       "      <td>[literally, eat, creamy, pesto, pasta, topped,...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>786 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id                                              tweet  emotion  score\n",
       "0    40000                   [depression, sucks, #depression]  sadness  0.958\n",
       "1    40001          [feeling, worthless, always, #depression]  sadness  0.958\n",
       "2    40002                       [feeling, worthless, always]  sadness  0.958\n",
       "3    40003  [#fibromyalgia, really, bad, lately, good, men...  sadness  0.946\n",
       "4    40004  [im, think, ima, lay, bed, day, sulk, life, hi...  sadness  0.934\n",
       "..     ...                                                ...      ...    ...\n",
       "781  40781  [thank, much, home, stunned, happy, dont, thin...  sadness  0.104\n",
       "782  40782                               [put, winter, duvet]  sadness  0.104\n",
       "783  40783  [thats, pretty, love, sky, background, purple,...  sadness  0.088\n",
       "784  40784  [second, artist, announcement, looking, good, ...  sadness  0.083\n",
       "785  40785  [literally, eat, creamy, pesto, pasta, topped,...  sadness  0.083\n",
       "\n",
       "[786 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "def extract_features(tweet):\n",
    "    tokenized_text = ' '.join(tweet)\n",
    "    input_ids = torch.tensor(tokenizer.encode(tokenized_text, add_special_tokens=True)).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    features = last_hidden_states.squeeze(0).numpy()\n",
    "    \n",
    "    return features\n",
    "\n",
    "df1['features'] = df1['tweet'].apply(extract_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>emotion</th>\n",
       "      <th>score</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>40250</td>\n",
       "      <td>[blame, manager, watching, players, play, abys...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.604</td>\n",
       "      <td>[[-0.18551016, 0.14533976, 0.095107906, -0.145...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>40275</td>\n",
       "      <td>[dont, let, behavior, others, destroy, ur, inn...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.583</td>\n",
       "      <td>[[-0.3205233, 0.28793436, -0.044567768, -0.051...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>40130</td>\n",
       "      <td>[weeks, full, mondays, end, #disheartened]</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.688</td>\n",
       "      <td>[[0.016893499, 0.10395066, 0.3136695, -0.03307...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>40644</td>\n",
       "      <td>[bows, moment, rather, sulk, im, going, opinio...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.312</td>\n",
       "      <td>[[-0.28913236, 0.28231645, -0.009142242, -0.26...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>40079</td>\n",
       "      <td>[god, full, shilling, seriously, need, major, ...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.750</td>\n",
       "      <td>[[-0.11578786, 0.16990525, 0.34702885, -0.2188...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684</th>\n",
       "      <td>40684</td>\n",
       "      <td>[#pessimist, complains, wind, #optimist, expec...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.271</td>\n",
       "      <td>[[-0.57726145, 0.15732256, -0.009420659, -0.09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>40542</td>\n",
       "      <td>[hard, tell, pic, mistake, either, way, nothin...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.375</td>\n",
       "      <td>[[-0.48719376, -0.17063761, 0.1746943, -0.0378...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>40530</td>\n",
       "      <td>[sulky, pants]</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.396</td>\n",
       "      <td>[[-0.28299236, 0.0910921, -0.27471957, 0.04074...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>40523</td>\n",
       "      <td>[theres, sitcom, better, cant, laugh, sheldon,...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.396</td>\n",
       "      <td>[[-0.41907048, -0.24152792, 0.14949818, -0.070...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>40392</td>\n",
       "      <td>[solution, punish, criminals, thats, way, disc...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.479</td>\n",
       "      <td>[[-0.3647489, -0.27338892, 0.043791763, -0.351...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>786 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id                                              tweet  emotion  score  \\\n",
       "250  40250  [blame, manager, watching, players, play, abys...  sadness  0.604   \n",
       "275  40275  [dont, let, behavior, others, destroy, ur, inn...  sadness  0.583   \n",
       "130  40130         [weeks, full, mondays, end, #disheartened]  sadness  0.688   \n",
       "644  40644  [bows, moment, rather, sulk, im, going, opinio...  sadness  0.312   \n",
       "79   40079  [god, full, shilling, seriously, need, major, ...  sadness  0.750   \n",
       "..     ...                                                ...      ...    ...   \n",
       "684  40684  [#pessimist, complains, wind, #optimist, expec...  sadness  0.271   \n",
       "542  40542  [hard, tell, pic, mistake, either, way, nothin...  sadness  0.375   \n",
       "530  40530                                     [sulky, pants]  sadness  0.396   \n",
       "523  40523  [theres, sitcom, better, cant, laugh, sheldon,...  sadness  0.396   \n",
       "392  40392  [solution, punish, criminals, thats, way, disc...  sadness  0.479   \n",
       "\n",
       "                                              features  \n",
       "250  [[-0.18551016, 0.14533976, 0.095107906, -0.145...  \n",
       "275  [[-0.3205233, 0.28793436, -0.044567768, -0.051...  \n",
       "130  [[0.016893499, 0.10395066, 0.3136695, -0.03307...  \n",
       "644  [[-0.28913236, 0.28231645, -0.009142242, -0.26...  \n",
       "79   [[-0.11578786, 0.16990525, 0.34702885, -0.2188...  \n",
       "..                                                 ...  \n",
       "684  [[-0.57726145, 0.15732256, -0.009420659, -0.09...  \n",
       "542  [[-0.48719376, -0.17063761, 0.1746943, -0.0378...  \n",
       "530  [[-0.28299236, 0.0910921, -0.27471957, 0.04074...  \n",
       "523  [[-0.41907048, -0.24152792, 0.14949818, -0.070...  \n",
       "392  [[-0.3647489, -0.27338892, 0.043791763, -0.351...  \n",
       "\n",
       "[786 rows x 5 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "df1 = shuffle(df1)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 768)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.features[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>emotion</th>\n",
       "      <th>score</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>40250</td>\n",
       "      <td>[blame, manager, watching, players, play, abys...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.604</td>\n",
       "      <td>[[-0.18551016, 0.14533976, 0.095107906, -0.145...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>40275</td>\n",
       "      <td>[dont, let, behavior, others, destroy, ur, inn...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.583</td>\n",
       "      <td>[[-0.3205233, 0.28793436, -0.044567768, -0.051...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>40130</td>\n",
       "      <td>[weeks, full, mondays, end, #disheartened]</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.688</td>\n",
       "      <td>[[0.016893499, 0.10395066, 0.3136695, -0.03307...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>40644</td>\n",
       "      <td>[bows, moment, rather, sulk, im, going, opinio...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.312</td>\n",
       "      <td>[[-0.28913236, 0.28231645, -0.009142242, -0.26...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>40079</td>\n",
       "      <td>[god, full, shilling, seriously, need, major, ...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.750</td>\n",
       "      <td>[[-0.11578786, 0.16990525, 0.34702885, -0.2188...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>40174</td>\n",
       "      <td>[dont, see, difference, courting, appealing, w...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.667</td>\n",
       "      <td>[[-0.70383626, 0.20942569, -0.16147777, -0.057...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>40514</td>\n",
       "      <td>[pessimist, someone, opportunity, knocks, comp...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.396</td>\n",
       "      <td>[[-0.028500024, 0.4877816, -0.030809958, -0.59...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>40529</td>\n",
       "      <td>[episode, today, whilst, editing, power, outag...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.396</td>\n",
       "      <td>[[-0.3621486, -0.09056032, 0.17661268, -0.1874...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>40423</td>\n",
       "      <td>[history, repeating, itselfgaa, culture, dare,...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.458</td>\n",
       "      <td>[[-0.40033904, -0.03608309, 0.027729334, -0.31...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>40592</td>\n",
       "      <td>[rich, fumes, sullen, sences, cheerd]</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.342</td>\n",
       "      <td>[[0.006814859, 0.261758, -0.032302003, -0.0964...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id                                              tweet  emotion  score  \\\n",
       "250  40250  [blame, manager, watching, players, play, abys...  sadness  0.604   \n",
       "275  40275  [dont, let, behavior, others, destroy, ur, inn...  sadness  0.583   \n",
       "130  40130         [weeks, full, mondays, end, #disheartened]  sadness  0.688   \n",
       "644  40644  [bows, moment, rather, sulk, im, going, opinio...  sadness  0.312   \n",
       "79   40079  [god, full, shilling, seriously, need, major, ...  sadness  0.750   \n",
       "174  40174  [dont, see, difference, courting, appealing, w...  sadness  0.667   \n",
       "514  40514  [pessimist, someone, opportunity, knocks, comp...  sadness  0.396   \n",
       "529  40529  [episode, today, whilst, editing, power, outag...  sadness  0.396   \n",
       "423  40423  [history, repeating, itselfgaa, culture, dare,...  sadness  0.458   \n",
       "592  40592              [rich, fumes, sullen, sences, cheerd]  sadness  0.342   \n",
       "\n",
       "                                              features  \n",
       "250  [[-0.18551016, 0.14533976, 0.095107906, -0.145...  \n",
       "275  [[-0.3205233, 0.28793436, -0.044567768, -0.051...  \n",
       "130  [[0.016893499, 0.10395066, 0.3136695, -0.03307...  \n",
       "644  [[-0.28913236, 0.28231645, -0.009142242, -0.26...  \n",
       "79   [[-0.11578786, 0.16990525, 0.34702885, -0.2188...  \n",
       "174  [[-0.70383626, 0.20942569, -0.16147777, -0.057...  \n",
       "514  [[-0.028500024, 0.4877816, -0.030809958, -0.59...  \n",
       "529  [[-0.3621486, -0.09056032, 0.17661268, -0.1874...  \n",
       "423  [[-0.40033904, -0.03608309, 0.027729334, -0.31...  \n",
       "592  [[0.006814859, 0.261758, -0.032302003, -0.0964...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "features = df1['features'].tolist()\n",
    "padded_features = pad_sequences(features, padding='post')\n",
    "padded_df = df1.copy()\n",
    "padded_df['features'] = padded_features.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.stack(padded_df['features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (786, 41, 768)\n",
      "Output shape: (786,)\n"
     ]
    }
   ],
   "source": [
    "y = np.array(padded_df['score'])    \n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.reshape(x, (786, 41* 768))  \n",
    "y = np.reshape(y, (786,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.604, 0.583, 0.688, 0.312, 0.75 , 0.667, 0.396, 0.396, 0.458,\n",
       "       0.342, 0.354, 0.461, 0.458, 0.667, 0.458, 0.812, 0.833, 0.417,\n",
       "       0.188, 0.771, 0.667, 0.458, 0.417, 0.333, 0.438, 0.438, 0.583,\n",
       "       0.333, 0.354, 0.583, 0.125, 0.562, 0.438, 0.708, 0.562, 0.375,\n",
       "       0.479, 0.438, 0.312, 0.604, 0.458, 0.417, 0.278, 0.417, 0.521,\n",
       "       0.46 , 0.214, 0.208, 0.321, 0.667, 0.854, 0.146, 0.458, 0.271,\n",
       "       0.646, 0.438, 0.24 , 0.333, 0.625, 0.542, 0.369, 0.729, 0.688,\n",
       "       0.312, 0.604, 0.25 , 0.375, 0.375, 0.604, 0.646, 0.375, 0.646,\n",
       "       0.375, 0.146, 0.44 , 0.5  , 0.188, 0.613, 0.771, 0.271, 0.375,\n",
       "       0.729, 0.167, 0.396, 0.583, 0.479, 0.25 , 0.354, 0.771, 0.604,\n",
       "       0.833, 0.75 , 0.87 , 0.583, 0.438, 0.479, 0.521, 0.542, 0.417,\n",
       "       0.52 , 0.625, 0.438, 0.812, 0.604, 0.669, 0.792, 0.132, 0.396,\n",
       "       0.396, 0.271, 0.958, 0.625, 0.417, 0.688, 0.458, 0.562, 0.375,\n",
       "       0.541, 0.604, 0.708, 0.271, 0.354, 0.667, 0.271, 0.312, 0.792,\n",
       "       0.688, 0.188, 0.438, 0.562, 0.417, 0.812, 0.934, 0.604, 0.667,\n",
       "       0.479, 0.583, 0.495, 0.604, 0.438, 0.271, 0.833, 0.521, 0.646,\n",
       "       0.729, 0.5  , 0.667, 0.604, 0.661, 0.36 , 0.333, 0.771, 0.896,\n",
       "       0.521, 0.438, 0.625, 0.188, 0.292, 0.25 , 0.229, 0.75 , 0.479,\n",
       "       0.729, 0.896, 0.415, 0.583, 0.562, 0.275, 0.475, 0.375, 0.667,\n",
       "       0.654, 0.396, 0.833, 0.812, 0.688, 0.625, 0.521, 0.375, 0.583,\n",
       "       0.271, 0.75 , 0.458, 0.458, 0.375, 0.206, 0.806, 0.271, 0.188,\n",
       "       0.729, 0.917, 0.445, 0.667, 0.458, 0.42 , 0.312, 0.667, 0.375,\n",
       "       0.208, 0.292, 0.542, 0.542, 0.167, 0.263, 0.688, 0.396, 0.729,\n",
       "       0.604, 0.521, 0.354, 0.088, 0.667, 0.625, 0.417, 0.729, 0.347,\n",
       "       0.542, 0.583, 0.25 , 0.358, 0.271, 0.521, 0.542, 0.333, 0.771,\n",
       "       0.667, 0.146, 0.833, 0.229, 0.604, 0.896, 0.417, 0.812, 0.604,\n",
       "       0.438, 0.729, 0.417, 0.5  , 0.75 , 0.396, 0.729, 0.479, 0.375,\n",
       "       0.417, 0.354, 0.229, 0.312, 0.5  , 0.5  , 0.688, 0.583, 0.583,\n",
       "       0.292, 0.354, 0.325, 0.562, 0.312, 0.396, 0.5  , 0.625, 0.208,\n",
       "       0.729, 0.292, 0.521, 0.333, 0.604, 0.64 , 0.458, 0.375, 0.74 ,\n",
       "       0.375, 0.312, 0.5  , 0.25 , 0.509, 0.417, 0.333, 0.271, 0.375,\n",
       "       0.479, 0.754, 0.542, 0.542, 0.771, 0.354, 0.625, 0.438, 0.208,\n",
       "       0.271, 0.312, 0.208, 0.729, 0.771, 0.646, 0.646, 0.479, 0.458,\n",
       "       0.426, 0.479, 0.417, 0.271, 0.625, 0.477, 0.708, 0.397, 0.729,\n",
       "       0.729, 0.208, 0.25 , 0.812, 0.438, 0.396, 0.208, 0.521, 0.688,\n",
       "       0.354, 0.375, 0.68 , 0.812, 0.521, 0.146, 0.271, 0.479, 0.417,\n",
       "       0.271, 0.542, 0.708, 0.542, 0.254, 0.729, 0.667, 0.604, 0.625,\n",
       "       0.854, 0.455, 0.263, 0.5  , 0.333, 0.271, 0.604, 0.303, 0.375,\n",
       "       0.521, 0.354, 0.604, 0.458, 0.208, 0.625, 0.729, 0.479, 0.5  ,\n",
       "       0.333, 0.604, 0.514, 0.854, 0.562, 0.653, 0.104, 0.354, 0.542,\n",
       "       0.68 , 0.646, 0.814, 0.083, 0.542, 0.583, 0.396, 0.271, 0.438,\n",
       "       0.312, 0.333, 0.188, 0.25 , 0.562, 0.312, 0.312, 0.167, 0.479,\n",
       "       0.2  , 0.354, 0.583, 0.479, 0.375, 0.333, 0.458, 0.229, 0.767,\n",
       "       0.5  , 0.167, 0.708, 0.292, 0.271, 0.438, 0.892, 0.318, 0.312,\n",
       "       0.708, 0.104, 0.766, 0.271, 0.457, 0.667, 0.274, 0.396, 0.5  ,\n",
       "       0.562, 0.792, 0.167, 0.312, 0.42 , 0.667, 0.354, 0.375, 0.729,\n",
       "       0.417, 0.438, 0.604, 0.208, 0.438, 0.396, 0.667, 0.896, 0.542,\n",
       "       0.417, 0.896, 0.708, 0.625, 0.542, 0.542, 0.646, 0.354, 0.5  ,\n",
       "       0.521, 0.396, 0.771, 0.25 , 0.604, 0.312, 0.875, 0.216, 0.667,\n",
       "       0.333, 0.458, 0.354, 0.667, 0.146, 0.583, 0.625, 0.833, 0.355,\n",
       "       0.417, 0.292, 0.646, 0.542, 0.27 , 0.396, 0.271, 0.708, 0.375,\n",
       "       0.25 , 0.625, 0.292, 0.542, 0.604, 0.458, 0.427, 0.292, 0.438,\n",
       "       0.235, 0.583, 0.438, 0.397, 0.354, 0.146, 0.208, 0.542, 0.333,\n",
       "       0.396, 0.229, 0.354, 0.5  , 0.238, 0.888, 0.618, 0.292, 0.521,\n",
       "       0.292, 0.375, 0.229, 0.542, 0.688, 0.562, 0.81 , 0.34 , 0.917,\n",
       "       0.75 , 0.688, 0.396, 0.485, 0.229, 0.604, 0.208, 0.542, 0.375,\n",
       "       0.754, 0.521, 0.708, 0.729, 0.542, 0.625, 0.562, 0.562, 0.729,\n",
       "       0.479, 0.562, 0.479, 0.667, 0.779, 0.354, 0.667, 0.5  , 0.229,\n",
       "       0.688, 0.646, 0.333, 0.542, 0.479, 0.75 , 0.167, 0.455, 0.479,\n",
       "       0.312, 0.306, 0.271, 0.083, 0.521, 0.438, 0.604, 0.708, 0.729,\n",
       "       0.479, 0.312, 0.675, 0.354, 0.188, 0.333, 0.811, 0.667, 0.542,\n",
       "       0.583, 0.5  , 0.396, 0.792, 0.312, 0.292, 0.562, 0.333, 0.571,\n",
       "       0.333, 0.271, 0.271, 0.562, 0.575, 0.25 , 0.312, 0.583, 0.312,\n",
       "       0.229, 0.299, 0.64 , 0.896, 0.396, 0.917, 0.396, 0.417, 0.271,\n",
       "       0.667, 0.812, 0.688, 0.479, 0.958, 0.354, 0.688, 0.333, 0.604,\n",
       "       0.436, 0.583, 0.875, 0.343, 0.548, 0.624, 0.354, 0.208, 0.604,\n",
       "       0.521, 0.438, 0.562, 0.146, 0.271, 0.521, 0.469, 0.311, 0.292,\n",
       "       0.688, 0.188, 0.604, 0.833, 0.542, 0.583, 0.417, 0.32 , 0.354,\n",
       "       0.667, 0.625, 0.75 , 0.521, 0.708, 0.333, 0.312, 0.583, 0.917,\n",
       "       0.167, 0.354, 0.839, 0.636, 0.292, 0.771, 0.66 , 0.583, 0.188,\n",
       "       0.521, 0.542, 0.479, 0.381, 0.396, 0.75 , 0.235, 0.688, 0.229,\n",
       "       0.708, 0.542, 0.771, 0.542, 0.417, 0.875, 0.667, 0.729, 0.688,\n",
       "       0.521, 0.562, 0.521, 0.67 , 0.458, 0.604, 0.771, 0.438, 0.396,\n",
       "       0.322, 0.458, 0.438, 0.604, 0.312, 0.312, 0.946, 0.312, 0.583,\n",
       "       0.688, 0.417, 0.896, 0.792, 0.396, 0.3  , 0.312, 0.958, 0.417,\n",
       "       0.214, 0.25 , 0.771, 0.5  , 0.414, 0.417, 0.604, 0.375, 0.196,\n",
       "       0.438, 0.396, 0.312, 0.458, 0.583, 0.125, 0.689, 0.729, 0.266,\n",
       "       0.351, 0.521, 0.36 , 0.729, 0.458, 0.542, 0.333, 0.333, 0.188,\n",
       "       0.75 , 0.771, 0.604, 0.521, 0.25 , 0.688, 0.833, 0.458, 0.688,\n",
       "       0.646, 0.479, 0.667, 0.438, 0.521, 0.688, 0.625, 0.646, 0.561,\n",
       "       0.854, 0.354, 0.458, 0.125, 0.792, 0.667, 0.75 , 0.333, 0.312,\n",
       "       0.646, 0.521, 0.208, 0.354, 0.542, 0.271, 0.45 , 0.729, 0.3  ,\n",
       "       0.354, 0.574, 0.583, 0.625, 0.396, 0.312, 0.417, 0.417, 0.521,\n",
       "       0.833, 0.583, 0.333, 0.75 , 0.174, 0.604, 0.542, 0.542, 0.861,\n",
       "       0.75 , 0.667, 0.688, 0.333, 0.479, 0.271, 0.875, 0.563, 0.646,\n",
       "       0.562, 0.854, 0.708, 0.404, 0.729, 0.229, 0.438, 0.464, 0.5  ,\n",
       "       0.625, 0.458, 0.354, 0.271, 0.542, 0.833, 0.562, 0.271, 0.375,\n",
       "       0.396, 0.396, 0.479])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=x.copy()  \n",
    "Y=y.copy()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=2000)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "alpha = 2000\n",
    "model = Ridge(alpha=alpha)\n",
    "model.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.02436696906053771\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X)\n",
    "mse = mean_squared_error(Y, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.12935629765480813\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mae = mean_absolute_error(Y, y_pred)\n",
    "\n",
    "print(\"Mean Absolute Error:\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['regression_model.pkl']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "save_path = 'regression_model.pkl'\n",
    "joblib.dump(model, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2= pd.read_csv(r\"C:\\Users\\i\\Downloads\\sadness-ratings-0to1.dev.gold.txt\", delimiter='\\t', header=None)\n",
    "df2.columns = ['Id', 'tweet', 'emotion', 'score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>emotion</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40786</td>\n",
       "      <td>@1johndes ball watching &amp;amp; Rojo'd header wa...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40787</td>\n",
       "      <td>A pessimist is someone who, when opportunity k...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40788</td>\n",
       "      <td>A .500 season is all I'm looking for at this p...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40789</td>\n",
       "      <td>Stars, when you shine,\\nYou know how I feel.\\n...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40790</td>\n",
       "      <td>All I want to do is watch some netflix but I a...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>40855</td>\n",
       "      <td>Common app just randomly logged me out as I wa...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>40856</td>\n",
       "      <td>I'd rather laugh with the rarest genius, in be...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>40857</td>\n",
       "      <td>If you #invest in my new #film I will stop ask...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>40858</td>\n",
       "      <td>Just watched Django Unchained, Other people ma...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>40859</td>\n",
       "      <td>@KeithOlbermann depressing how despicable Trum...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id                                              tweet  emotion  score\n",
       "0   40786  @1johndes ball watching &amp; Rojo'd header wa...  sadness  0.583\n",
       "1   40787  A pessimist is someone who, when opportunity k...  sadness  0.188\n",
       "2   40788  A .500 season is all I'm looking for at this p...  sadness  0.688\n",
       "3   40789  Stars, when you shine,\\nYou know how I feel.\\n...  sadness  0.292\n",
       "4   40790  All I want to do is watch some netflix but I a...  sadness  0.667\n",
       "..    ...                                                ...      ...    ...\n",
       "69  40855  Common app just randomly logged me out as I wa...  sadness  0.833\n",
       "70  40856  I'd rather laugh with the rarest genius, in be...  sadness  0.688\n",
       "71  40857  If you #invest in my new #film I will stop ask...  sadness  0.458\n",
       "72  40858  Just watched Django Unchained, Other people ma...  sadness  0.333\n",
       "73  40859  @KeithOlbermann depressing how despicable Trum...  sadness  0.708\n",
       "\n",
       "[74 rows x 4 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", tweet)\n",
    "    \n",
    "    return tweet\n",
    "df2['tweet'] = df2['tweet'].apply(preprocess_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_mentions(tweet):\n",
    "    return re.sub(r'@\\w+', '', tweet)\n",
    "\n",
    "df2['tweet'] = df2['tweet'].apply(remove_mentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s#@]', '', text)  \n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "df2['tweet'] = df2['tweet'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_emoji(tweet):\n",
    "    text = emoji.demojize(tweet)\n",
    "    return text\n",
    "df2['tweet'] = df2['tweet'].apply(convert_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_tweets(tweet):\n",
    "    tokenizer = TweetTokenizer()\n",
    "    return tokenizer.tokenize(tweet)\n",
    "\n",
    "df2['tweet'] = df2['tweet'].apply(tokenize_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\n",
    "    'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from', 'has', 'he', 'in', 'is', 'it', 'its',\n",
    "    'of', 'on', 'that', 'the', 'to', 'was', 'were', 'will', 'with', 'i', 'you', 'your', 'so', 'all',\n",
    "    'about', 'above', 'after', 'again', 'against', 'ain', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\",\n",
    "    'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can',\n",
    "    'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\",\n",
    "    'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\",\n",
    "    'have', 'haven', \"haven't\", 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how',\n",
    "    'i', 'if', 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
    "    'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no',\n",
    "    'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves',\n",
    "    'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she's\", 'should', \"should've\", 'shouldn',\n",
    "    \"shouldn't\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them',\n",
    "    'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until',\n",
    "    'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', 'were', 'weren', \"weren't\", 'what', 'when', 'where',\n",
    "    'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you',\n",
    "    \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n",
    "\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    filtered_tokens = [token for token in tokens if len(token) > 1 and token.lower() not in stop_words]\n",
    "    return filtered_tokens\n",
    "df2['tweet'] = df2['tweet'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "def extract_features(tweet):\n",
    "    \n",
    "    tokenized_text = ' '.join(tweet)\n",
    "    input_ids = torch.tensor(tokenizer.encode(tokenized_text, add_special_tokens=True)).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    features = last_hidden_states.squeeze(0).numpy()\n",
    "    \n",
    "    return features\n",
    "df2['features'] = df2['tweet'].apply(extract_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "features2 = df2['features'].tolist()\n",
    "padded_features2 = pad_sequences(features2, padding='post')\n",
    "padded_df2 = df2.copy()\n",
    "padded_df2['features'] = padded_features2.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input feature shape: (74, 30, 768)\n"
     ]
    }
   ],
   "source": [
    "X = np.stack(padded_df2['features'])\n",
    "print('Input feature shape:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input feature shape: (74, 30, 768)\n"
     ]
    }
   ],
   "source": [
    "print('Input feature shape:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "max_sequence_length=41\n",
    "X_=pad_sequences(X,maxlen=max_sequence_length,padding=\"post\",truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = np.reshape(X_, (74, 41 * 768))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_path = 'regression_model.pkl'\n",
    "loaded_model = joblib.load(saved_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.48784655, 0.4817829 , 0.51502776, 0.48867738, 0.51152269,\n",
       "       0.47268669, 0.51427968, 0.54073416, 0.49474977, 0.45404355,\n",
       "       0.47473635, 0.53372453, 0.5461583 , 0.51911273, 0.50145796,\n",
       "       0.49031687, 0.46854585, 0.50248791, 0.46383519, 0.50515464,\n",
       "       0.50919774, 0.5064765 , 0.5306432 , 0.47929434, 0.45840868,\n",
       "       0.4404831 , 0.47490827, 0.52702248, 0.48262534, 0.45128623,\n",
       "       0.5590954 , 0.56246497, 0.48169456, 0.50034924, 0.45543836,\n",
       "       0.55754581, 0.55863748, 0.47730018, 0.51305815, 0.52449995,\n",
       "       0.50874266, 0.50665062, 0.51627427, 0.44890362, 0.4688324 ,\n",
       "       0.52229636, 0.56065039, 0.46657713, 0.4651449 , 0.46823075,\n",
       "       0.51649416, 0.49475265, 0.44925808, 0.50765783, 0.48376244,\n",
       "       0.49233456, 0.49040889, 0.51422773, 0.51234926, 0.48375273,\n",
       "       0.55012716, 0.44118504, 0.47085625, 0.49575477, 0.46674028,\n",
       "       0.5094721 , 0.44222625, 0.49247741, 0.46084586, 0.49587525,\n",
       "       0.49701455, 0.44745434, 0.45570542, 0.52423579])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = loaded_model.predict(X_)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['predict']=pd.DataFrame(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_square_error: 0.02818928876422047\n"
     ]
    }
   ],
   "source": [
    "mse=mean_squared_error(df2['score'],df2['predict'])\n",
    "print(\"mean_square_error:\",mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.14187912392873117\n"
     ]
    }
   ],
   "source": [
    "mae = mean_absolute_error(df2['score'],df2['predict'])\n",
    "print(\"Mean Absolute Error:\", mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>emotion</th>\n",
       "      <th>score</th>\n",
       "      <th>features</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40786</td>\n",
       "      <td>[ball, watching, amp, rojod, header, equally, ...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.583</td>\n",
       "      <td>[[-0.43671992, 0.057505384, -0.19090268, -0.25...</td>\n",
       "      <td>0.487847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40787</td>\n",
       "      <td>[pessimist, someone, opportunity, knocks, comp...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.188</td>\n",
       "      <td>[[-0.04478655, 0.30576694, 0.06919098, -0.2915...</td>\n",
       "      <td>0.481783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40788</td>\n",
       "      <td>[season, im, looking, point, #depressing, #roy...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.688</td>\n",
       "      <td>[[-0.3687506, 0.162658, -0.02078579, -0.126300...</td>\n",
       "      <td>0.515028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40789</td>\n",
       "      <td>[stars, shinenyou, know, feelnscent, pine, nyo...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.292</td>\n",
       "      <td>[[-0.2151165, 0.2684997, -0.24530822, 0.244578...</td>\n",
       "      <td>0.488677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40790</td>\n",
       "      <td>[want, watch, netflix, stuck, class, #depressing]</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.667</td>\n",
       "      <td>[[0.16816223, -0.09243305, 0.078344375, 0.0814...</td>\n",
       "      <td>0.511523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>40855</td>\n",
       "      <td>[common, app, randomly, logged, writing, last,...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.833</td>\n",
       "      <td>[[-0.33207858, -0.17362213, -0.4620683, -0.113...</td>\n",
       "      <td>0.495875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>40856</td>\n",
       "      <td>[id, rather, laugh, rarest, genius, beautiful,...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.688</td>\n",
       "      <td>[[-0.18101707, 0.03217506, 0.012613542, -0.036...</td>\n",
       "      <td>0.497015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>40857</td>\n",
       "      <td>[#invest, new, #film, stop, asking, invest, ne...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.458</td>\n",
       "      <td>[[0.06594525, 0.13042212, 0.05061286, -0.07066...</td>\n",
       "      <td>0.447454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>40858</td>\n",
       "      <td>[watched, django, unchained, people, may, frow...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.333</td>\n",
       "      <td>[[0.15952902, 0.13285297, 0.2045169, 0.0801301...</td>\n",
       "      <td>0.455705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>40859</td>\n",
       "      <td>[depressing, despicable, trump, policies, camp...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.708</td>\n",
       "      <td>[[-0.19762044, 0.12002628, -0.21899879, -0.278...</td>\n",
       "      <td>0.524236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id                                              tweet  emotion  score  \\\n",
       "0   40786  [ball, watching, amp, rojod, header, equally, ...  sadness  0.583   \n",
       "1   40787  [pessimist, someone, opportunity, knocks, comp...  sadness  0.188   \n",
       "2   40788  [season, im, looking, point, #depressing, #roy...  sadness  0.688   \n",
       "3   40789  [stars, shinenyou, know, feelnscent, pine, nyo...  sadness  0.292   \n",
       "4   40790  [want, watch, netflix, stuck, class, #depressing]  sadness  0.667   \n",
       "..    ...                                                ...      ...    ...   \n",
       "69  40855  [common, app, randomly, logged, writing, last,...  sadness  0.833   \n",
       "70  40856  [id, rather, laugh, rarest, genius, beautiful,...  sadness  0.688   \n",
       "71  40857  [#invest, new, #film, stop, asking, invest, ne...  sadness  0.458   \n",
       "72  40858  [watched, django, unchained, people, may, frow...  sadness  0.333   \n",
       "73  40859  [depressing, despicable, trump, policies, camp...  sadness  0.708   \n",
       "\n",
       "                                             features   predict  \n",
       "0   [[-0.43671992, 0.057505384, -0.19090268, -0.25...  0.487847  \n",
       "1   [[-0.04478655, 0.30576694, 0.06919098, -0.2915...  0.481783  \n",
       "2   [[-0.3687506, 0.162658, -0.02078579, -0.126300...  0.515028  \n",
       "3   [[-0.2151165, 0.2684997, -0.24530822, 0.244578...  0.488677  \n",
       "4   [[0.16816223, -0.09243305, 0.078344375, 0.0814...  0.511523  \n",
       "..                                                ...       ...  \n",
       "69  [[-0.33207858, -0.17362213, -0.4620683, -0.113...  0.495875  \n",
       "70  [[-0.18101707, 0.03217506, 0.012613542, -0.036...  0.497015  \n",
       "71  [[0.06594525, 0.13042212, 0.05061286, -0.07066...  0.447454  \n",
       "72  [[0.15952902, 0.13285297, 0.2045169, 0.0801301...  0.455705  \n",
       "73  [[-0.19762044, 0.12002628, -0.21899879, -0.278...  0.524236  \n",
       "\n",
       "[74 rows x 6 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3= pd.read_csv(r\"C:\\Users\\i\\Downloads\\sadness-ratings-0to1.test.gold.txt\", delimiter='\\t', header=None)\n",
    "df3.columns = ['Id', 'tweet', 'emotion', 'score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", tweet)\n",
    "    \n",
    "    return tweet\n",
    "df3['tweet'] = df3['tweet'].apply(preprocess_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_mentions(tweet):\n",
    "    return re.sub(r'@\\w+', '', tweet)\n",
    "\n",
    "df3['tweet'] = df3['tweet'].apply(remove_mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s#@]', '', text)  \n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "df3['tweet'] = df3['tweet'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_emoji(tweet):\n",
    "    text = emoji.demojize(tweet)\n",
    "    return text\n",
    "df3['tweet'] = df3['tweet'].apply(convert_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_tweets(tweet):\n",
    "    tokenizer = TweetTokenizer()\n",
    "    return tokenizer.tokenize(tweet)\n",
    "\n",
    "df3['tweet'] = df3['tweet'].apply(tokenize_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\n",
    "    'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from', 'has', 'he', 'in', 'is', 'it', 'its',\n",
    "    'of', 'on', 'that', 'the', 'to', 'was', 'were', 'will', 'with', 'i', 'you', 'your', 'so', 'all',\n",
    "    'about', 'above', 'after', 'again', 'against', 'ain', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\",\n",
    "    'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can',\n",
    "    'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\",\n",
    "    'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\",\n",
    "    'have', 'haven', \"haven't\", 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how',\n",
    "    'i', 'if', 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
    "    'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no',\n",
    "    'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves',\n",
    "    'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she's\", 'should', \"should've\", 'shouldn',\n",
    "    \"shouldn't\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them',\n",
    "    'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until',\n",
    "    'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', 'were', 'weren', \"weren't\", 'what', 'when', 'where',\n",
    "    'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you',\n",
    "    \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n",
    "\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    filtered_tokens = [token for token in tokens if len(token) > 1 and token.lower() not in stop_words]\n",
    "    return filtered_tokens\n",
    "df3['tweet'] = df3['tweet'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "def extract_features(tweet):\n",
    "    \n",
    "    tokenized_text = ' '.join(tweet)\n",
    "    input_ids = torch.tensor(tokenizer.encode(tokenized_text, add_special_tokens=True)).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    features = last_hidden_states.squeeze(0).numpy()\n",
    "    \n",
    "    return features\n",
    "df3['features'] = df3['tweet'].apply(extract_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "features3 = df3['features'].tolist()\n",
    "padded_features3 = pad_sequences(features3, padding='post')\n",
    "padded_df3 = df3.copy()\n",
    "padded_df3['features'] = padded_features3.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input feature shape: (673, 37, 768)\n"
     ]
    }
   ],
   "source": [
    "X = np.stack(padded_df3['features'])\n",
    "print('Input feature shape:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "max_sequence_length=41\n",
    "X_=pad_sequences(X,maxlen=max_sequence_length,padding=\"post\",truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = np.reshape(X_, (673, 41 * 768))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_path = 'regression_model.pkl'\n",
    "loaded_model = joblib.load(saved_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.46550617, 0.48496906, 0.4413564 , 0.51697883, 0.53039954,\n",
       "       0.50574699, 0.4649814 , 0.4939384 , 0.47088382, 0.51133294,\n",
       "       0.55577054, 0.48435694, 0.49318285, 0.46069021, 0.53101276,\n",
       "       0.4482887 , 0.46273076, 0.49654763, 0.44395651, 0.55308603,\n",
       "       0.49224111, 0.49849717, 0.48515881, 0.53256576, 0.47714708,\n",
       "       0.47669385, 0.47740875, 0.47837025, 0.47489953, 0.48593391,\n",
       "       0.57561818, 0.46628537, 0.47929332, 0.46262995, 0.467198  ,\n",
       "       0.48732086, 0.49067047, 0.50705313, 0.48771711, 0.49097021,\n",
       "       0.45419565, 0.4538678 , 0.51537679, 0.51195464, 0.45544081,\n",
       "       0.53256127, 0.51689947, 0.49481387, 0.46616372, 0.5317275 ,\n",
       "       0.48900705, 0.48223959, 0.47944081, 0.53620596, 0.49536154,\n",
       "       0.49845128, 0.49308951, 0.48557988, 0.47463475, 0.50573635,\n",
       "       0.48139452, 0.44471647, 0.55929044, 0.5142178 , 0.47361906,\n",
       "       0.55544939, 0.56867547, 0.48858857, 0.4981999 , 0.50127756,\n",
       "       0.43687922, 0.5166149 , 0.49274696, 0.4222919 , 0.50296436,\n",
       "       0.45835211, 0.49044723, 0.48773896, 0.4951833 , 0.51753187,\n",
       "       0.4966618 , 0.46926123, 0.4759427 , 0.49345049, 0.48447439,\n",
       "       0.51916439, 0.54996146, 0.47023267, 0.46791095, 0.51574044,\n",
       "       0.56570877, 0.56115183, 0.4486799 , 0.50808054, 0.50166661,\n",
       "       0.59415068, 0.54680589, 0.56525617, 0.491225  , 0.45682635,\n",
       "       0.47225312, 0.49551495, 0.50011061, 0.55501861, 0.43954505,\n",
       "       0.44577688, 0.40222361, 0.49506204, 0.52761685, 0.49765512,\n",
       "       0.49149879, 0.46605663, 0.41886879, 0.4994034 , 0.54818954,\n",
       "       0.43213279, 0.45128814, 0.53002258, 0.48656598, 0.48246605,\n",
       "       0.49722306, 0.49139078, 0.53331757, 0.45853403, 0.52553923,\n",
       "       0.47820263, 0.51721814, 0.54157965, 0.54669802, 0.49434495,\n",
       "       0.50520331, 0.50612982, 0.46069621, 0.48096746, 0.55099879,\n",
       "       0.47407557, 0.50101926, 0.47406622, 0.56302006, 0.4828246 ,\n",
       "       0.58552424, 0.48978695, 0.45425723, 0.53780571, 0.56582486,\n",
       "       0.477401  , 0.45297004, 0.48571754, 0.4429739 , 0.48266734,\n",
       "       0.48212545, 0.46987562, 0.44208281, 0.49759755, 0.46994502,\n",
       "       0.49083745, 0.41905618, 0.42795409, 0.43070766, 0.50135962,\n",
       "       0.48319504, 0.48708581, 0.49912564, 0.49323473, 0.54221522,\n",
       "       0.49796911, 0.45667246, 0.47435675, 0.45316787, 0.45478748,\n",
       "       0.46771789, 0.50685951, 0.51962859, 0.55716336, 0.57393486,\n",
       "       0.53727085, 0.46219856, 0.53663082, 0.45856213, 0.554423  ,\n",
       "       0.55728419, 0.56682308, 0.51340238, 0.4953647 , 0.47678396,\n",
       "       0.50450437, 0.40977551, 0.49896551, 0.54026234, 0.50388882,\n",
       "       0.51606803, 0.48854637, 0.52365394, 0.49185016, 0.42016717,\n",
       "       0.48143751, 0.57509871, 0.49574529, 0.51185051, 0.50436377,\n",
       "       0.45545153, 0.50968813, 0.52631076, 0.49400043, 0.51379254,\n",
       "       0.47569473, 0.52510836, 0.48901723, 0.47083762, 0.46266113,\n",
       "       0.47918362, 0.54077827, 0.47413236, 0.49561115, 0.50655175,\n",
       "       0.50370586, 0.5191204 , 0.51944289, 0.49272499, 0.41803314,\n",
       "       0.46127336, 0.53402816, 0.54299007, 0.45477918, 0.47096959,\n",
       "       0.52699634, 0.46611014, 0.47160808, 0.5020982 , 0.4745064 ,\n",
       "       0.52367636, 0.4790124 , 0.5800275 , 0.40743669, 0.47426807,\n",
       "       0.48315735, 0.47426578, 0.45760415, 0.46701284, 0.45456733,\n",
       "       0.55800913, 0.48009519, 0.54177337, 0.51579258, 0.42042705,\n",
       "       0.53120018, 0.48132114, 0.54448709, 0.53486121, 0.48888407,\n",
       "       0.47726578, 0.51068894, 0.50825628, 0.44555437, 0.39544427,\n",
       "       0.44582455, 0.46795726, 0.53995574, 0.49660755, 0.50127952,\n",
       "       0.51927272, 0.50129033, 0.40522496, 0.5060866 , 0.57629399,\n",
       "       0.46971913, 0.48830972, 0.46322514, 0.51534345, 0.4908826 ,\n",
       "       0.52228877, 0.49849601, 0.48153117, 0.50827337, 0.44429183,\n",
       "       0.52602479, 0.48553235, 0.4871044 , 0.57282864, 0.54707363,\n",
       "       0.51287616, 0.48887421, 0.48100872, 0.4411446 , 0.52661168,\n",
       "       0.49047748, 0.55136866, 0.5502749 , 0.40908921, 0.50612349,\n",
       "       0.51409086, 0.45161093, 0.52458952, 0.46297782, 0.41072605,\n",
       "       0.51753539, 0.51244188, 0.49832342, 0.49005893, 0.48899046,\n",
       "       0.46197371, 0.51234647, 0.49730484, 0.53166561, 0.5299607 ,\n",
       "       0.5114417 , 0.54874808, 0.50973398, 0.47383848, 0.55147549,\n",
       "       0.48390607, 0.47888024, 0.51595257, 0.45907734, 0.4958292 ,\n",
       "       0.46305028, 0.50614927, 0.51861499, 0.53604132, 0.45599579,\n",
       "       0.48322203, 0.50218864, 0.47698156, 0.51675647, 0.46757723,\n",
       "       0.5427066 , 0.44702264, 0.53070666, 0.55139671, 0.5149739 ,\n",
       "       0.45907596, 0.46413588, 0.50689981, 0.49047661, 0.43193161,\n",
       "       0.47994797, 0.51106448, 0.50722678, 0.50952031, 0.50712379,\n",
       "       0.48730584, 0.46936439, 0.50147688, 0.54362022, 0.42841815,\n",
       "       0.49791542, 0.48804661, 0.41259563, 0.51967542, 0.5155964 ,\n",
       "       0.58443765, 0.55793582, 0.47636341, 0.55989406, 0.53677608,\n",
       "       0.5548992 , 0.47158932, 0.45638611, 0.50177158, 0.51748435,\n",
       "       0.51366959, 0.48523748, 0.50166456, 0.46501925, 0.49407166,\n",
       "       0.47733243, 0.46300088, 0.52170274, 0.5008146 , 0.49919699,\n",
       "       0.48399811, 0.45908731, 0.58765579, 0.5753224 , 0.39960027,\n",
       "       0.56502949, 0.44605578, 0.49624223, 0.47000048, 0.52968107,\n",
       "       0.42632596, 0.53192592, 0.51693503, 0.48601189, 0.47810484,\n",
       "       0.46537905, 0.52729462, 0.52751965, 0.49256952, 0.4955406 ,\n",
       "       0.4823787 , 0.49158201, 0.52901807, 0.50010501, 0.48191655,\n",
       "       0.4881953 , 0.51294452, 0.49793919, 0.48848777, 0.5757614 ,\n",
       "       0.50846421, 0.51951845, 0.48291359, 0.57340336, 0.5106729 ,\n",
       "       0.5150485 , 0.49962848, 0.5671381 , 0.4938218 , 0.48512966,\n",
       "       0.53325413, 0.47979727, 0.50045906, 0.50519072, 0.54894386,\n",
       "       0.48307518, 0.53145067, 0.46199404, 0.47679337, 0.48753554,\n",
       "       0.54084562, 0.48703664, 0.49227588, 0.46546281, 0.54719407,\n",
       "       0.48080891, 0.51523355, 0.45611392, 0.50539494, 0.4547707 ,\n",
       "       0.42478874, 0.49280585, 0.51189849, 0.52207925, 0.50698004,\n",
       "       0.5246115 , 0.54077326, 0.50036451, 0.50285932, 0.48928991,\n",
       "       0.47236067, 0.47534249, 0.50336418, 0.46470643, 0.46782095,\n",
       "       0.48562038, 0.5317012 , 0.47169561, 0.44295728, 0.48725089,\n",
       "       0.51031476, 0.55981834, 0.53046056, 0.44713727, 0.45476956,\n",
       "       0.44666877, 0.51707463, 0.47661947, 0.48557895, 0.57150448,\n",
       "       0.51376211, 0.4625577 , 0.47005373, 0.57136114, 0.60830876,\n",
       "       0.44202211, 0.56476122, 0.50869939, 0.55552466, 0.47415638,\n",
       "       0.56405487, 0.49518225, 0.48645891, 0.4342614 , 0.50506345,\n",
       "       0.55440087, 0.45949276, 0.54225737, 0.51514794, 0.47648284,\n",
       "       0.52667304, 0.49944747, 0.49815508, 0.41616781, 0.47949054,\n",
       "       0.52002092, 0.46762163, 0.52358015, 0.47480406, 0.53150545,\n",
       "       0.46643877, 0.45735282, 0.48566825, 0.49598347, 0.49337937,\n",
       "       0.4889345 , 0.45401664, 0.55293114, 0.47004527, 0.60919863,\n",
       "       0.52850739, 0.5631108 , 0.47387397, 0.45616294, 0.47944402,\n",
       "       0.48222191, 0.47080816, 0.50120154, 0.53787033, 0.56056443,\n",
       "       0.4714677 , 0.47421208, 0.46112169, 0.44689513, 0.51610313,\n",
       "       0.48408265, 0.50860326, 0.57078082, 0.5126917 , 0.54707078,\n",
       "       0.55842233, 0.43961228, 0.46110484, 0.51488514, 0.48522545,\n",
       "       0.45111926, 0.55252999, 0.46252726, 0.51658174, 0.52037914,\n",
       "       0.48948611, 0.48109099, 0.53224052, 0.48941901, 0.55273234,\n",
       "       0.52082958, 0.5090826 , 0.50856964, 0.48310996, 0.45227884,\n",
       "       0.47127725, 0.49663118, 0.51812873, 0.54036399, 0.55104737,\n",
       "       0.56143826, 0.60470016, 0.4810809 , 0.58410812, 0.52715313,\n",
       "       0.52661982, 0.47513553, 0.49470043, 0.4528264 , 0.42677714,\n",
       "       0.4839322 , 0.45078552, 0.43057036, 0.49046392, 0.51834484,\n",
       "       0.48444884, 0.48123687, 0.53021913, 0.55117678, 0.49870042,\n",
       "       0.42728649, 0.47714035, 0.4931392 , 0.52547328, 0.49027016,\n",
       "       0.51813571, 0.50802928, 0.5329757 , 0.52225626, 0.48809751,\n",
       "       0.45559968, 0.47241168, 0.52598309, 0.47784182, 0.53367383,\n",
       "       0.51380878, 0.46102995, 0.56888806, 0.47492554, 0.56693835,\n",
       "       0.49928982, 0.47364759, 0.50488926, 0.45686058, 0.50220827,\n",
       "       0.49854329, 0.48579502, 0.4952431 , 0.51752064, 0.50479246,\n",
       "       0.57350831, 0.48753341, 0.50193161, 0.54987917, 0.53486852,\n",
       "       0.48941084, 0.49800071, 0.47933874, 0.51479363, 0.55157352,\n",
       "       0.53313693, 0.50027584, 0.49208593, 0.50092732, 0.48339113,\n",
       "       0.55333452, 0.51102295, 0.49669565, 0.45308609, 0.47305357,\n",
       "       0.48695117, 0.45504117, 0.50435611, 0.47174232, 0.39309439,\n",
       "       0.4842797 , 0.53689442, 0.47272546, 0.50139473, 0.49117266,\n",
       "       0.49058594, 0.48791189, 0.45288081, 0.50976362, 0.44485761,\n",
       "       0.55133986, 0.48579248, 0.49711977, 0.46840526, 0.47665039,\n",
       "       0.50559029, 0.53625775, 0.57808677, 0.54710222, 0.54956448,\n",
       "       0.510473  , 0.45286421, 0.48508655, 0.48806585, 0.57458206,\n",
       "       0.46665363, 0.4593145 , 0.47228446, 0.53880023, 0.51852952,\n",
       "       0.50315672, 0.47651575, 0.47661245, 0.55223617, 0.43438399,\n",
       "       0.43973103, 0.47795929, 0.52187765, 0.48076471, 0.50402666,\n",
       "       0.50189418, 0.51570975, 0.43935743, 0.49074324, 0.47705331,\n",
       "       0.47827556, 0.49506533, 0.47724624, 0.51682223, 0.48678459,\n",
       "       0.43764852, 0.53485436, 0.45558424])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicttest = loaded_model.predict(X_)\n",
    "predicttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['predicttest']=pd.DataFrame(predicttest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=df3.drop(['score','features'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>emotion</th>\n",
       "      <th>predicttest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40860</td>\n",
       "      <td>[teens, sons, left, car, get, haircuts, im, pr...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.465506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40861</td>\n",
       "      <td>[teens, sons, left, car, get, haircuts, im, pr...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.484969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40862</td>\n",
       "      <td>[hartramseysuplift, youre, still, discouraged,...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.441356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40863</td>\n",
       "      <td>[nearly, dropped, phone, sink, hahahaha]</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.516979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40864</td>\n",
       "      <td>[whenever, im, feeling, sad, listen, monsta, h...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.530400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>41528</td>\n",
       "      <td>[candice, constantly, pout, #gbbo]</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.516822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>41529</td>\n",
       "      <td>[#unhappy, #redbus, cc, talked, week, still, d...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.486785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>41530</td>\n",
       "      <td>[pull, afew, weeks, ago, sadly, theres, game, ...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.437649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>41531</td>\n",
       "      <td>[im, buying, art, supplies, im, debating, seri...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.534854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>41532</td>\n",
       "      <td>[could, ask, chafford, hundred, store, turn, c...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.455584</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>673 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id                                              tweet  emotion  \\\n",
       "0    40860  [teens, sons, left, car, get, haircuts, im, pr...  sadness   \n",
       "1    40861  [teens, sons, left, car, get, haircuts, im, pr...  sadness   \n",
       "2    40862  [hartramseysuplift, youre, still, discouraged,...  sadness   \n",
       "3    40863           [nearly, dropped, phone, sink, hahahaha]  sadness   \n",
       "4    40864  [whenever, im, feeling, sad, listen, monsta, h...  sadness   \n",
       "..     ...                                                ...      ...   \n",
       "668  41528                 [candice, constantly, pout, #gbbo]  sadness   \n",
       "669  41529  [#unhappy, #redbus, cc, talked, week, still, d...  sadness   \n",
       "670  41530  [pull, afew, weeks, ago, sadly, theres, game, ...  sadness   \n",
       "671  41531  [im, buying, art, supplies, im, debating, seri...  sadness   \n",
       "672  41532  [could, ask, chafford, hundred, store, turn, c...  sadness   \n",
       "\n",
       "     predicttest  \n",
       "0       0.465506  \n",
       "1       0.484969  \n",
       "2       0.441356  \n",
       "3       0.516979  \n",
       "4       0.530400  \n",
       "..           ...  \n",
       "668     0.516822  \n",
       "669     0.486785  \n",
       "670     0.437649  \n",
       "671     0.534854  \n",
       "672     0.455584  \n",
       "\n",
       "[673 rows x 4 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
