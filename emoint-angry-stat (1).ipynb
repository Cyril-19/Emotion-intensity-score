{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1= pd.read_csv(r\"C:\\Users\\i\\Downloads\\anger-ratings-0to1.train.txt\", delimiter='\\t', header=None)\n",
    "df1.columns = ['Id', 'tweet', 'emotion', 'score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Id                                              tweet emotion  score\n",
      "0    10000  How the fu*k! Who the heck! moved my fridge!.....   anger  0.938\n",
      "1    10001  So my Indian Uber driver just called someone t...   anger  0.896\n",
      "2    10002  @DPD_UK I asked for my parcel to be delivered ...   anger  0.896\n",
      "3    10003  so ef whichever butt wipe pulled the fire alar...   anger  0.896\n",
      "4    10004  Don't join @BTCare they put the phone down on ...   anger  0.896\n",
      "..     ...                                                ...     ...    ...\n",
      "852  10852   rose incense are the best thing I've ever bought   anger  0.125\n",
      "853  10853         @jaaames1993 Literally burst out laughing.   anger  0.067\n",
      "854  10854           Follow up. Follow through. Be . #success   anger  0.125\n",
      "855  10855  Wrinkles should merely hide where frown have b...   anger  0.125\n",
      "856  10856  Love the new song I can't stop thinking about ...   anger  0.083\n",
      "\n",
      "[857 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import emoji\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", tweet)\n",
    "    \n",
    "    return tweet\n",
    "df1['tweet'] = df1['tweet'].apply(preprocess_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_mentions(tweet):\n",
    "    return re.sub(r'@\\w+', '', tweet)\n",
    "\n",
    "df1['tweet'] = df1['tweet'].apply(remove_mentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s#@]', '', text) \n",
    "    text = re.sub(r'\\d+', '', text)  \n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "df1['tweet'] = df1['tweet'].apply(clean_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>emotion</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000</td>\n",
       "      <td>how the fuk who the heck moved my fridge shoul...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10001</td>\n",
       "      <td>so my indian uber driver just called someone t...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10002</td>\n",
       "      <td>i asked for my parcel to be delivered to a pic...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10003</td>\n",
       "      <td>so ef whichever butt wipe pulled the fire alar...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10004</td>\n",
       "      <td>dont join they put the phone down on you talk ...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852</th>\n",
       "      <td>10852</td>\n",
       "      <td>rose incense are the best thing ive ever bought</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853</th>\n",
       "      <td>10853</td>\n",
       "      <td>literally burst out laughing</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854</th>\n",
       "      <td>10854</td>\n",
       "      <td>follow up follow through be #success</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>10855</td>\n",
       "      <td>wrinkles should merely hide where frown have b...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>10856</td>\n",
       "      <td>love the new song i cant stop thinking about y...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>857 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id                                              tweet emotion  score\n",
       "0    10000  how the fuk who the heck moved my fridge shoul...   anger  0.938\n",
       "1    10001  so my indian uber driver just called someone t...   anger  0.896\n",
       "2    10002  i asked for my parcel to be delivered to a pic...   anger  0.896\n",
       "3    10003  so ef whichever butt wipe pulled the fire alar...   anger  0.896\n",
       "4    10004  dont join they put the phone down on you talk ...   anger  0.896\n",
       "..     ...                                                ...     ...    ...\n",
       "852  10852    rose incense are the best thing ive ever bought   anger  0.125\n",
       "853  10853                       literally burst out laughing   anger  0.067\n",
       "854  10854               follow up follow through be #success   anger  0.125\n",
       "855  10855  wrinkles should merely hide where frown have b...   anger  0.125\n",
       "856  10856  love the new song i cant stop thinking about y...   anger  0.083\n",
       "\n",
       "[857 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_emoji(tweet):\n",
    "    text = emoji.demojize(tweet)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['tweet'] = df1['tweet'].apply(convert_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_tweets(tweet):\n",
    "    tokenizer = TweetTokenizer()\n",
    "    return tokenizer.tokenize(tweet)\n",
    "\n",
    "df1['tweet'] = df1['tweet'].apply(tokenize_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>emotion</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000</td>\n",
       "      <td>[how, the, fuk, who, the, heck, moved, my, fri...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10001</td>\n",
       "      <td>[so, my, indian, uber, driver, just, called, s...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10002</td>\n",
       "      <td>[i, asked, for, my, parcel, to, be, delivered,...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10003</td>\n",
       "      <td>[so, ef, whichever, butt, wipe, pulled, the, f...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10004</td>\n",
       "      <td>[dont, join, they, put, the, phone, down, on, ...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852</th>\n",
       "      <td>10852</td>\n",
       "      <td>[rose, incense, are, the, best, thing, ive, ev...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853</th>\n",
       "      <td>10853</td>\n",
       "      <td>[literally, burst, out, laughing]</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854</th>\n",
       "      <td>10854</td>\n",
       "      <td>[follow, up, follow, through, be, #success]</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>10855</td>\n",
       "      <td>[wrinkles, should, merely, hide, where, frown,...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>10856</td>\n",
       "      <td>[love, the, new, song, i, cant, stop, thinking...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>857 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id                                              tweet emotion  score\n",
       "0    10000  [how, the, fuk, who, the, heck, moved, my, fri...   anger  0.938\n",
       "1    10001  [so, my, indian, uber, driver, just, called, s...   anger  0.896\n",
       "2    10002  [i, asked, for, my, parcel, to, be, delivered,...   anger  0.896\n",
       "3    10003  [so, ef, whichever, butt, wipe, pulled, the, f...   anger  0.896\n",
       "4    10004  [dont, join, they, put, the, phone, down, on, ...   anger  0.896\n",
       "..     ...                                                ...     ...    ...\n",
       "852  10852  [rose, incense, are, the, best, thing, ive, ev...   anger  0.125\n",
       "853  10853                  [literally, burst, out, laughing]   anger  0.067\n",
       "854  10854        [follow, up, follow, through, be, #success]   anger  0.125\n",
       "855  10855  [wrinkles, should, merely, hide, where, frown,...   anger  0.125\n",
       "856  10856  [love, the, new, song, i, cant, stop, thinking...   anger  0.083\n",
       "\n",
       "[857 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\n",
    "    'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from', 'has', 'he', 'in', 'is', 'it', 'its',\n",
    "    'of', 'on', 'that', 'the', 'to', 'was', 'were', 'will', 'with', 'i', 'you', 'your', 'so', 'all',\n",
    "    'about', 'above', 'after', 'again', 'against', 'ain', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\",\n",
    "    'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can',\n",
    "    'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\",\n",
    "    'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\",\n",
    "    'have', 'haven', \"haven't\", 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how',\n",
    "    'i', 'if', 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
    "    'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no',\n",
    "    'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves',\n",
    "    'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she's\", 'should', \"should've\", 'shouldn',\n",
    "    \"shouldn't\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them',\n",
    "    'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until',\n",
    "    'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', 'were', 'weren', \"weren't\", 'what', 'when', 'where',\n",
    "    'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you',\n",
    "    \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n",
    "def remove_stopwords(tokens):\n",
    "    filtered_tokens = [token for token in tokens if len(token) > 1 and token.lower() not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "df1['tweet'] = df1['tweet'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>emotion</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000</td>\n",
       "      <td>[fuk, heck, moved, fridge, knock, landlord, do...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10001</td>\n",
       "      <td>[indian, uber, driver, called, someone, word, ...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10002</td>\n",
       "      <td>[asked, parcel, delivered, pick, store, addres...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10003</td>\n",
       "      <td>[ef, whichever, butt, wipe, pulled, fire, alar...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10004</td>\n",
       "      <td>[dont, join, put, phone, talk, rude, taking, m...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852</th>\n",
       "      <td>10852</td>\n",
       "      <td>[rose, incense, best, thing, ive, ever, bought]</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853</th>\n",
       "      <td>10853</td>\n",
       "      <td>[literally, burst, laughing]</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854</th>\n",
       "      <td>10854</td>\n",
       "      <td>[follow, follow, #success]</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>10855</td>\n",
       "      <td>[wrinkles, merely, hide, frown, mark, twain]</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>10856</td>\n",
       "      <td>[love, new, song, cant, stop, thinking]</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>857 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id                                              tweet emotion  score\n",
       "0    10000  [fuk, heck, moved, fridge, knock, landlord, do...   anger  0.938\n",
       "1    10001  [indian, uber, driver, called, someone, word, ...   anger  0.896\n",
       "2    10002  [asked, parcel, delivered, pick, store, addres...   anger  0.896\n",
       "3    10003  [ef, whichever, butt, wipe, pulled, fire, alar...   anger  0.896\n",
       "4    10004  [dont, join, put, phone, talk, rude, taking, m...   anger  0.896\n",
       "..     ...                                                ...     ...    ...\n",
       "852  10852    [rose, incense, best, thing, ive, ever, bought]   anger  0.125\n",
       "853  10853                       [literally, burst, laughing]   anger  0.067\n",
       "854  10854                         [follow, follow, #success]   anger  0.125\n",
       "855  10855       [wrinkles, merely, hide, frown, mark, twain]   anger  0.125\n",
       "856  10856            [love, new, song, cant, stop, thinking]   anger  0.083\n",
       "\n",
       "[857 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "def extract_features(tweet):\n",
    "    tokenized_text = ' '.join(tweet)\n",
    "    input_ids = torch.tensor(tokenizer.encode(tokenized_text, add_special_tokens=True)).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    features = last_hidden_states.squeeze(0).numpy()\n",
    "    \n",
    "    return features\n",
    "\n",
    "df1['features'] = df1['tweet'].apply(extract_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>emotion</th>\n",
       "      <th>score</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>774</th>\n",
       "      <td>10774</td>\n",
       "      <td>[focus, dont, let, others, love, define, youre...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.292</td>\n",
       "      <td>[[-0.16542822, 0.34364292, -0.38134283, -0.027...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>10745</td>\n",
       "      <td>[wolfpack, theme, trons, sting, wore, wolf, sh...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.312</td>\n",
       "      <td>[[0.09369362, -0.03939514, -0.03917374, 0.2356...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>10101</td>\n",
       "      <td>[majority, people, irritate, fuck, cba, people...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.862</td>\n",
       "      <td>[[-0.17213908, 0.3319891, -0.037328538, -0.553...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>10191</td>\n",
       "      <td>[half, epicenter, shut, police, yall, burst, t...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.625</td>\n",
       "      <td>[[-0.124072574, 0.36686358, 0.25633517, -0.231...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>10767</td>\n",
       "      <td>[thats, joke, know, incense]</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.222</td>\n",
       "      <td>[[-0.0002665827, 0.39932182, -0.053959202, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>10059</td>\n",
       "      <td>[ikr, people, still, got, grudge, reason, like...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.750</td>\n",
       "      <td>[[0.060917467, 0.10603421, 0.18070087, 0.02237...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761</th>\n",
       "      <td>10761</td>\n",
       "      <td>[houston, might, lose, coach, tomorrow, midnig...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.312</td>\n",
       "      <td>[[0.00772772, -0.0036625378, 0.317061, -0.0307...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000</td>\n",
       "      <td>[fuk, heck, moved, fridge, knock, landlord, do...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.938</td>\n",
       "      <td>[[-0.09296838, 0.5572394, 0.11283746, -0.31033...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>10176</td>\n",
       "      <td>[girl, sitting, front, chewing, gum, like, cow...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.646</td>\n",
       "      <td>[[-0.118008755, 0.057007827, 0.22342765, 0.076...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>10039</td>\n",
       "      <td>[#angry, order, #months, ago, placed, order, c...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.771</td>\n",
       "      <td>[[-0.21696734, -0.040554434, 0.026198037, -0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>857 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id                                              tweet emotion  score  \\\n",
       "774  10774  [focus, dont, let, others, love, define, youre...   anger  0.292   \n",
       "745  10745  [wolfpack, theme, trons, sting, wore, wolf, sh...   anger  0.312   \n",
       "101  10101  [majority, people, irritate, fuck, cba, people...   anger  0.862   \n",
       "191  10191  [half, epicenter, shut, police, yall, burst, t...   anger  0.625   \n",
       "767  10767                       [thats, joke, know, incense]   anger  0.222   \n",
       "..     ...                                                ...     ...    ...   \n",
       "59   10059  [ikr, people, still, got, grudge, reason, like...   anger  0.750   \n",
       "761  10761  [houston, might, lose, coach, tomorrow, midnig...   anger  0.312   \n",
       "0    10000  [fuk, heck, moved, fridge, knock, landlord, do...   anger  0.938   \n",
       "176  10176  [girl, sitting, front, chewing, gum, like, cow...   anger  0.646   \n",
       "39   10039  [#angry, order, #months, ago, placed, order, c...   anger  0.771   \n",
       "\n",
       "                                              features  \n",
       "774  [[-0.16542822, 0.34364292, -0.38134283, -0.027...  \n",
       "745  [[0.09369362, -0.03939514, -0.03917374, 0.2356...  \n",
       "101  [[-0.17213908, 0.3319891, -0.037328538, -0.553...  \n",
       "191  [[-0.124072574, 0.36686358, 0.25633517, -0.231...  \n",
       "767  [[-0.0002665827, 0.39932182, -0.053959202, 0.0...  \n",
       "..                                                 ...  \n",
       "59   [[0.060917467, 0.10603421, 0.18070087, 0.02237...  \n",
       "761  [[0.00772772, -0.0036625378, 0.317061, -0.0307...  \n",
       "0    [[-0.09296838, 0.5572394, 0.11283746, -0.31033...  \n",
       "176  [[-0.118008755, 0.057007827, 0.22342765, 0.076...  \n",
       "39   [[-0.21696734, -0.040554434, 0.026198037, -0.0...  \n",
       "\n",
       "[857 rows x 5 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "df1 = shuffle(df1)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 768)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.features[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>emotion</th>\n",
       "      <th>score</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>774</th>\n",
       "      <td>10774</td>\n",
       "      <td>[focus, dont, let, others, love, define, youre...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.292</td>\n",
       "      <td>[[-0.16542822, 0.34364292, -0.38134283, -0.027...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>10745</td>\n",
       "      <td>[wolfpack, theme, trons, sting, wore, wolf, sh...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.312</td>\n",
       "      <td>[[0.09369362, -0.03939514, -0.03917374, 0.2356...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>10101</td>\n",
       "      <td>[majority, people, irritate, fuck, cba, people...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.862</td>\n",
       "      <td>[[-0.17213908, 0.3319891, -0.037328538, -0.553...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>10191</td>\n",
       "      <td>[half, epicenter, shut, police, yall, burst, t...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.625</td>\n",
       "      <td>[[-0.124072574, 0.36686358, 0.25633517, -0.231...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>10767</td>\n",
       "      <td>[thats, joke, know, incense]</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.222</td>\n",
       "      <td>[[-0.0002665827, 0.39932182, -0.053959202, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>10023</td>\n",
       "      <td>[tasers, immobilize, taser, someone, fuck, nee...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.812</td>\n",
       "      <td>[[-0.08561629, 0.37572518, 0.14344436, -0.5390...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>10387</td>\n",
       "      <td>[#epipen, public, outrage, occurs, expand, #pa...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.500</td>\n",
       "      <td>[[-0.15601857, -0.055495482, 0.10573532, -0.06...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776</th>\n",
       "      <td>10776</td>\n",
       "      <td>[frown, cup, cheeks, hands, step, aside, angel...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.304</td>\n",
       "      <td>[[-0.24261428, 0.18392316, -0.07424502, -0.218...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>10753</td>\n",
       "      <td>[orange, one, #poisonous, #bitter, shouldnt, j...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.312</td>\n",
       "      <td>[[-0.32387605, 0.12601873, -0.3613245, 0.01979...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640</th>\n",
       "      <td>10640</td>\n",
       "      <td>[dude, new, madden, haha]</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.231</td>\n",
       "      <td>[[-0.4428942, 0.1035945, 0.012004156, -0.16633...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id                                              tweet emotion  score  \\\n",
       "774  10774  [focus, dont, let, others, love, define, youre...   anger  0.292   \n",
       "745  10745  [wolfpack, theme, trons, sting, wore, wolf, sh...   anger  0.312   \n",
       "101  10101  [majority, people, irritate, fuck, cba, people...   anger  0.862   \n",
       "191  10191  [half, epicenter, shut, police, yall, burst, t...   anger  0.625   \n",
       "767  10767                       [thats, joke, know, incense]   anger  0.222   \n",
       "23   10023  [tasers, immobilize, taser, someone, fuck, nee...   anger  0.812   \n",
       "387  10387  [#epipen, public, outrage, occurs, expand, #pa...   anger  0.500   \n",
       "776  10776  [frown, cup, cheeks, hands, step, aside, angel...   anger  0.304   \n",
       "753  10753  [orange, one, #poisonous, #bitter, shouldnt, j...   anger  0.312   \n",
       "640  10640                          [dude, new, madden, haha]   anger  0.231   \n",
       "\n",
       "                                              features  \n",
       "774  [[-0.16542822, 0.34364292, -0.38134283, -0.027...  \n",
       "745  [[0.09369362, -0.03939514, -0.03917374, 0.2356...  \n",
       "101  [[-0.17213908, 0.3319891, -0.037328538, -0.553...  \n",
       "191  [[-0.124072574, 0.36686358, 0.25633517, -0.231...  \n",
       "767  [[-0.0002665827, 0.39932182, -0.053959202, 0.0...  \n",
       "23   [[-0.08561629, 0.37572518, 0.14344436, -0.5390...  \n",
       "387  [[-0.15601857, -0.055495482, 0.10573532, -0.06...  \n",
       "776  [[-0.24261428, 0.18392316, -0.07424502, -0.218...  \n",
       "753  [[-0.32387605, 0.12601873, -0.3613245, 0.01979...  \n",
       "640  [[-0.4428942, 0.1035945, 0.012004156, -0.16633...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "features = df1['features'].tolist()\n",
    "padded_features = pad_sequences(features, padding='post')\n",
    "padded_df = df1.copy()\n",
    "padded_df['features'] = padded_features.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.stack(padded_df['features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (857, 39, 768)\n",
      "Output shape: (857,)\n"
     ]
    }
   ],
   "source": [
    "y = np.array(padded_df['score'])    \n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.reshape(x, (857, 39 * 768))  \n",
    "y = np.reshape(y, (857,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.292, 0.312, 0.862, 0.625, 0.222, 0.812, 0.5  , 0.304, 0.312,\n",
       "       0.231, 0.574, 0.458, 0.125, 0.425, 0.375, 0.333, 0.37 , 0.458,\n",
       "       0.562, 0.125, 0.562, 0.864, 0.417, 0.542, 0.458, 0.479, 0.5  ,\n",
       "       0.479, 0.375, 0.75 , 0.729, 0.583, 0.417, 0.75 , 0.354, 0.417,\n",
       "       0.479, 0.75 , 0.583, 0.479, 0.521, 0.271, 0.553, 0.938, 0.354,\n",
       "       0.625, 0.69 , 0.417, 0.188, 0.083, 0.458, 0.792, 0.479, 0.542,\n",
       "       0.708, 0.863, 0.458, 0.583, 0.146, 0.229, 0.479, 0.438, 0.438,\n",
       "       0.688, 0.571, 0.312, 0.375, 0.375, 0.75 , 0.5  , 0.521, 0.396,\n",
       "       0.396, 0.75 , 0.646, 0.625, 0.479, 0.396, 0.396, 0.479, 0.519,\n",
       "       0.667, 0.625, 0.521, 0.491, 0.229, 0.208, 0.312, 0.517, 0.5  ,\n",
       "       0.25 , 0.458, 0.188, 0.5  , 0.462, 0.604, 0.521, 0.333, 0.562,\n",
       "       0.328, 0.067, 0.479, 0.625, 0.208, 0.521, 0.375, 0.517, 0.296,\n",
       "       0.479, 0.472, 0.229, 0.792, 0.562, 0.466, 0.271, 0.146, 0.542,\n",
       "       0.354, 0.479, 0.667, 0.208, 0.292, 0.188, 0.583, 0.844, 0.143,\n",
       "       0.875, 0.479, 0.188, 0.417, 0.5  , 0.415, 0.729, 0.729, 0.521,\n",
       "       0.521, 0.792, 0.396, 0.792, 0.854, 0.396, 0.583, 0.708, 0.458,\n",
       "       0.542, 0.708, 0.441, 0.167, 0.479, 0.438, 0.646, 0.583, 0.396,\n",
       "       0.792, 0.521, 0.562, 0.438, 0.354, 0.646, 0.667, 0.5  , 0.375,\n",
       "       0.525, 0.708, 0.458, 0.75 , 0.667, 0.479, 0.25 , 0.542, 0.729,\n",
       "       0.438, 0.542, 0.688, 0.542, 0.354, 0.771, 0.479, 0.354, 0.646,\n",
       "       0.521, 0.333, 0.667, 0.29 , 0.521, 0.434, 0.604, 0.542, 0.562,\n",
       "       0.75 , 0.542, 0.479, 0.875, 0.438, 0.583, 0.458, 0.604, 0.271,\n",
       "       0.438, 0.764, 0.5  , 0.542, 0.5  , 0.26 , 0.438, 0.538, 0.854,\n",
       "       0.354, 0.604, 0.254, 0.341, 0.484, 0.333, 0.583, 0.491, 0.438,\n",
       "       0.542, 0.354, 0.562, 0.625, 0.667, 0.417, 0.812, 0.542, 0.542,\n",
       "       0.5  , 0.792, 0.396, 0.396, 0.766, 0.521, 0.771, 0.188, 0.322,\n",
       "       0.375, 0.354, 0.625, 0.521, 0.271, 0.479, 0.604, 0.333, 0.542,\n",
       "       0.479, 0.375, 0.792, 0.75 , 0.354, 0.458, 0.542, 0.667, 0.812,\n",
       "       0.5  , 0.562, 0.479, 0.324, 0.583, 0.583, 0.583, 0.5  , 0.625,\n",
       "       0.562, 0.5  , 0.759, 0.292, 0.375, 0.646, 0.458, 0.885, 0.667,\n",
       "       0.562, 0.292, 0.562, 0.396, 0.817, 0.479, 0.458, 0.521, 0.442,\n",
       "       0.479, 0.646, 0.646, 0.771, 0.75 , 0.625, 0.583, 0.494, 0.417,\n",
       "       0.333, 0.531, 0.562, 0.5  , 0.146, 0.354, 0.729, 0.354, 0.646,\n",
       "       0.521, 0.542, 0.479, 0.375, 0.417, 0.583, 0.646, 0.232, 0.708,\n",
       "       0.312, 0.312, 0.271, 0.604, 0.354, 0.458, 0.5  , 0.833, 0.354,\n",
       "       0.542, 0.356, 0.295, 0.25 , 0.52 , 0.354, 0.896, 0.519, 0.417,\n",
       "       0.375, 0.189, 0.625, 0.167, 0.646, 0.854, 0.646, 0.542, 0.396,\n",
       "       0.458, 0.472, 0.167, 0.25 , 0.562, 0.708, 0.479, 0.208, 0.583,\n",
       "       0.583, 0.84 , 0.562, 0.333, 0.417, 0.333, 0.479, 0.435, 0.583,\n",
       "       0.292, 0.312, 0.185, 0.625, 0.729, 0.312, 0.357, 0.125, 0.438,\n",
       "       0.792, 0.373, 0.396, 0.354, 0.438, 0.646, 0.458, 0.356, 0.438,\n",
       "       0.562, 0.468, 0.646, 0.375, 0.438, 0.188, 0.354, 0.729, 0.521,\n",
       "       0.417, 0.604, 0.5  , 0.492, 0.375, 0.521, 0.25 , 0.75 , 0.375,\n",
       "       0.333, 0.593, 0.396, 0.172, 0.417, 0.562, 0.5  , 0.688, 0.646,\n",
       "       0.688, 0.271, 0.479, 0.521, 0.396, 0.75 , 0.708, 0.5  , 0.417,\n",
       "       0.542, 0.188, 0.375, 0.562, 0.542, 0.458, 0.562, 0.479, 0.562,\n",
       "       0.292, 0.491, 0.417, 0.417, 0.5  , 0.147, 0.208, 0.5  , 0.729,\n",
       "       0.396, 0.438, 0.375, 0.375, 0.146, 0.479, 0.229, 0.312, 0.433,\n",
       "       0.562, 0.708, 0.458, 0.354, 0.458, 0.833, 0.583, 0.458, 0.292,\n",
       "       0.5  , 0.479, 0.625, 0.242, 0.646, 0.417, 0.447, 0.458, 0.458,\n",
       "       0.873, 0.127, 0.271, 0.708, 0.312, 0.646, 0.167, 0.854, 0.333,\n",
       "       0.625, 0.375, 0.177, 0.729, 0.375, 0.667, 0.542, 0.75 , 0.312,\n",
       "       0.333, 0.479, 0.396, 0.583, 0.688, 0.562, 0.562, 0.604, 0.604,\n",
       "       0.583, 0.479, 0.521, 0.583, 0.396, 0.5  , 0.833, 0.771, 0.417,\n",
       "       0.271, 0.25 , 0.396, 0.688, 0.771, 0.521, 0.25 , 0.542, 0.354,\n",
       "       0.271, 0.333, 0.417, 0.891, 0.667, 0.458, 0.771, 0.708, 0.562,\n",
       "       0.188, 0.583, 0.667, 0.312, 0.438, 0.562, 0.792, 0.521, 0.562,\n",
       "       0.646, 0.625, 0.667, 0.75 , 0.396, 0.896, 0.271, 0.521, 0.578,\n",
       "       0.583, 0.354, 0.479, 0.479, 0.542, 0.479, 0.458, 0.438, 0.417,\n",
       "       0.479, 0.444, 0.875, 0.202, 0.5  , 0.556, 0.625, 0.4  , 0.5  ,\n",
       "       0.688, 0.354, 0.354, 0.708, 0.271, 0.646, 0.646, 0.604, 0.562,\n",
       "       0.188, 0.521, 0.729, 0.646, 0.396, 0.604, 0.2  , 0.417, 0.396,\n",
       "       0.354, 0.182, 0.562, 0.667, 0.646, 0.875, 0.521, 0.5  , 0.604,\n",
       "       0.562, 0.188, 0.625, 0.583, 0.5  , 0.333, 0.157, 0.375, 0.521,\n",
       "       0.583, 0.125, 0.479, 0.792, 0.271, 0.625, 0.729, 0.438, 0.542,\n",
       "       0.333, 0.625, 0.646, 0.314, 0.5  , 0.271, 0.5  , 0.417, 0.688,\n",
       "       0.354, 0.479, 0.438, 0.425, 0.255, 0.648, 0.604, 0.75 , 0.417,\n",
       "       0.396, 0.458, 0.312, 0.396, 0.896, 0.562, 0.792, 0.333, 0.208,\n",
       "       0.729, 0.271, 0.414, 0.625, 0.458, 0.75 , 0.271, 0.556, 0.354,\n",
       "       0.596, 0.312, 0.417, 0.542, 0.375, 0.375, 0.354, 0.417, 0.25 ,\n",
       "       0.566, 0.667, 0.417, 0.479, 0.417, 0.479, 0.625, 0.271, 0.708,\n",
       "       0.333, 0.417, 0.438, 0.375, 0.389, 0.375, 0.312, 0.417, 0.167,\n",
       "       0.775, 0.646, 0.458, 0.792, 0.375, 0.292, 0.867, 0.417, 0.646,\n",
       "       0.771, 0.688, 0.479, 0.417, 0.831, 0.458, 0.417, 0.271, 0.312,\n",
       "       0.521, 0.208, 0.543, 0.5  , 0.9  , 0.417, 0.562, 0.583, 0.583,\n",
       "       0.527, 0.562, 0.688, 0.438, 0.396, 0.646, 0.333, 0.25 , 0.354,\n",
       "       0.604, 0.396, 0.729, 0.812, 0.479, 0.333, 0.375, 0.427, 0.521,\n",
       "       0.542, 0.438, 0.5  , 0.667, 0.458, 0.421, 0.479, 0.521, 0.75 ,\n",
       "       0.479, 0.854, 0.396, 0.417, 0.542, 0.208, 0.875, 0.375, 0.479,\n",
       "       0.479, 0.875, 0.333, 0.479, 0.292, 0.521, 0.312, 0.417, 0.312,\n",
       "       0.792, 0.125, 0.375, 0.688, 0.438, 0.5  , 0.521, 0.271, 0.598,\n",
       "       0.604, 0.604, 0.542, 0.438, 0.625, 0.125, 0.438, 0.438, 0.146,\n",
       "       0.562, 0.479, 0.417, 0.417, 0.542, 0.646, 0.375, 0.688, 0.562,\n",
       "       0.896, 0.667, 0.542, 0.479, 0.5  , 0.417, 0.417, 0.417, 0.733,\n",
       "       0.354, 0.375, 0.333, 0.75 , 0.438, 0.521, 0.438, 0.579, 0.69 ,\n",
       "       0.5  , 0.354, 0.458, 0.458, 0.458, 0.542, 0.667, 0.625, 0.688,\n",
       "       0.292, 0.75 , 0.5  , 0.438, 0.375, 0.646, 0.458, 0.458, 0.438,\n",
       "       0.458, 0.542, 0.271, 0.708, 0.542, 0.417, 0.688, 0.521, 0.667,\n",
       "       0.629, 0.688, 0.792, 0.583, 0.396, 0.479, 0.438, 0.521, 0.438,\n",
       "       0.75 , 0.458, 0.33 , 0.792, 0.396, 0.396, 0.396, 0.729, 0.5  ,\n",
       "       0.375, 0.417, 0.542, 0.604, 0.438, 0.375, 0.542, 0.396, 0.333,\n",
       "       0.812, 0.75 , 0.479, 0.438, 0.333, 0.333, 0.771, 0.438, 0.375,\n",
       "       0.417, 0.5  , 0.479, 0.458, 0.625, 0.479, 0.521, 0.458, 0.396,\n",
       "       0.729, 0.5  , 0.396, 0.583, 0.422, 0.625, 0.458, 0.312, 0.438,\n",
       "       0.468, 0.646, 0.481, 0.562, 0.438, 0.521, 0.75 , 0.312, 0.938,\n",
       "       0.646, 0.771])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=x.copy()  \n",
    "Y=y.copy()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=2000)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "alpha = 2000\n",
    "model = Ridge(alpha=alpha)\n",
    "model.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.01872813812461101\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X)\n",
    "mse = mean_squared_error(Y, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.10816342059710127\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mae = mean_absolute_error(Y, y_pred)\n",
    "\n",
    "print(\"Mean Absolute Error:\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['regression_model.pkl']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "save_path = 'regression_model.pkl'\n",
    "joblib.dump(model, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2= pd.read_csv(r\"C:\\Users\\i\\Downloads\\anger-ratings-0to1.dev.gold.txt\", delimiter='\\t', header=None)\n",
    "df2.columns = ['Id', 'tweet', 'emotion', 'score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>emotion</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10857</td>\n",
       "      <td>@ZubairSabirPTI  pls dont insult the word 'Molna'</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10858</td>\n",
       "      <td>@ArcticFantasy I would have almost took offens...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10859</td>\n",
       "      <td>@IllinoisLoyalty that Rutgers game was an abom...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10860</td>\n",
       "      <td>@CozanGaming that's what lisa asked before she...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10861</td>\n",
       "      <td>Sometimes I get mad over something so minuscul...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>10936</td>\n",
       "      <td>@Jen_ny69 People will always get offended ever...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>10937</td>\n",
       "      <td>@gayla_weeks1 I try not to let my anger seep i...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>10938</td>\n",
       "      <td>I hope my hustle don't offend nobody</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>10939</td>\n",
       "      <td>Just watched Django Unchained, Other people ma...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>10940</td>\n",
       "      <td>Lol little things like that make me so angry x</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.604</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id                                              tweet emotion  score\n",
       "0   10857  @ZubairSabirPTI  pls dont insult the word 'Molna'   anger  0.479\n",
       "1   10858  @ArcticFantasy I would have almost took offens...   anger  0.458\n",
       "2   10859  @IllinoisLoyalty that Rutgers game was an abom...   anger  0.562\n",
       "3   10860  @CozanGaming that's what lisa asked before she...   anger  0.500\n",
       "4   10861  Sometimes I get mad over something so minuscul...   anger  0.708\n",
       "..    ...                                                ...     ...    ...\n",
       "79  10936  @Jen_ny69 People will always get offended ever...   anger  0.562\n",
       "80  10937  @gayla_weeks1 I try not to let my anger seep i...   anger  0.625\n",
       "81  10938               I hope my hustle don't offend nobody   anger  0.292\n",
       "82  10939  Just watched Django Unchained, Other people ma...   anger  0.229\n",
       "83  10940     Lol little things like that make me so angry x   anger  0.604\n",
       "\n",
       "[84 rows x 4 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", tweet)\n",
    "    \n",
    "    return tweet\n",
    "df2['tweet'] = df2['tweet'].apply(preprocess_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_mentions(tweet):\n",
    "    return re.sub(r'@\\w+', '', tweet)\n",
    "\n",
    "df2['tweet'] = df2['tweet'].apply(remove_mentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s#@]', '', text)  \n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "df2['tweet'] = df2['tweet'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_emoji(tweet):\n",
    "    text = emoji.demojize(tweet)\n",
    "    return text\n",
    "df2['tweet'] = df2['tweet'].apply(convert_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_tweets(tweet):\n",
    "    tokenizer = TweetTokenizer()\n",
    "    return tokenizer.tokenize(tweet)\n",
    "\n",
    "df2['tweet'] = df2['tweet'].apply(tokenize_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\n",
    "    'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from', 'has', 'he', 'in', 'is', 'it', 'its',\n",
    "    'of', 'on', 'that', 'the', 'to', 'was', 'were', 'will', 'with', 'i', 'you', 'your', 'so', 'all',\n",
    "    'about', 'above', 'after', 'again', 'against', 'ain', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\",\n",
    "    'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can',\n",
    "    'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\",\n",
    "    'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\",\n",
    "    'have', 'haven', \"haven't\", 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how',\n",
    "    'i', 'if', 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
    "    'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no',\n",
    "    'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves',\n",
    "    'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she's\", 'should', \"should've\", 'shouldn',\n",
    "    \"shouldn't\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them',\n",
    "    'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until',\n",
    "    'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', 'were', 'weren', \"weren't\", 'what', 'when', 'where',\n",
    "    'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you',\n",
    "    \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n",
    "\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    filtered_tokens = [token for token in tokens if len(token) > 1 and token.lower() not in stop_words]\n",
    "    return filtered_tokens\n",
    "df2['tweet'] = df2['tweet'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "def extract_features(tweet):\n",
    "    \n",
    "    tokenized_text = ' '.join(tweet)\n",
    "    input_ids = torch.tensor(tokenizer.encode(tokenized_text, add_special_tokens=True)).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    features = last_hidden_states.squeeze(0).numpy()\n",
    "    \n",
    "    return features\n",
    "df2['features'] = df2['tweet'].apply(extract_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "features2 = df2['features'].tolist()\n",
    "padded_features2 = pad_sequences(features2, padding='post')\n",
    "padded_df2 = df2.copy()\n",
    "padded_df2['features'] = padded_features2.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input feature shape: (84, 42, 768)\n"
     ]
    }
   ],
   "source": [
    "X = np.stack(padded_df2['features'])\n",
    "print('Input feature shape:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input feature shape: (84, 42, 768)\n"
     ]
    }
   ],
   "source": [
    "print('Input feature shape:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "max_sequence_length=39\n",
    "X_=pad_sequences(X,maxlen=max_sequence_length,padding=\"post\",truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = np.reshape(X_, (84, 39 * 768))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_path = 'regression_model.pkl'\n",
    "loaded_model = joblib.load(saved_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.49842736, 0.46203439, 0.53270571, 0.49154141, 0.51946799,\n",
       "       0.49900371, 0.52337554, 0.50929572, 0.54080388, 0.49334245,\n",
       "       0.50756418, 0.50020949, 0.50724266, 0.53093603, 0.50609313,\n",
       "       0.52949005, 0.53945058, 0.50126373, 0.47355087, 0.48728093,\n",
       "       0.49374787, 0.49815871, 0.4703646 , 0.51879091, 0.51026489,\n",
       "       0.48904454, 0.48825501, 0.50775732, 0.5571534 , 0.56283485,\n",
       "       0.47003905, 0.49016105, 0.48620832, 0.46474187, 0.51269573,\n",
       "       0.51122319, 0.50963272, 0.47945398, 0.5067111 , 0.49413188,\n",
       "       0.46216495, 0.49043235, 0.43621519, 0.43578475, 0.53542   ,\n",
       "       0.5537343 , 0.49283092, 0.4785767 , 0.54021132, 0.49563876,\n",
       "       0.47192046, 0.49303141, 0.46593574, 0.50834858, 0.50291306,\n",
       "       0.51509979, 0.4609621 , 0.49610944, 0.52690274, 0.49458277,\n",
       "       0.49568274, 0.48274893, 0.50553983, 0.51949117, 0.47356065,\n",
       "       0.47066734, 0.48628545, 0.46517613, 0.46517613, 0.49919687,\n",
       "       0.46144758, 0.47798402, 0.50057393, 0.50495191, 0.4603027 ,\n",
       "       0.45080199, 0.52085242, 0.46844247, 0.53604426, 0.49881151,\n",
       "       0.5236037 , 0.46237383, 0.50338315, 0.50319204])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = loaded_model.predict(X_)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['predict']=pd.DataFrame(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_square_error: 0.022288857486363632\n"
     ]
    }
   ],
   "source": [
    "mse=mean_squared_error(df2['score'],df2['predict'])\n",
    "print(\"mean_square_error:\",mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.11620193283453203\n"
     ]
    }
   ],
   "source": [
    "mae = mean_absolute_error(df2['score'],df2['predict'])\n",
    "print(\"Mean Absolute Error:\", mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>emotion</th>\n",
       "      <th>score</th>\n",
       "      <th>features</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10857</td>\n",
       "      <td>[pls, dont, insult, word, molna]</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.479</td>\n",
       "      <td>[[-0.16410865, 0.59082854, -0.29882985, -0.219...</td>\n",
       "      <td>0.498427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10858</td>\n",
       "      <td>[would, almost, took, offense, actually, snapped]</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.458</td>\n",
       "      <td>[[0.016600985, 0.094489805, 0.05158711, 0.0325...</td>\n",
       "      <td>0.462034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10859</td>\n",
       "      <td>[rutgers, game, abomination, affront, god, man...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.562</td>\n",
       "      <td>[[-0.05811101, 0.47538888, -0.17922018, -0.149...</td>\n",
       "      <td>0.532706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10860</td>\n",
       "      <td>[thats, lisa, asked, started, raging, call, heh]</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.500</td>\n",
       "      <td>[[-0.23008505, 0.32468075, 0.22509159, -0.2303...</td>\n",
       "      <td>0.491541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10861</td>\n",
       "      <td>[sometimes, get, mad, something, minuscule, tr...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.708</td>\n",
       "      <td>[[0.26804066, -0.19931054, 0.14129303, -0.0347...</td>\n",
       "      <td>0.519468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>10936</td>\n",
       "      <td>[people, always, get, offended, everyones, sit...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.562</td>\n",
       "      <td>[[0.28602043, 0.51173747, -0.058281485, 0.2042...</td>\n",
       "      <td>0.498812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>10937</td>\n",
       "      <td>[try, let, anger, seep, reviews, resent, time,...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.625</td>\n",
       "      <td>[[0.1119683, -0.38341117, -0.030351479, -0.021...</td>\n",
       "      <td>0.523604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>10938</td>\n",
       "      <td>[hope, hustle, dont, offend, nobody]</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.292</td>\n",
       "      <td>[[-0.023534112, 0.5019074, -0.48025265, -0.017...</td>\n",
       "      <td>0.462374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>10939</td>\n",
       "      <td>[watched, django, unchained, people, may, frow...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.229</td>\n",
       "      <td>[[0.15952902, 0.13285297, 0.2045169, 0.0801301...</td>\n",
       "      <td>0.503383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>10940</td>\n",
       "      <td>[lol, little, things, like, make, angry]</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.604</td>\n",
       "      <td>[[-0.13832302, 0.4406552, -0.2898925, 0.069618...</td>\n",
       "      <td>0.503192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id                                              tweet emotion  score  \\\n",
       "0   10857                   [pls, dont, insult, word, molna]   anger  0.479   \n",
       "1   10858  [would, almost, took, offense, actually, snapped]   anger  0.458   \n",
       "2   10859  [rutgers, game, abomination, affront, god, man...   anger  0.562   \n",
       "3   10860   [thats, lisa, asked, started, raging, call, heh]   anger  0.500   \n",
       "4   10861  [sometimes, get, mad, something, minuscule, tr...   anger  0.708   \n",
       "..    ...                                                ...     ...    ...   \n",
       "79  10936  [people, always, get, offended, everyones, sit...   anger  0.562   \n",
       "80  10937  [try, let, anger, seep, reviews, resent, time,...   anger  0.625   \n",
       "81  10938               [hope, hustle, dont, offend, nobody]   anger  0.292   \n",
       "82  10939  [watched, django, unchained, people, may, frow...   anger  0.229   \n",
       "83  10940           [lol, little, things, like, make, angry]   anger  0.604   \n",
       "\n",
       "                                             features   predict  \n",
       "0   [[-0.16410865, 0.59082854, -0.29882985, -0.219...  0.498427  \n",
       "1   [[0.016600985, 0.094489805, 0.05158711, 0.0325...  0.462034  \n",
       "2   [[-0.05811101, 0.47538888, -0.17922018, -0.149...  0.532706  \n",
       "3   [[-0.23008505, 0.32468075, 0.22509159, -0.2303...  0.491541  \n",
       "4   [[0.26804066, -0.19931054, 0.14129303, -0.0347...  0.519468  \n",
       "..                                                ...       ...  \n",
       "79  [[0.28602043, 0.51173747, -0.058281485, 0.2042...  0.498812  \n",
       "80  [[0.1119683, -0.38341117, -0.030351479, -0.021...  0.523604  \n",
       "81  [[-0.023534112, 0.5019074, -0.48025265, -0.017...  0.462374  \n",
       "82  [[0.15952902, 0.13285297, 0.2045169, 0.0801301...  0.503383  \n",
       "83  [[-0.13832302, 0.4406552, -0.2898925, 0.069618...  0.503192  \n",
       "\n",
       "[84 rows x 6 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3= pd.read_csv(r\"C:\\Users\\i\\Downloads\\anger-ratings-0to1.test.target.txt\", delimiter='\\t', header=None)\n",
    "df3.columns = ['Id', 'tweet', 'emotion', 'score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", tweet)\n",
    "    \n",
    "    return tweet\n",
    "df3['tweet'] = df3['tweet'].apply(preprocess_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_mentions(tweet):\n",
    "    return re.sub(r'@\\w+', '', tweet)\n",
    "\n",
    "df3['tweet'] = df3['tweet'].apply(remove_mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s#@]', '', text)  \n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "df3['tweet'] = df3['tweet'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_emoji(tweet):\n",
    "    text = emoji.demojize(tweet)\n",
    "    return text\n",
    "df3['tweet'] = df3['tweet'].apply(convert_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_tweets(tweet):\n",
    "    tokenizer = TweetTokenizer()\n",
    "    return tokenizer.tokenize(tweet)\n",
    "\n",
    "df3['tweet'] = df3['tweet'].apply(tokenize_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\n",
    "    'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from', 'has', 'he', 'in', 'is', 'it', 'its',\n",
    "    'of', 'on', 'that', 'the', 'to', 'was', 'were', 'will', 'with', 'i', 'you', 'your', 'so', 'all',\n",
    "    'about', 'above', 'after', 'again', 'against', 'ain', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\",\n",
    "    'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can',\n",
    "    'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\",\n",
    "    'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\",\n",
    "    'have', 'haven', \"haven't\", 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how',\n",
    "    'i', 'if', 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
    "    'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no',\n",
    "    'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves',\n",
    "    'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she's\", 'should', \"should've\", 'shouldn',\n",
    "    \"shouldn't\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them',\n",
    "    'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until',\n",
    "    'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', 'were', 'weren', \"weren't\", 'what', 'when', 'where',\n",
    "    'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you',\n",
    "    \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n",
    "\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    filtered_tokens = [token for token in tokens if len(token) > 1 and token.lower() not in stop_words]\n",
    "    return filtered_tokens\n",
    "df3['tweet'] = df3['tweet'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "def extract_features(tweet):\n",
    "    \n",
    "    tokenized_text = ' '.join(tweet)\n",
    "    input_ids = torch.tensor(tokenizer.encode(tokenized_text, add_special_tokens=True)).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    features = last_hidden_states.squeeze(0).numpy()\n",
    "    \n",
    "    return features\n",
    "df3['features'] = df3['tweet'].apply(extract_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "features3 = df3['features'].tolist()\n",
    "padded_features3 = pad_sequences(features3, padding='post')\n",
    "padded_df3 = df3.copy()\n",
    "padded_df3['features'] = padded_features3.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input feature shape: (760, 43, 768)\n"
     ]
    }
   ],
   "source": [
    "X = np.stack(padded_df3['features'])\n",
    "print('Input feature shape:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "max_sequence_length=39\n",
    "X_=pad_sequences(X,maxlen=max_sequence_length,padding=\"post\",truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = np.reshape(X_, (760, 39 * 768))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_path = 'regression_model.pkl'\n",
    "loaded_model = joblib.load(saved_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.52533331, 0.44690213, 0.5098315 , 0.480763  , 0.50630038,\n",
       "       0.52030501, 0.46563691, 0.50365541, 0.4819241 , 0.4390828 ,\n",
       "       0.47887386, 0.51775205, 0.46161318, 0.53332154, 0.50154011,\n",
       "       0.43225537, 0.45251208, 0.49180472, 0.49995856, 0.50379784,\n",
       "       0.47390469, 0.48423737, 0.54341471, 0.49623301, 0.50899592,\n",
       "       0.46581053, 0.46323017, 0.51584305, 0.50587553, 0.47632581,\n",
       "       0.45678251, 0.52166921, 0.53935576, 0.52754997, 0.51392598,\n",
       "       0.47453121, 0.50854318, 0.52659933, 0.44910776, 0.56092853,\n",
       "       0.57932237, 0.48773483, 0.52134678, 0.57392528, 0.52223061,\n",
       "       0.52313121, 0.50651197, 0.49360887, 0.48583735, 0.45523913,\n",
       "       0.45543686, 0.48453976, 0.4880368 , 0.51375376, 0.52473707,\n",
       "       0.41667003, 0.42019073, 0.50541617, 0.49828903, 0.49014918,\n",
       "       0.50489872, 0.46513667, 0.49689789, 0.52662026, 0.50512586,\n",
       "       0.48088079, 0.4592799 , 0.43297746, 0.50578651, 0.47916262,\n",
       "       0.51760875, 0.49159447, 0.47466437, 0.43710115, 0.56649829,\n",
       "       0.523609  , 0.4830959 , 0.53322804, 0.46914811, 0.49658188,\n",
       "       0.53487563, 0.55179582, 0.46077961, 0.5138525 , 0.48125753,\n",
       "       0.49083367, 0.48304423, 0.51611005, 0.49689857, 0.50021368,\n",
       "       0.46846752, 0.47764466, 0.50557022, 0.48472576, 0.54590797,\n",
       "       0.55087546, 0.61569842, 0.58994864, 0.55055519, 0.49132017,\n",
       "       0.45946362, 0.48948031, 0.49709623, 0.5063102 , 0.47306037,\n",
       "       0.47624908, 0.48269301, 0.49857945, 0.47878776, 0.47550272,\n",
       "       0.44473261, 0.54749884, 0.48051315, 0.45595117, 0.49472095,\n",
       "       0.47908205, 0.48325019, 0.52129464, 0.52816451, 0.46081443,\n",
       "       0.46399083, 0.46449501, 0.54946048, 0.47013239, 0.44776987,\n",
       "       0.48690886, 0.48476775, 0.45361272, 0.48988685, 0.50875268,\n",
       "       0.51531448, 0.52266338, 0.53293801, 0.48979158, 0.56190643,\n",
       "       0.55663246, 0.5139667 , 0.48771215, 0.46762805, 0.47496671,\n",
       "       0.47809048, 0.49323787, 0.4658721 , 0.45042029, 0.48223979,\n",
       "       0.54267574, 0.50199061, 0.50110564, 0.44499645, 0.52901435,\n",
       "       0.49054034, 0.51108606, 0.55158786, 0.50588635, 0.50016421,\n",
       "       0.49748782, 0.60173355, 0.53592143, 0.51691541, 0.50690723,\n",
       "       0.53650413, 0.53267519, 0.47410571, 0.43827101, 0.47884428,\n",
       "       0.46028994, 0.49681197, 0.4446778 , 0.52708951, 0.54324738,\n",
       "       0.50084681, 0.50918246, 0.50668093, 0.50856832, 0.54749143,\n",
       "       0.45382939, 0.46388269, 0.49353651, 0.48526217, 0.54041184,\n",
       "       0.4522289 , 0.45114002, 0.53099318, 0.52402599, 0.52866248,\n",
       "       0.50985658, 0.48622985, 0.509573  , 0.50618211, 0.47206763,\n",
       "       0.47670676, 0.45720522, 0.51622194, 0.48159874, 0.47778812,\n",
       "       0.49852275, 0.48542484, 0.48863101, 0.46766029, 0.52704319,\n",
       "       0.47957684, 0.44166756, 0.49355138, 0.53122407, 0.48484299,\n",
       "       0.52303482, 0.45664252, 0.47801367, 0.48393352, 0.43968528,\n",
       "       0.48122264, 0.50113484, 0.44787373, 0.48262046, 0.56529575,\n",
       "       0.53033158, 0.54502524, 0.49337277, 0.49033789, 0.50466465,\n",
       "       0.46469019, 0.43841268, 0.56900261, 0.53199574, 0.4839275 ,\n",
       "       0.55686975, 0.48577168, 0.43915315, 0.47268484, 0.45272387,\n",
       "       0.51935425, 0.47710876, 0.5205265 , 0.50355502, 0.48912828,\n",
       "       0.47412924, 0.4760755 , 0.4783726 , 0.44554248, 0.41814963,\n",
       "       0.51524069, 0.4858134 , 0.4873576 , 0.463742  , 0.52907119,\n",
       "       0.48933194, 0.52135333, 0.49170653, 0.4977603 , 0.51408353,\n",
       "       0.52396482, 0.53472766, 0.49174686, 0.49136963, 0.43381391,\n",
       "       0.46530883, 0.4647863 , 0.49586831, 0.45786512, 0.51127037,\n",
       "       0.51417724, 0.48966614, 0.52579038, 0.52377244, 0.56606891,\n",
       "       0.54292154, 0.48545586, 0.48074443, 0.52228566, 0.51265654,\n",
       "       0.49346665, 0.53080201, 0.51671278, 0.5028842 , 0.4630997 ,\n",
       "       0.4809249 , 0.51461119, 0.52413917, 0.48822389, 0.49302535,\n",
       "       0.50304776, 0.52123639, 0.43964106, 0.52897351, 0.43398257,\n",
       "       0.44387544, 0.52640021, 0.49525407, 0.50436373, 0.48465998,\n",
       "       0.51869756, 0.4695963 , 0.48781627, 0.49622025, 0.50007938,\n",
       "       0.51300284, 0.5048923 , 0.48990007, 0.50402942, 0.49825307,\n",
       "       0.50712966, 0.48248576, 0.51414336, 0.44650194, 0.52530106,\n",
       "       0.51636443, 0.49657155, 0.49734277, 0.48265606, 0.49169802,\n",
       "       0.4842873 , 0.49809927, 0.4945799 , 0.51861877, 0.47166121,\n",
       "       0.46221941, 0.49352074, 0.52569637, 0.48202278, 0.47106415,\n",
       "       0.5391992 , 0.50458127, 0.49353885, 0.46152004, 0.47925944,\n",
       "       0.52604097, 0.48874862, 0.50815166, 0.45361507, 0.51056315,\n",
       "       0.45022065, 0.48883861, 0.48287514, 0.50532451, 0.5072317 ,\n",
       "       0.48372527, 0.48899481, 0.45129724, 0.54889516, 0.48900635,\n",
       "       0.483833  , 0.51825405, 0.52783068, 0.51267449, 0.50972077,\n",
       "       0.53063958, 0.4999816 , 0.47181988, 0.50961723, 0.55835255,\n",
       "       0.46338009, 0.48912939, 0.47081669, 0.4931415 , 0.52397031,\n",
       "       0.50042702, 0.48716676, 0.48912826, 0.49120949, 0.45910496,\n",
       "       0.52043108, 0.49190561, 0.493345  , 0.54832948, 0.53286115,\n",
       "       0.53908686, 0.47099941, 0.49132685, 0.48938223, 0.54653697,\n",
       "       0.5046421 , 0.47623898, 0.51695031, 0.45281443, 0.49787783,\n",
       "       0.43308256, 0.53123338, 0.44867754, 0.45219115, 0.51276964,\n",
       "       0.50087118, 0.51840961, 0.49855536, 0.50733666, 0.4587729 ,\n",
       "       0.52954923, 0.53991015, 0.47865661, 0.52506243, 0.53679482,\n",
       "       0.48562427, 0.49721182, 0.48122534, 0.50190777, 0.58032129,\n",
       "       0.52850865, 0.49437356, 0.47586936, 0.47459055, 0.49884833,\n",
       "       0.47755847, 0.45785444, 0.44214362, 0.45638194, 0.43184818,\n",
       "       0.55205885, 0.51223618, 0.48394547, 0.48754203, 0.47594921,\n",
       "       0.55685008, 0.50995131, 0.46673747, 0.48220095, 0.51359076,\n",
       "       0.53742536, 0.49149752, 0.5037632 , 0.4855542 , 0.51502803,\n",
       "       0.47906414, 0.47315067, 0.52655992, 0.472257  , 0.53238696,\n",
       "       0.48424263, 0.47386482, 0.53437042, 0.52866711, 0.48113098,\n",
       "       0.58666438, 0.50015571, 0.47782346, 0.55965343, 0.49866758,\n",
       "       0.47463299, 0.50206487, 0.48914138, 0.51602108, 0.5297781 ,\n",
       "       0.57903098, 0.50893197, 0.46232861, 0.50728234, 0.48316227,\n",
       "       0.51821184, 0.50523779, 0.45849783, 0.50308825, 0.48891818,\n",
       "       0.50594322, 0.52049415, 0.53995361, 0.49237364, 0.49559752,\n",
       "       0.44518307, 0.49055101, 0.48684451, 0.48796457, 0.48903961,\n",
       "       0.51206879, 0.49687297, 0.45481641, 0.48717232, 0.46996146,\n",
       "       0.48405414, 0.48738821, 0.46924276, 0.52252772, 0.51224736,\n",
       "       0.4666625 , 0.44952693, 0.51628711, 0.51384906, 0.51775205,\n",
       "       0.47521069, 0.49112732, 0.46976348, 0.45507616, 0.47614133,\n",
       "       0.5591865 , 0.5267606 , 0.56306713, 0.48347596, 0.50955266,\n",
       "       0.49305992, 0.51085603, 0.46733908, 0.57162873, 0.48715277,\n",
       "       0.51290425, 0.52700441, 0.51168705, 0.49807311, 0.50075609,\n",
       "       0.49359751, 0.49480975, 0.46601796, 0.56483729, 0.49766499,\n",
       "       0.4846932 , 0.48993326, 0.50591793, 0.44703008, 0.51502803,\n",
       "       0.466661  , 0.4659336 , 0.47018506, 0.55106542, 0.54769557,\n",
       "       0.50906104, 0.45210444, 0.51271243, 0.48994934, 0.50247236,\n",
       "       0.45306221, 0.51895149, 0.4782085 , 0.52387355, 0.49723778,\n",
       "       0.4832709 , 0.46750932, 0.53644699, 0.49101778, 0.48976581,\n",
       "       0.4927216 , 0.48919667, 0.43433382, 0.52005992, 0.49378096,\n",
       "       0.50181286, 0.53091798, 0.54316021, 0.4296401 , 0.45263792,\n",
       "       0.49149261, 0.5183032 , 0.48282975, 0.48132345, 0.48035459,\n",
       "       0.53905261, 0.54136835, 0.49651409, 0.51453255, 0.51314791,\n",
       "       0.4611854 , 0.51521623, 0.51013407, 0.43423003, 0.52257042,\n",
       "       0.50314144, 0.47105979, 0.49638524, 0.52457715, 0.52623414,\n",
       "       0.51694673, 0.54546979, 0.59004938, 0.57725174, 0.49955857,\n",
       "       0.54790094, 0.53272844, 0.56389831, 0.48655582, 0.43496091,\n",
       "       0.50032777, 0.55789139, 0.50150223, 0.48105513, 0.48014987,\n",
       "       0.55163501, 0.5450351 , 0.49221943, 0.50870822, 0.52302099,\n",
       "       0.51540803, 0.51494181, 0.55488016, 0.54337894, 0.51053478,\n",
       "       0.52308684, 0.48208418, 0.50602678, 0.47084443, 0.46694462,\n",
       "       0.47552761, 0.54963359, 0.51644933, 0.51863678, 0.46340605,\n",
       "       0.51393669, 0.50486056, 0.52973381, 0.46682223, 0.51648157,\n",
       "       0.4942173 , 0.43741862, 0.48727268, 0.44734742, 0.53779484,\n",
       "       0.44556864, 0.52099209, 0.522432  , 0.49775878, 0.50140124,\n",
       "       0.50071449, 0.5094833 , 0.50372618, 0.49499401, 0.45649902,\n",
       "       0.52715989, 0.48946053, 0.4826685 , 0.45774421, 0.47073144,\n",
       "       0.50600419, 0.49393743, 0.49618931, 0.48029814, 0.47647009,\n",
       "       0.48617565, 0.47360004, 0.50789416, 0.48170935, 0.53703531,\n",
       "       0.52170661, 0.51807064, 0.48104752, 0.52966053, 0.49766135,\n",
       "       0.48282129, 0.52548566, 0.45835772, 0.49015158, 0.49634681,\n",
       "       0.53428539, 0.52832986, 0.47034795, 0.50662101, 0.45006515,\n",
       "       0.50749506, 0.51826694, 0.51127081, 0.55495517, 0.53017661,\n",
       "       0.51725093, 0.51846793, 0.51117947, 0.491037  , 0.51137482,\n",
       "       0.54120692, 0.53561265, 0.53787948, 0.5068997 , 0.52040238,\n",
       "       0.54373733, 0.51185778, 0.56470358, 0.55966248, 0.56591564,\n",
       "       0.51656151, 0.46252241, 0.49886859, 0.49178633, 0.52638402,\n",
       "       0.47885208, 0.45842822, 0.44686182, 0.48991832, 0.49964783,\n",
       "       0.55313255, 0.49889665, 0.48412673, 0.50285991, 0.51408757,\n",
       "       0.48006324, 0.43616242, 0.51486046, 0.48392411, 0.49878043,\n",
       "       0.48724131, 0.50771713, 0.48111717, 0.48915168, 0.52690255,\n",
       "       0.4562429 , 0.48450835, 0.48445103, 0.50117054, 0.46596051,\n",
       "       0.45308142, 0.47617445, 0.46212801, 0.4594095 , 0.50531279,\n",
       "       0.4506467 , 0.47154897, 0.47212287, 0.51362903, 0.50415503,\n",
       "       0.49014362, 0.52187501, 0.49568847, 0.50494408, 0.43790788,\n",
       "       0.50823516, 0.449749  , 0.47635085, 0.47894296, 0.53035969,\n",
       "       0.50083478, 0.52808957, 0.53340805, 0.5175043 , 0.48612353,\n",
       "       0.47906715, 0.56503335, 0.50590133, 0.47692425, 0.55166848,\n",
       "       0.43576624, 0.46749093, 0.47874292, 0.48370525, 0.52162055,\n",
       "       0.5297683 , 0.44472976, 0.48798937, 0.53665266, 0.5442148 ,\n",
       "       0.48758421, 0.52792147, 0.5171751 , 0.51474004, 0.46983029,\n",
       "       0.48245462, 0.48168145, 0.47517736, 0.59854118, 0.59356631,\n",
       "       0.47045867, 0.48528687, 0.48086473, 0.48452355, 0.5020835 ,\n",
       "       0.50279648, 0.49052712, 0.46112396, 0.47737489, 0.43576624,\n",
       "       0.52484274, 0.4667664 , 0.48473785, 0.50394761, 0.49847606,\n",
       "       0.49405473, 0.48958727, 0.48566312, 0.50115474, 0.52556977])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicttest = loaded_model.predict(X_)\n",
    "predicttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['predicttest']=pd.DataFrame(predicttest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=df3.drop(['score','features'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>emotion</th>\n",
       "      <th>predicttest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10941</td>\n",
       "      <td>[point, today, someone, says, something, remot...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.525333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10942</td>\n",
       "      <td>[game, day, minus, #relentless]</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.446902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10943</td>\n",
       "      <td>[game, pissed, game, year, blood, boiling, tim...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.509832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10944</td>\n",
       "      <td>[ive, found, candice, candace, pout, likes]</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.480763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10945</td>\n",
       "      <td>[cant, come, muma, th, tweets, #soreloser]</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.506300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>11696</td>\n",
       "      <td>[supposed, animosity, bullshit, con, iranians]</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.494055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>756</th>\n",
       "      <td>11697</td>\n",
       "      <td>[byus, offense, score, vs, wvu]</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.489587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757</th>\n",
       "      <td>11698</td>\n",
       "      <td>[id, love, gyimah, action, coach, holding, gru...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.485663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>11699</td>\n",
       "      <td>[forgiving, means, operating, gods, spirit, am...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.501155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>11700</td>\n",
       "      <td>[ive, got, lot, tokens, saved, wanna, spam, ev...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.525570</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>760 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id                                              tweet emotion  \\\n",
       "0    10941  [point, today, someone, says, something, remot...   anger   \n",
       "1    10942                    [game, day, minus, #relentless]   anger   \n",
       "2    10943  [game, pissed, game, year, blood, boiling, tim...   anger   \n",
       "3    10944        [ive, found, candice, candace, pout, likes]   anger   \n",
       "4    10945         [cant, come, muma, th, tweets, #soreloser]   anger   \n",
       "..     ...                                                ...     ...   \n",
       "755  11696     [supposed, animosity, bullshit, con, iranians]   anger   \n",
       "756  11697                    [byus, offense, score, vs, wvu]   anger   \n",
       "757  11698  [id, love, gyimah, action, coach, holding, gru...   anger   \n",
       "758  11699  [forgiving, means, operating, gods, spirit, am...   anger   \n",
       "759  11700  [ive, got, lot, tokens, saved, wanna, spam, ev...   anger   \n",
       "\n",
       "     predicttest  \n",
       "0       0.525333  \n",
       "1       0.446902  \n",
       "2       0.509832  \n",
       "3       0.480763  \n",
       "4       0.506300  \n",
       "..           ...  \n",
       "755     0.494055  \n",
       "756     0.489587  \n",
       "757     0.485663  \n",
       "758     0.501155  \n",
       "759     0.525570  \n",
       "\n",
       "[760 rows x 4 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
