{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1= pd.read_csv(r\"C:\\Users\\i\\Downloads\\fear-ratings-0to1.train.txt\", delimiter='\\t', header=None)\n",
    "df1.columns = ['Id', 'tweet', 'emotion', 'score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Id                                              tweet emotion  score\n",
      "0     20000  I feel like I am drowning. #depression #anxiet...    fear  0.979\n",
      "1     20001  I get so nervous even thinking about talking t...    fear  0.979\n",
      "2     20002                     I lost my blinders .... #panic    fear  0.975\n",
      "3     20003  I feel like I am drowning. #depression  #falur...    fear  0.938\n",
      "4     20004  This is the scariest American Horror Story out...    fear  0.938\n",
      "...     ...                                                ...     ...    ...\n",
      "1142  21142     Pull over #tonight and make your car #shake ðŸ˜‹ðŸ’¦    fear  0.104\n",
      "1143  21143  @Melanie_Pierce @HunterHayes awe ain't he a sw...    fear  0.083\n",
      "1144  21144         @FraserKeegan just had a steak pie supper     fear  0.083\n",
      "1145  21145      @annalisewrobel_ awe thank you so much love ðŸ’•    fear  0.062\n",
      "1146  21146                             Omg he kissed herðŸ™ˆ  #w    fear  0.062\n",
      "\n",
      "[1147 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import emoji\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", tweet)\n",
    "    \n",
    "    return tweet\n",
    "df1['tweet'] = df1['tweet'].apply(preprocess_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_mentions(tweet):\n",
    "    return re.sub(r'@\\w+', '', tweet)\n",
    "\n",
    "df1['tweet'] = df1['tweet'].apply(remove_mentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s#@]', '', text) \n",
    "    text = re.sub(r'\\d+', '', text)  \n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "df1['tweet'] = df1['tweet'].apply(clean_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>emotion</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20000</td>\n",
       "      <td>i feel like i am drowning #depression #anxiety...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20001</td>\n",
       "      <td>i get so nervous even thinking about talking t...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20002</td>\n",
       "      <td>i lost my blinders #panic</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20003</td>\n",
       "      <td>i feel like i am drowning #depression #falure ...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20004</td>\n",
       "      <td>this is the scariest american horror story out...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1142</th>\n",
       "      <td>21142</td>\n",
       "      <td>pull over #tonight and make your car #shake</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143</th>\n",
       "      <td>21143</td>\n",
       "      <td>awe aint he a sweetheart hes adorable</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1144</th>\n",
       "      <td>21144</td>\n",
       "      <td>just had a steak pie supper</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1145</th>\n",
       "      <td>21145</td>\n",
       "      <td>awe thank you so much love</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1146</th>\n",
       "      <td>21146</td>\n",
       "      <td>omg he kissed her #w</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1147 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Id                                              tweet emotion  score\n",
       "0     20000  i feel like i am drowning #depression #anxiety...    fear  0.979\n",
       "1     20001  i get so nervous even thinking about talking t...    fear  0.979\n",
       "2     20002                          i lost my blinders #panic    fear  0.975\n",
       "3     20003  i feel like i am drowning #depression #falure ...    fear  0.938\n",
       "4     20004  this is the scariest american horror story out...    fear  0.938\n",
       "...     ...                                                ...     ...    ...\n",
       "1142  21142        pull over #tonight and make your car #shake    fear  0.104\n",
       "1143  21143              awe aint he a sweetheart hes adorable    fear  0.083\n",
       "1144  21144                        just had a steak pie supper    fear  0.083\n",
       "1145  21145                         awe thank you so much love    fear  0.062\n",
       "1146  21146                               omg he kissed her #w    fear  0.062\n",
       "\n",
       "[1147 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_emoji(tweet):\n",
    "    text = emoji.demojize(tweet)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['tweet'] = df1['tweet'].apply(convert_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_tweets(tweet):\n",
    "    tokenizer = TweetTokenizer()\n",
    "    return tokenizer.tokenize(tweet)\n",
    "\n",
    "df1['tweet'] = df1['tweet'].apply(tokenize_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>emotion</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20000</td>\n",
       "      <td>[i, feel, like, i, am, drowning, #depression, ...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20001</td>\n",
       "      <td>[i, get, so, nervous, even, thinking, about, t...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20002</td>\n",
       "      <td>[i, lost, my, blinders, #panic]</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20003</td>\n",
       "      <td>[i, feel, like, i, am, drowning, #depression, ...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20004</td>\n",
       "      <td>[this, is, the, scariest, american, horror, st...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1142</th>\n",
       "      <td>21142</td>\n",
       "      <td>[pull, over, #tonight, and, make, your, car, #...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143</th>\n",
       "      <td>21143</td>\n",
       "      <td>[awe, aint, he, a, sweetheart, hes, adorable]</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1144</th>\n",
       "      <td>21144</td>\n",
       "      <td>[just, had, a, steak, pie, supper]</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1145</th>\n",
       "      <td>21145</td>\n",
       "      <td>[awe, thank, you, so, much, love]</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1146</th>\n",
       "      <td>21146</td>\n",
       "      <td>[omg, he, kissed, her, #, w]</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1147 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Id                                              tweet emotion  score\n",
       "0     20000  [i, feel, like, i, am, drowning, #depression, ...    fear  0.979\n",
       "1     20001  [i, get, so, nervous, even, thinking, about, t...    fear  0.979\n",
       "2     20002                    [i, lost, my, blinders, #panic]    fear  0.975\n",
       "3     20003  [i, feel, like, i, am, drowning, #depression, ...    fear  0.938\n",
       "4     20004  [this, is, the, scariest, american, horror, st...    fear  0.938\n",
       "...     ...                                                ...     ...    ...\n",
       "1142  21142  [pull, over, #tonight, and, make, your, car, #...    fear  0.104\n",
       "1143  21143      [awe, aint, he, a, sweetheart, hes, adorable]    fear  0.083\n",
       "1144  21144                 [just, had, a, steak, pie, supper]    fear  0.083\n",
       "1145  21145                  [awe, thank, you, so, much, love]    fear  0.062\n",
       "1146  21146                       [omg, he, kissed, her, #, w]    fear  0.062\n",
       "\n",
       "[1147 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\n",
    "    'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from', 'has', 'he', 'in', 'is', 'it', 'its',\n",
    "    'of', 'on', 'that', 'the', 'to', 'was', 'were', 'will', 'with', 'i', 'you', 'your', 'so', 'all',\n",
    "    'about', 'above', 'after', 'again', 'against', 'ain', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\",\n",
    "    'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can',\n",
    "    'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\",\n",
    "    'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\",\n",
    "    'have', 'haven', \"haven't\", 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how',\n",
    "    'i', 'if', 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
    "    'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no',\n",
    "    'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves',\n",
    "    'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she's\", 'should', \"should've\", 'shouldn',\n",
    "    \"shouldn't\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them',\n",
    "    'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until',\n",
    "    'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', 'were', 'weren', \"weren't\", 'what', 'when', 'where',\n",
    "    'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you',\n",
    "    \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n",
    "def remove_stopwords(tokens):\n",
    "    filtered_tokens = [token for token in tokens if len(token) > 1 and token.lower() not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "df1['tweet'] = df1['tweet'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>emotion</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20000</td>\n",
       "      <td>[feel, like, drowning, #depression, #anxiety, ...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20001</td>\n",
       "      <td>[get, nervous, even, thinking, talking, wanna,...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20002</td>\n",
       "      <td>[lost, blinders, #panic]</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20003</td>\n",
       "      <td>[feel, like, drowning, #depression, #falure, #...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20004</td>\n",
       "      <td>[scariest, american, horror, story, im, gonna,...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1142</th>\n",
       "      <td>21142</td>\n",
       "      <td>[pull, #tonight, make, car, #shake]</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143</th>\n",
       "      <td>21143</td>\n",
       "      <td>[awe, aint, sweetheart, hes, adorable]</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1144</th>\n",
       "      <td>21144</td>\n",
       "      <td>[steak, pie, supper]</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1145</th>\n",
       "      <td>21145</td>\n",
       "      <td>[awe, thank, much, love]</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1146</th>\n",
       "      <td>21146</td>\n",
       "      <td>[omg, kissed]</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1147 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Id                                              tweet emotion  score\n",
       "0     20000  [feel, like, drowning, #depression, #anxiety, ...    fear  0.979\n",
       "1     20001  [get, nervous, even, thinking, talking, wanna,...    fear  0.979\n",
       "2     20002                           [lost, blinders, #panic]    fear  0.975\n",
       "3     20003  [feel, like, drowning, #depression, #falure, #...    fear  0.938\n",
       "4     20004  [scariest, american, horror, story, im, gonna,...    fear  0.938\n",
       "...     ...                                                ...     ...    ...\n",
       "1142  21142                [pull, #tonight, make, car, #shake]    fear  0.104\n",
       "1143  21143             [awe, aint, sweetheart, hes, adorable]    fear  0.083\n",
       "1144  21144                               [steak, pie, supper]    fear  0.083\n",
       "1145  21145                           [awe, thank, much, love]    fear  0.062\n",
       "1146  21146                                      [omg, kissed]    fear  0.062\n",
       "\n",
       "[1147 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "def extract_features(tweet):\n",
    "    tokenized_text = ' '.join(tweet)\n",
    "    input_ids = torch.tensor(tokenizer.encode(tokenized_text, add_special_tokens=True)).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    features = last_hidden_states.squeeze(0).numpy()\n",
    "    \n",
    "    return features\n",
    "\n",
    "df1['features'] = df1['tweet'].apply(extract_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>emotion</th>\n",
       "      <th>score</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>20481</td>\n",
       "      <td>[peoples, deepest, passions, often, scare, muc...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.521</td>\n",
       "      <td>[[-0.10522045, 0.3467327, -0.115954235, 0.0466...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>20779</td>\n",
       "      <td>[worry, emphasis, keeping, family, together, g...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.396</td>\n",
       "      <td>[[0.02802974, 0.16173235, -0.29791534, -0.2053...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1111</th>\n",
       "      <td>21111</td>\n",
       "      <td>[mattmilne, thank, letting, us, know, please, ...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.146</td>\n",
       "      <td>[[-0.44774464, 0.15478161, 0.22762844, -0.2787...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713</th>\n",
       "      <td>20713</td>\n",
       "      <td>[says, teufel, reassigned, within, organizatio...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.417</td>\n",
       "      <td>[[-0.993639, -0.15127987, -0.2219348, -0.25596...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>20753</td>\n",
       "      <td>[follow, amazing, australian, author, #fiction...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.396</td>\n",
       "      <td>[[-0.003132152, 0.0018000547, -0.03818727, 0.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>20147</td>\n",
       "      <td>[go, back, weeks, start, seriously, dreadful]</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.729</td>\n",
       "      <td>[[0.18815905, 0.42615047, -0.043639872, 0.1192...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>20327</td>\n",
       "      <td>[overtime, #teamna, #wch, #nervous]</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.604</td>\n",
       "      <td>[[-0.74993813, -0.17208055, -0.07791391, -0.13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>20432</td>\n",
       "      <td>[terrorism, intimidate, populace, case, held, ...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.562</td>\n",
       "      <td>[[0.16953777, 0.2876624, -0.24679744, -0.06413...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>20885</td>\n",
       "      <td>[ending, met, mother, dreadful]</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.333</td>\n",
       "      <td>[[-0.22394809, -0.0057124486, -0.115094736, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>21000</td>\n",
       "      <td>[everywhere, go, air, breathe, tastes, like, h...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.271</td>\n",
       "      <td>[[-0.25973818, 0.3977982, 0.30027336, -0.37580...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1147 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Id                                              tweet emotion  score  \\\n",
       "481   20481  [peoples, deepest, passions, often, scare, muc...    fear  0.521   \n",
       "779   20779  [worry, emphasis, keeping, family, together, g...    fear  0.396   \n",
       "1111  21111  [mattmilne, thank, letting, us, know, please, ...    fear  0.146   \n",
       "713   20713  [says, teufel, reassigned, within, organizatio...    fear  0.417   \n",
       "753   20753  [follow, amazing, australian, author, #fiction...    fear  0.396   \n",
       "...     ...                                                ...     ...    ...   \n",
       "147   20147      [go, back, weeks, start, seriously, dreadful]    fear  0.729   \n",
       "327   20327                [overtime, #teamna, #wch, #nervous]    fear  0.604   \n",
       "432   20432  [terrorism, intimidate, populace, case, held, ...    fear  0.562   \n",
       "885   20885                    [ending, met, mother, dreadful]    fear  0.333   \n",
       "1000  21000  [everywhere, go, air, breathe, tastes, like, h...    fear  0.271   \n",
       "\n",
       "                                               features  \n",
       "481   [[-0.10522045, 0.3467327, -0.115954235, 0.0466...  \n",
       "779   [[0.02802974, 0.16173235, -0.29791534, -0.2053...  \n",
       "1111  [[-0.44774464, 0.15478161, 0.22762844, -0.2787...  \n",
       "713   [[-0.993639, -0.15127987, -0.2219348, -0.25596...  \n",
       "753   [[-0.003132152, 0.0018000547, -0.03818727, 0.2...  \n",
       "...                                                 ...  \n",
       "147   [[0.18815905, 0.42615047, -0.043639872, 0.1192...  \n",
       "327   [[-0.74993813, -0.17208055, -0.07791391, -0.13...  \n",
       "432   [[0.16953777, 0.2876624, -0.24679744, -0.06413...  \n",
       "885   [[-0.22394809, -0.0057124486, -0.115094736, -0...  \n",
       "1000  [[-0.25973818, 0.3977982, 0.30027336, -0.37580...  \n",
       "\n",
       "[1147 rows x 5 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "df1 = shuffle(df1)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 768)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.features[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>emotion</th>\n",
       "      <th>score</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>20481</td>\n",
       "      <td>[peoples, deepest, passions, often, scare, muc...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.521</td>\n",
       "      <td>[[-0.10522045, 0.3467327, -0.115954235, 0.0466...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>20779</td>\n",
       "      <td>[worry, emphasis, keeping, family, together, g...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.396</td>\n",
       "      <td>[[0.02802974, 0.16173235, -0.29791534, -0.2053...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1111</th>\n",
       "      <td>21111</td>\n",
       "      <td>[mattmilne, thank, letting, us, know, please, ...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.146</td>\n",
       "      <td>[[-0.44774464, 0.15478161, 0.22762844, -0.2787...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713</th>\n",
       "      <td>20713</td>\n",
       "      <td>[says, teufel, reassigned, within, organizatio...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.417</td>\n",
       "      <td>[[-0.993639, -0.15127987, -0.2219348, -0.25596...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>20753</td>\n",
       "      <td>[follow, amazing, australian, author, #fiction...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.396</td>\n",
       "      <td>[[-0.003132152, 0.0018000547, -0.03818727, 0.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>20125</td>\n",
       "      <td>[focal, points, war, lie, #terrorism, #un, nee...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.750</td>\n",
       "      <td>[[-0.3313101, 0.19597748, -0.13628939, -0.1737...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>20939</td>\n",
       "      <td>[walk, right, see, way, past, dont, even, hesi...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.312</td>\n",
       "      <td>[[-0.12020782, 0.26181188, -0.15412205, -0.371...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>20320</td>\n",
       "      <td>[first, take, room, wanna, beat, #bully]</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.604</td>\n",
       "      <td>[[-0.29258454, -0.11945913, 0.15943666, -0.004...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>20358</td>\n",
       "      <td>[im, shy, girl]</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.583</td>\n",
       "      <td>[[-0.39446625, 0.20099491, -0.23982096, -0.095...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>20274</td>\n",
       "      <td>[librarians, scare]</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.646</td>\n",
       "      <td>[[-0.3052356, 0.12111623, -0.3220107, -0.06530...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Id                                              tweet emotion  score  \\\n",
       "481   20481  [peoples, deepest, passions, often, scare, muc...    fear  0.521   \n",
       "779   20779  [worry, emphasis, keeping, family, together, g...    fear  0.396   \n",
       "1111  21111  [mattmilne, thank, letting, us, know, please, ...    fear  0.146   \n",
       "713   20713  [says, teufel, reassigned, within, organizatio...    fear  0.417   \n",
       "753   20753  [follow, amazing, australian, author, #fiction...    fear  0.396   \n",
       "125   20125  [focal, points, war, lie, #terrorism, #un, nee...    fear  0.750   \n",
       "939   20939  [walk, right, see, way, past, dont, even, hesi...    fear  0.312   \n",
       "320   20320           [first, take, room, wanna, beat, #bully]    fear  0.604   \n",
       "358   20358                                    [im, shy, girl]    fear  0.583   \n",
       "274   20274                                [librarians, scare]    fear  0.646   \n",
       "\n",
       "                                               features  \n",
       "481   [[-0.10522045, 0.3467327, -0.115954235, 0.0466...  \n",
       "779   [[0.02802974, 0.16173235, -0.29791534, -0.2053...  \n",
       "1111  [[-0.44774464, 0.15478161, 0.22762844, -0.2787...  \n",
       "713   [[-0.993639, -0.15127987, -0.2219348, -0.25596...  \n",
       "753   [[-0.003132152, 0.0018000547, -0.03818727, 0.2...  \n",
       "125   [[-0.3313101, 0.19597748, -0.13628939, -0.1737...  \n",
       "939   [[-0.12020782, 0.26181188, -0.15412205, -0.371...  \n",
       "320   [[-0.29258454, -0.11945913, 0.15943666, -0.004...  \n",
       "358   [[-0.39446625, 0.20099491, -0.23982096, -0.095...  \n",
       "274   [[-0.3052356, 0.12111623, -0.3220107, -0.06530...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "features = df1['features'].tolist()\n",
    "padded_features = pad_sequences(features, padding='post')\n",
    "padded_df = df1.copy()\n",
    "padded_df['features'] = padded_features.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.stack(padded_df['features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (1147, 41, 768)\n",
      "Output shape: (1147,)\n"
     ]
    }
   ],
   "source": [
    "y = np.array(padded_df['score'])    \n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.reshape(x, (1147, 41 * 768))  \n",
    "y = np.reshape(y, (1147,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.521, 0.396, 0.146, ..., 0.562, 0.333, 0.271])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=x.copy()  \n",
    "Y=y.copy()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=2000)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "alpha = 2000\n",
    "model = Ridge(alpha=alpha)\n",
    "model.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.024659146675577955\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X)\n",
    "mse = mean_squared_error(Y, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.12857036427267948\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mae = mean_absolute_error(Y, y_pred)\n",
    "\n",
    "print(\"Mean Absolute Error:\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['regression_model.pkl']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "save_path = 'regression_model.pkl'\n",
    "joblib.dump(model, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2= pd.read_csv(r\"C:\\Users\\i\\Downloads\\fear-ratings-0to1.dev.gold.txt\", delimiter='\\t', header=None)\n",
    "df2.columns = ['Id', 'tweet', 'emotion', 'score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>emotion</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21147</td>\n",
       "      <td>I know this is going to be one of those nights...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21148</td>\n",
       "      <td>This is #horrible: Lewis Dunk has begun networ...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21149</td>\n",
       "      <td>@JeffersonLake speaking of ex cobblers, saw Ri...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21150</td>\n",
       "      <td>@1johndes ball watching &amp;amp; Rojo'd header wa...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21151</td>\n",
       "      <td>Really.....#Jumanji 2....w/ The Rock, Jack Bla...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>21252</td>\n",
       "      <td>Staff on @ryainair FR1005. Asked for info and ...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>21253</td>\n",
       "      <td>Staff on @ryainair FR1005. Asked for info and ...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>21254</td>\n",
       "      <td>An adviser to the #European #Unionâ€™s top #cour...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>21255</td>\n",
       "      <td>So about 18mths ago i signed up to @Lumo_Energ...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>21256</td>\n",
       "      <td>So about 18mths ago i signed up to @Lumo_Energ...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.271</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id                                              tweet emotion  score\n",
       "0    21147  I know this is going to be one of those nights...    fear  0.771\n",
       "1    21148  This is #horrible: Lewis Dunk has begun networ...    fear  0.479\n",
       "2    21149  @JeffersonLake speaking of ex cobblers, saw Ri...    fear  0.417\n",
       "3    21150  @1johndes ball watching &amp; Rojo'd header wa...    fear  0.475\n",
       "4    21151  Really.....#Jumanji 2....w/ The Rock, Jack Bla...    fear  0.542\n",
       "..     ...                                                ...     ...    ...\n",
       "105  21252  Staff on @ryainair FR1005. Asked for info and ...    fear  0.312\n",
       "106  21253  Staff on @ryainair FR1005. Asked for info and ...    fear  0.271\n",
       "107  21254  An adviser to the #European #Unionâ€™s top #cour...    fear  0.500\n",
       "108  21255  So about 18mths ago i signed up to @Lumo_Energ...    fear  0.479\n",
       "109  21256  So about 18mths ago i signed up to @Lumo_Energ...    fear  0.271\n",
       "\n",
       "[110 rows x 4 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", tweet)\n",
    "    \n",
    "    return tweet\n",
    "df2['tweet'] = df2['tweet'].apply(preprocess_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_mentions(tweet):\n",
    "    return re.sub(r'@\\w+', '', tweet)\n",
    "\n",
    "df2['tweet'] = df2['tweet'].apply(remove_mentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s#@]', '', text)  \n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "df2['tweet'] = df2['tweet'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_emoji(tweet):\n",
    "    text = emoji.demojize(tweet)\n",
    "    return text\n",
    "df2['tweet'] = df2['tweet'].apply(convert_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_tweets(tweet):\n",
    "    tokenizer = TweetTokenizer()\n",
    "    return tokenizer.tokenize(tweet)\n",
    "\n",
    "df2['tweet'] = df2['tweet'].apply(tokenize_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\n",
    "    'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from', 'has', 'he', 'in', 'is', 'it', 'its',\n",
    "    'of', 'on', 'that', 'the', 'to', 'was', 'were', 'will', 'with', 'i', 'you', 'your', 'so', 'all',\n",
    "    'about', 'above', 'after', 'again', 'against', 'ain', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\",\n",
    "    'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can',\n",
    "    'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\",\n",
    "    'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\",\n",
    "    'have', 'haven', \"haven't\", 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how',\n",
    "    'i', 'if', 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
    "    'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no',\n",
    "    'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves',\n",
    "    'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she's\", 'should', \"should've\", 'shouldn',\n",
    "    \"shouldn't\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them',\n",
    "    'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until',\n",
    "    'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', 'were', 'weren', \"weren't\", 'what', 'when', 'where',\n",
    "    'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you',\n",
    "    \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n",
    "\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    filtered_tokens = [token for token in tokens if len(token) > 1 and token.lower() not in stop_words]\n",
    "    return filtered_tokens\n",
    "df2['tweet'] = df2['tweet'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "def extract_features(tweet):\n",
    "    \n",
    "    tokenized_text = ' '.join(tweet)\n",
    "    input_ids = torch.tensor(tokenizer.encode(tokenized_text, add_special_tokens=True)).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    features = last_hidden_states.squeeze(0).numpy()\n",
    "    \n",
    "    return features\n",
    "df2['features'] = df2['tweet'].apply(extract_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "features2 = df2['features'].tolist()\n",
    "padded_features2 = pad_sequences(features2, padding='post')\n",
    "padded_df2 = df2.copy()\n",
    "padded_df2['features'] = padded_features2.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input feature shape: (110, 40, 768)\n"
     ]
    }
   ],
   "source": [
    "X = np.stack(padded_df2['features'])\n",
    "print('Input feature shape:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input feature shape: (110, 40, 768)\n"
     ]
    }
   ],
   "source": [
    "print('Input feature shape:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "max_sequence_length=41\n",
    "X_=pad_sequences(X,maxlen=max_sequence_length,padding=\"post\",truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = np.reshape(X_, (110, 41 * 768))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_path = 'regression_model.pkl'\n",
    "loaded_model = joblib.load(saved_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.47317431, 0.51554596, 0.51838605, 0.48937823, 0.46480545,\n",
       "       0.4940073 , 0.46339023, 0.46041275, 0.51100973, 0.57354942,\n",
       "       0.54629037, 0.50375776, 0.54503522, 0.51344387, 0.51606033,\n",
       "       0.50013619, 0.5203113 , 0.49215348, 0.47690336, 0.53233144,\n",
       "       0.45409032, 0.40398657, 0.53771556, 0.51697993, 0.5133957 ,\n",
       "       0.49163083, 0.5535666 , 0.52349757, 0.48908679, 0.49028713,\n",
       "       0.46914896, 0.49873005, 0.4455776 , 0.53374283, 0.52689141,\n",
       "       0.49410597, 0.53443457, 0.49314568, 0.50862165, 0.53405874,\n",
       "       0.49104613, 0.52270607, 0.47841745, 0.51828496, 0.45073503,\n",
       "       0.53704695, 0.50829522, 0.53490113, 0.49431941, 0.48801483,\n",
       "       0.50556369, 0.52735146, 0.50163971, 0.4270643 , 0.47559902,\n",
       "       0.48699087, 0.48636851, 0.48628448, 0.52353165, 0.50077538,\n",
       "       0.48705047, 0.4627847 , 0.38544998, 0.48306581, 0.50358646,\n",
       "       0.48745773, 0.47107445, 0.52047463, 0.49071267, 0.49484818,\n",
       "       0.51687043, 0.48300814, 0.49893933, 0.50645369, 0.51864361,\n",
       "       0.49968753, 0.4821455 , 0.53310495, 0.54176448, 0.53988291,\n",
       "       0.49361399, 0.46678334, 0.44554089, 0.4311031 , 0.4549834 ,\n",
       "       0.4869882 , 0.49041997, 0.52003413, 0.49285958, 0.54860168,\n",
       "       0.51854803, 0.45184422, 0.44934447, 0.48457573, 0.43779447,\n",
       "       0.40676456, 0.51076593, 0.42498079, 0.52562442, 0.51577312,\n",
       "       0.48155861, 0.45256486, 0.4126057 , 0.44393082, 0.48735081,\n",
       "       0.47311135, 0.49690663, 0.52656458, 0.50519151, 0.46255246])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = loaded_model.predict(X_)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['predict']=pd.DataFrame(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_square_error: 0.030899198410693333\n"
     ]
    }
   ],
   "source": [
    "mse=mean_squared_error(df2['score'],df2['predict'])\n",
    "print(\"mean_square_error:\",mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.1430721146543378\n"
     ]
    }
   ],
   "source": [
    "mae = mean_absolute_error(df2['score'],df2['predict'])\n",
    "print(\"Mean Absolute Error:\", mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>emotion</th>\n",
       "      <th>score</th>\n",
       "      <th>features</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21147</td>\n",
       "      <td>[know, going, one, nights, takes, act, god, fa...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.771</td>\n",
       "      <td>[[-0.3144856, 0.15769926, 0.43722436, -0.39113...</td>\n",
       "      <td>0.473174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21148</td>\n",
       "      <td>[#horrible, lewis, dunk, begun, networking, ne...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.479</td>\n",
       "      <td>[[-0.25642213, 0.11772158, 0.17571282, -0.0315...</td>\n",
       "      <td>0.515546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21149</td>\n",
       "      <td>[speaking, ex, cobblers, saw, ricky, holmes, c...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.417</td>\n",
       "      <td>[[-0.11143019, 0.11400332, 0.12179603, -0.3693...</td>\n",
       "      <td>0.518386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21150</td>\n",
       "      <td>[ball, watching, amp, rojod, header, equally, ...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.475</td>\n",
       "      <td>[[-0.43671992, 0.057505384, -0.19090268, -0.25...</td>\n",
       "      <td>0.489378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21151</td>\n",
       "      <td>[really, #jumanji, rock, jack, black, kevin, h...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.542</td>\n",
       "      <td>[[-0.3343676, 0.090099744, -0.1592446, -0.0180...</td>\n",
       "      <td>0.464805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>21252</td>\n",
       "      <td>[staff, fr, asked, info, told, look, online, g...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.312</td>\n",
       "      <td>[[-0.1450364, 0.06649841, 0.24776919, -0.07256...</td>\n",
       "      <td>0.473111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>21253</td>\n",
       "      <td>[staff, fr, asked, info, told, look, online, g...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.271</td>\n",
       "      <td>[[-0.14554901, 0.13766387, 0.14240709, -0.0481...</td>\n",
       "      <td>0.496907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>21254</td>\n",
       "      <td>[adviser, #european, #unions, top, #court, sai...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.500</td>\n",
       "      <td>[[-0.66618294, 0.1709193, -0.10170758, -0.2963...</td>\n",
       "      <td>0.526565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>21255</td>\n",
       "      <td>[mths, ago, signed, velocity, ff, deal, months...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.479</td>\n",
       "      <td>[[-0.97019094, -0.03449127, 0.4789242, -0.0327...</td>\n",
       "      <td>0.505192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>21256</td>\n",
       "      <td>[mths, ago, signed, velocity, ff, deal, months...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.271</td>\n",
       "      <td>[[-0.9431952, 0.07778479, 0.43429676, -0.16182...</td>\n",
       "      <td>0.462552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id                                              tweet emotion  score  \\\n",
       "0    21147  [know, going, one, nights, takes, act, god, fa...    fear  0.771   \n",
       "1    21148  [#horrible, lewis, dunk, begun, networking, ne...    fear  0.479   \n",
       "2    21149  [speaking, ex, cobblers, saw, ricky, holmes, c...    fear  0.417   \n",
       "3    21150  [ball, watching, amp, rojod, header, equally, ...    fear  0.475   \n",
       "4    21151  [really, #jumanji, rock, jack, black, kevin, h...    fear  0.542   \n",
       "..     ...                                                ...     ...    ...   \n",
       "105  21252  [staff, fr, asked, info, told, look, online, g...    fear  0.312   \n",
       "106  21253  [staff, fr, asked, info, told, look, online, g...    fear  0.271   \n",
       "107  21254  [adviser, #european, #unions, top, #court, sai...    fear  0.500   \n",
       "108  21255  [mths, ago, signed, velocity, ff, deal, months...    fear  0.479   \n",
       "109  21256  [mths, ago, signed, velocity, ff, deal, months...    fear  0.271   \n",
       "\n",
       "                                              features   predict  \n",
       "0    [[-0.3144856, 0.15769926, 0.43722436, -0.39113...  0.473174  \n",
       "1    [[-0.25642213, 0.11772158, 0.17571282, -0.0315...  0.515546  \n",
       "2    [[-0.11143019, 0.11400332, 0.12179603, -0.3693...  0.518386  \n",
       "3    [[-0.43671992, 0.057505384, -0.19090268, -0.25...  0.489378  \n",
       "4    [[-0.3343676, 0.090099744, -0.1592446, -0.0180...  0.464805  \n",
       "..                                                 ...       ...  \n",
       "105  [[-0.1450364, 0.06649841, 0.24776919, -0.07256...  0.473111  \n",
       "106  [[-0.14554901, 0.13766387, 0.14240709, -0.0481...  0.496907  \n",
       "107  [[-0.66618294, 0.1709193, -0.10170758, -0.2963...  0.526565  \n",
       "108  [[-0.97019094, -0.03449127, 0.4789242, -0.0327...  0.505192  \n",
       "109  [[-0.9431952, 0.07778479, 0.43429676, -0.16182...  0.462552  \n",
       "\n",
       "[110 rows x 6 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3= pd.read_csv(r\"C:\\Users\\i\\Downloads\\fear-ratings-0to1.test.target.txt\", delimiter='\\t', header=None)\n",
    "df3.columns = ['Id', 'tweet', 'emotion', 'score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", tweet)\n",
    "    \n",
    "    return tweet\n",
    "df3['tweet'] = df3['tweet'].apply(preprocess_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_mentions(tweet):\n",
    "    return re.sub(r'@\\w+', '', tweet)\n",
    "\n",
    "df3['tweet'] = df3['tweet'].apply(remove_mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s#@]', '', text)  \n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "df3['tweet'] = df3['tweet'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_emoji(tweet):\n",
    "    text = emoji.demojize(tweet)\n",
    "    return text\n",
    "df3['tweet'] = df3['tweet'].apply(convert_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_tweets(tweet):\n",
    "    tokenizer = TweetTokenizer()\n",
    "    return tokenizer.tokenize(tweet)\n",
    "\n",
    "df3['tweet'] = df3['tweet'].apply(tokenize_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\n",
    "    'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from', 'has', 'he', 'in', 'is', 'it', 'its',\n",
    "    'of', 'on', 'that', 'the', 'to', 'was', 'were', 'will', 'with', 'i', 'you', 'your', 'so', 'all',\n",
    "    'about', 'above', 'after', 'again', 'against', 'ain', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\",\n",
    "    'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can',\n",
    "    'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\",\n",
    "    'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\",\n",
    "    'have', 'haven', \"haven't\", 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how',\n",
    "    'i', 'if', 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
    "    'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no',\n",
    "    'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves',\n",
    "    'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she's\", 'should', \"should've\", 'shouldn',\n",
    "    \"shouldn't\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them',\n",
    "    'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until',\n",
    "    'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', 'were', 'weren', \"weren't\", 'what', 'when', 'where',\n",
    "    'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you',\n",
    "    \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n",
    "\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    filtered_tokens = [token for token in tokens if len(token) > 1 and token.lower() not in stop_words]\n",
    "    return filtered_tokens\n",
    "df3['tweet'] = df3['tweet'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "def extract_features(tweet):\n",
    "    \n",
    "    tokenized_text = ' '.join(tweet)\n",
    "    input_ids = torch.tensor(tokenizer.encode(tokenized_text, add_special_tokens=True)).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    features = last_hidden_states.squeeze(0).numpy()\n",
    "    \n",
    "    return features\n",
    "df3['features'] = df3['tweet'].apply(extract_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 768)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3['features'][713].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "features3 = df3['features'].tolist()\n",
    "padded_features3 = pad_sequences(features3, padding='post')\n",
    "padded_df3 = df3.copy()\n",
    "padded_df3['features'] = padded_features3.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input feature shape: (995, 38, 768)\n"
     ]
    }
   ],
   "source": [
    "X = np.stack(padded_df3['features'])\n",
    "print('Input feature shape:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "max_sequence_length=41\n",
    "X_=pad_sequences(X,maxlen=max_sequence_length,padding=\"post\",truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = np.reshape(X_, (995, 41 * 768))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_path = 'regression_model.pkl'\n",
    "loaded_model = joblib.load(saved_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.56163864, 0.51306322, 0.4911582 , 0.48265492, 0.50799626,\n",
       "       0.48488318, 0.55193031, 0.49922372, 0.53275937, 0.59387914,\n",
       "       0.53834522, 0.50110795, 0.51439792, 0.53761572, 0.5607375 ,\n",
       "       0.52954809, 0.49246717, 0.50880134, 0.44178479, 0.49924436,\n",
       "       0.46037391, 0.44879661, 0.49210093, 0.49768345, 0.5224496 ,\n",
       "       0.50648248, 0.51934193, 0.52158265, 0.47562154, 0.48426472,\n",
       "       0.43132165, 0.55888839, 0.52327494, 0.47467362, 0.45285242,\n",
       "       0.52749383, 0.48436272, 0.53138231, 0.58127494, 0.55932536,\n",
       "       0.4564952 , 0.5140102 , 0.51263206, 0.55578844, 0.51086681,\n",
       "       0.56617973, 0.45498749, 0.48492568, 0.51306675, 0.44088841,\n",
       "       0.51016353, 0.5084152 , 0.51109518, 0.52650817, 0.5334673 ,\n",
       "       0.53578055, 0.50776358, 0.48758918, 0.50693228, 0.45353677,\n",
       "       0.40191551, 0.53343769, 0.51720829, 0.52467952, 0.56076556,\n",
       "       0.53943856, 0.51978126, 0.45764097, 0.50910672, 0.483982  ,\n",
       "       0.43510129, 0.51312987, 0.5722104 , 0.45628035, 0.47976659,\n",
       "       0.53524626, 0.53075662, 0.50267277, 0.4973227 , 0.45018345,\n",
       "       0.51777441, 0.49338075, 0.55325229, 0.50707393, 0.48075   ,\n",
       "       0.4737537 , 0.54379244, 0.50932785, 0.52023865, 0.49244802,\n",
       "       0.52520396, 0.48736971, 0.46142068, 0.48824055, 0.55912246,\n",
       "       0.48605705, 0.49310732, 0.47365206, 0.50761612, 0.48323526,\n",
       "       0.45232599, 0.52982556, 0.52212095, 0.46828817, 0.52499717,\n",
       "       0.524043  , 0.46664758, 0.44291427, 0.43111212, 0.48807366,\n",
       "       0.55226571, 0.52385018, 0.50353889, 0.50975708, 0.52169316,\n",
       "       0.46530754, 0.50176679, 0.53283624, 0.52021458, 0.45815375,\n",
       "       0.47417938, 0.4978642 , 0.51208833, 0.50052679, 0.4232982 ,\n",
       "       0.4282966 , 0.49445907, 0.50079817, 0.42518884, 0.49043469,\n",
       "       0.43155749, 0.51330533, 0.45841242, 0.43801862, 0.54952743,\n",
       "       0.49849507, 0.53196331, 0.5310362 , 0.51637984, 0.45929461,\n",
       "       0.47334808, 0.49435268, 0.50108129, 0.49341716, 0.50512049,\n",
       "       0.48161142, 0.51906137, 0.46614066, 0.53378287, 0.49234184,\n",
       "       0.48133407, 0.49692095, 0.50275234, 0.56152742, 0.49959889,\n",
       "       0.46569052, 0.42218085, 0.46338492, 0.4632208 , 0.5407026 ,\n",
       "       0.45251425, 0.47940524, 0.49768281, 0.55649534, 0.56267679,\n",
       "       0.52907195, 0.50060098, 0.52036075, 0.49589067, 0.47062866,\n",
       "       0.52292353, 0.48653066, 0.48575353, 0.50461378, 0.47397405,\n",
       "       0.40672807, 0.53309255, 0.55182925, 0.55013198, 0.5126413 ,\n",
       "       0.45646392, 0.50495422, 0.49471868, 0.56504353, 0.52568782,\n",
       "       0.50509945, 0.49677825, 0.44536392, 0.4568178 , 0.47207411,\n",
       "       0.47560197, 0.47778573, 0.48055215, 0.48575973, 0.51319265,\n",
       "       0.55892095, 0.44769567, 0.49102117, 0.47247142, 0.5049696 ,\n",
       "       0.49373616, 0.52549376, 0.55885633, 0.5788833 , 0.50550848,\n",
       "       0.50376519, 0.51949631, 0.49490605, 0.48829348, 0.47778464,\n",
       "       0.53522635, 0.52277664, 0.50262056, 0.51275759, 0.52371356,\n",
       "       0.4990559 , 0.50374571, 0.47366192, 0.42493385, 0.46054843,\n",
       "       0.54111168, 0.52555752, 0.51415259, 0.47677078, 0.52380772,\n",
       "       0.48427148, 0.52405939, 0.58022642, 0.51598056, 0.60947929,\n",
       "       0.56256784, 0.55558485, 0.56104501, 0.50394897, 0.47889971,\n",
       "       0.47548034, 0.49320582, 0.49053095, 0.49013636, 0.42070479,\n",
       "       0.52108381, 0.49947751, 0.45878533, 0.50609935, 0.4896492 ,\n",
       "       0.49410082, 0.45637338, 0.56035213, 0.45560264, 0.51973394,\n",
       "       0.52359716, 0.52031205, 0.4753918 , 0.50934542, 0.46292228,\n",
       "       0.52627743, 0.51195742, 0.5162183 , 0.53455758, 0.48578459,\n",
       "       0.44053853, 0.53913944, 0.50550077, 0.51440335, 0.53876618,\n",
       "       0.45351423, 0.52607672, 0.40958118, 0.41098752, 0.45549235,\n",
       "       0.50749323, 0.4451647 , 0.45911591, 0.49338289, 0.45390738,\n",
       "       0.51871376, 0.53034609, 0.49173107, 0.53598925, 0.47158392,\n",
       "       0.5030401 , 0.48723984, 0.44338141, 0.53075164, 0.50757708,\n",
       "       0.51468417, 0.51396707, 0.52596024, 0.48741256, 0.49036923,\n",
       "       0.52387059, 0.53455828, 0.4947924 , 0.45016305, 0.53118004,\n",
       "       0.52196838, 0.51318185, 0.55897448, 0.46502086, 0.43231628,\n",
       "       0.54536509, 0.5223502 , 0.51650724, 0.41068844, 0.47236611,\n",
       "       0.53216991, 0.54103894, 0.39996296, 0.50535868, 0.48730406,\n",
       "       0.50121021, 0.51576104, 0.5102989 , 0.48644021, 0.4831572 ,\n",
       "       0.45349937, 0.5402849 , 0.48613086, 0.52615794, 0.46785591,\n",
       "       0.46719739, 0.51356963, 0.48516225, 0.4975397 , 0.51105304,\n",
       "       0.46204746, 0.463877  , 0.46629468, 0.48221207, 0.53469727,\n",
       "       0.52126947, 0.51523393, 0.52423008, 0.51384928, 0.54781382,\n",
       "       0.48851089, 0.45599203, 0.50133779, 0.46837227, 0.40068329,\n",
       "       0.43508141, 0.46528117, 0.45013912, 0.51679443, 0.54115949,\n",
       "       0.55007047, 0.47603173, 0.50172472, 0.44905809, 0.48895378,\n",
       "       0.54369825, 0.51205049, 0.41222227, 0.48280559, 0.49348297,\n",
       "       0.49499559, 0.48700982, 0.50189654, 0.49121491, 0.50595562,\n",
       "       0.53140834, 0.52523361, 0.49798643, 0.5016031 , 0.51482316,\n",
       "       0.54543522, 0.50776108, 0.4310623 , 0.45358327, 0.49157691,\n",
       "       0.48815667, 0.48902833, 0.54374538, 0.54219567, 0.51825011,\n",
       "       0.47368456, 0.51574469, 0.51842484, 0.49967735, 0.50996724,\n",
       "       0.51676445, 0.44632074, 0.50793832, 0.51176797, 0.47755086,\n",
       "       0.46207152, 0.54890715, 0.51405904, 0.54324747, 0.47854476,\n",
       "       0.48258635, 0.52840679, 0.43016399, 0.50765046, 0.45681461,\n",
       "       0.50463548, 0.50742055, 0.47979851, 0.50385574, 0.51807792,\n",
       "       0.52747385, 0.53252299, 0.51694479, 0.56469667, 0.54458227,\n",
       "       0.48451304, 0.52018703, 0.49812963, 0.52286422, 0.51158132,\n",
       "       0.49809491, 0.52540856, 0.5269335 , 0.52868267, 0.49666223,\n",
       "       0.45872257, 0.46291975, 0.49535077, 0.45119133, 0.48311824,\n",
       "       0.5062309 , 0.47926698, 0.48892352, 0.45630809, 0.52207452,\n",
       "       0.46511231, 0.46213288, 0.47314311, 0.52893101, 0.53424614,\n",
       "       0.516797  , 0.58581483, 0.53358147, 0.4660685 , 0.43178291,\n",
       "       0.48352836, 0.52884399, 0.47356961, 0.43433456, 0.43728885,\n",
       "       0.44123633, 0.52775304, 0.51298097, 0.48046325, 0.47882313,\n",
       "       0.51294882, 0.56182084, 0.55389572, 0.46444048, 0.44631363,\n",
       "       0.47507248, 0.46295449, 0.46498232, 0.48566821, 0.46525782,\n",
       "       0.46428028, 0.44801024, 0.50463513, 0.51257386, 0.49000902,\n",
       "       0.51913356, 0.52296977, 0.52191083, 0.49948518, 0.47253085,\n",
       "       0.45841283, 0.47752156, 0.48244236, 0.54874885, 0.46444693,\n",
       "       0.56615615, 0.47454449, 0.4895925 , 0.48554176, 0.48598155,\n",
       "       0.4255879 , 0.4664662 , 0.49264438, 0.46322048, 0.5892081 ,\n",
       "       0.49806489, 0.42049308, 0.48506053, 0.47029556, 0.49879505,\n",
       "       0.53934758, 0.43599811, 0.47258378, 0.50050501, 0.42619511,\n",
       "       0.48730159, 0.50117827, 0.50217208, 0.49807479, 0.50074378,\n",
       "       0.50463155, 0.5075278 , 0.41521929, 0.48199896, 0.45635435,\n",
       "       0.41728605, 0.53185542, 0.58832834, 0.52905833, 0.52682296,\n",
       "       0.41891317, 0.48387572, 0.46918406, 0.48441141, 0.43289089,\n",
       "       0.48560092, 0.52009266, 0.48352461, 0.5240511 , 0.50001299,\n",
       "       0.49429769, 0.50709059, 0.52885514, 0.5362569 , 0.55972024,\n",
       "       0.55695989, 0.52172911, 0.49108322, 0.53153632, 0.51456382,\n",
       "       0.46517802, 0.50563575, 0.46681839, 0.48839777, 0.5239305 ,\n",
       "       0.54102993, 0.52516697, 0.4710856 , 0.41968333, 0.50255534,\n",
       "       0.50152261, 0.47821467, 0.46245123, 0.51121297, 0.46475715,\n",
       "       0.55176137, 0.48621066, 0.4959162 , 0.49494913, 0.54988658,\n",
       "       0.54476148, 0.49005367, 0.44226332, 0.48488642, 0.50804787,\n",
       "       0.5372424 , 0.52327618, 0.48401366, 0.47914303, 0.47656809,\n",
       "       0.45609958, 0.44291257, 0.47897098, 0.49949326, 0.51917741,\n",
       "       0.54508459, 0.44461709, 0.4491088 , 0.48760116, 0.50884811,\n",
       "       0.52643125, 0.53759403, 0.48519546, 0.55075937, 0.51520982,\n",
       "       0.47567291, 0.43696902, 0.53171178, 0.49268529, 0.53446233,\n",
       "       0.46998379, 0.55282873, 0.5337631 , 0.45015074, 0.52338558,\n",
       "       0.49369943, 0.49488426, 0.41303237, 0.46810496, 0.52409366,\n",
       "       0.56527651, 0.55058   , 0.48278873, 0.48571038, 0.47417233,\n",
       "       0.48850641, 0.48185111, 0.54113722, 0.49710536, 0.4922782 ,\n",
       "       0.4860753 , 0.48418783, 0.53739175, 0.52406222, 0.53601031,\n",
       "       0.54228961, 0.51721304, 0.51405028, 0.49210911, 0.50424849,\n",
       "       0.53150276, 0.53687994, 0.44425604, 0.56590567, 0.45114501,\n",
       "       0.50144823, 0.48773344, 0.52892042, 0.51280565, 0.54121959,\n",
       "       0.47658288, 0.50389948, 0.48137557, 0.53680117, 0.52089133,\n",
       "       0.48606991, 0.49045008, 0.52295234, 0.52541091, 0.47776082,\n",
       "       0.46697598, 0.49224448, 0.47760976, 0.52578403, 0.51498263,\n",
       "       0.479851  , 0.49531565, 0.52789847, 0.54952743, 0.4815557 ,\n",
       "       0.46664465, 0.48621313, 0.51019456, 0.47852705, 0.49579671,\n",
       "       0.48125486, 0.56115102, 0.49553226, 0.51325698, 0.49332085,\n",
       "       0.45680133, 0.50145984, 0.48109296, 0.47227467, 0.51805983,\n",
       "       0.53204132, 0.44822379, 0.50178496, 0.53134163, 0.53396178,\n",
       "       0.46131966, 0.48702598, 0.5096936 , 0.51221992, 0.5399126 ,\n",
       "       0.4920213 , 0.50874702, 0.46736628, 0.49210563, 0.49838187,\n",
       "       0.46112851, 0.47915375, 0.54563226, 0.54544291, 0.52606777,\n",
       "       0.41211768, 0.4752378 , 0.51087832, 0.48328556, 0.50654583,\n",
       "       0.50390528, 0.48920701, 0.44955202, 0.43197856, 0.47320124,\n",
       "       0.49231139, 0.49051807, 0.51766218, 0.47819627, 0.52052555,\n",
       "       0.47463889, 0.48355805, 0.4745055 , 0.54368081, 0.49830865,\n",
       "       0.58638041, 0.57921415, 0.46547003, 0.50252901, 0.49375058,\n",
       "       0.46770131, 0.46249923, 0.49902069, 0.49376513, 0.4564016 ,\n",
       "       0.46937966, 0.54744976, 0.48588527, 0.52519754, 0.49329547,\n",
       "       0.43219831, 0.46007827, 0.48168246, 0.5015834 , 0.56394273,\n",
       "       0.4660264 , 0.44274297, 0.55486264, 0.41962767, 0.48494464,\n",
       "       0.48177184, 0.47628799, 0.49965952, 0.4492017 , 0.46051622,\n",
       "       0.5136858 , 0.47119956, 0.49932264, 0.49847365, 0.49045769,\n",
       "       0.4954799 , 0.55901097, 0.5338241 , 0.50942842, 0.51265286,\n",
       "       0.5215514 , 0.5114309 , 0.53009868, 0.52911219, 0.49063517,\n",
       "       0.54208524, 0.59498124, 0.58302193, 0.46669977, 0.51445139,\n",
       "       0.58497187, 0.53692632, 0.52993515, 0.47112071, 0.46513285,\n",
       "       0.44404457, 0.52182176, 0.47439786, 0.51818553, 0.52359716,\n",
       "       0.45350637, 0.49166021, 0.53899037, 0.53283536, 0.49096294,\n",
       "       0.56128542, 0.48502936, 0.534278  , 0.46634757, 0.47014774,\n",
       "       0.43700793, 0.48934156, 0.5089776 , 0.46320132, 0.49590813,\n",
       "       0.45013912, 0.51596588, 0.51839677, 0.40075413, 0.55197178,\n",
       "       0.56038129, 0.5073652 , 0.5094395 , 0.47782518, 0.4722309 ,\n",
       "       0.4875048 , 0.45981688, 0.559207  , 0.54629116, 0.45652576,\n",
       "       0.48042769, 0.4618387 , 0.49025513, 0.50951672, 0.47446985,\n",
       "       0.55462295, 0.54595204, 0.50507879, 0.55453419, 0.59446616,\n",
       "       0.44472909, 0.50748483, 0.53099083, 0.49752707, 0.49950575,\n",
       "       0.47938909, 0.54701201, 0.53147757, 0.54437524, 0.48917598,\n",
       "       0.60871446, 0.50939755, 0.52138752, 0.517491  , 0.4451647 ,\n",
       "       0.51839956, 0.55914975, 0.50907188, 0.48116543, 0.49048845,\n",
       "       0.44592176, 0.49604741, 0.50630246, 0.55785208, 0.48083138,\n",
       "       0.44570172, 0.51869377, 0.46348714, 0.50907202, 0.50486456,\n",
       "       0.50445327, 0.55486228, 0.52615323, 0.49786801, 0.45525951,\n",
       "       0.52324547, 0.52909007, 0.53292554, 0.49923645, 0.4942549 ,\n",
       "       0.45717027, 0.53174162, 0.54360032, 0.52696304, 0.51487403,\n",
       "       0.46399784, 0.50177257, 0.44885954, 0.52497121, 0.52241612,\n",
       "       0.49228536, 0.46859233, 0.4640316 , 0.50937934, 0.52000342,\n",
       "       0.47808064, 0.44271016, 0.47188592, 0.49343114, 0.46327683,\n",
       "       0.48131365, 0.49387617, 0.48968343, 0.49133034, 0.49639451,\n",
       "       0.5354638 , 0.48309363, 0.49150803, 0.49267633, 0.55114767,\n",
       "       0.55290673, 0.5148576 , 0.49517609, 0.49494964, 0.47419855,\n",
       "       0.4801749 , 0.50737129, 0.52809685, 0.51330824, 0.49397788,\n",
       "       0.48928851, 0.51077604, 0.5114039 , 0.46933668, 0.3990139 ,\n",
       "       0.50411534, 0.53035008, 0.51796695, 0.47850465, 0.47355432,\n",
       "       0.54112507, 0.47514785, 0.49429416, 0.53844806, 0.50725823,\n",
       "       0.46058908, 0.50372864, 0.50372864, 0.50005155, 0.45810013,\n",
       "       0.48979094, 0.47708561, 0.50387183, 0.46048283, 0.5217693 ,\n",
       "       0.52838478, 0.53912396, 0.52395294, 0.49126677, 0.48113506,\n",
       "       0.47749357, 0.46523742, 0.50916435, 0.48490332, 0.51445481,\n",
       "       0.48735787, 0.49335341, 0.50574554, 0.52688233, 0.44590952,\n",
       "       0.40840575, 0.47271556, 0.49742194, 0.43220895, 0.51012506,\n",
       "       0.49376815, 0.52457317, 0.45491171, 0.49978243, 0.5191575 ,\n",
       "       0.49132046, 0.53108255, 0.50395378, 0.4969161 , 0.51650736,\n",
       "       0.49031697, 0.44921835, 0.47264033, 0.45436351, 0.49056711,\n",
       "       0.42693205, 0.45885464, 0.52693058, 0.50102006, 0.50164249,\n",
       "       0.51064888, 0.53555042, 0.51209479, 0.48998432, 0.49125324,\n",
       "       0.45109145, 0.50823571, 0.45866839, 0.5601431 , 0.47134838,\n",
       "       0.50591843, 0.47610441, 0.5304595 , 0.49091397, 0.49600156,\n",
       "       0.5386623 , 0.51690279, 0.53900861, 0.48632565, 0.52483274,\n",
       "       0.4966677 , 0.45629265, 0.46376117, 0.55892305, 0.55033236,\n",
       "       0.5477537 , 0.51920054, 0.5318197 , 0.57979904, 0.47268979,\n",
       "       0.49284987, 0.49218699, 0.48180239, 0.48170613, 0.48681393,\n",
       "       0.48846778, 0.5007963 , 0.52891536, 0.52460198, 0.47290287,\n",
       "       0.46495717, 0.40791303, 0.44404427, 0.47554941, 0.46925486,\n",
       "       0.49897748, 0.5193425 , 0.50612524, 0.43870817, 0.48963868,\n",
       "       0.49133371, 0.48802263, 0.57177488, 0.52257345, 0.48760942,\n",
       "       0.45145931, 0.50710304, 0.59180456, 0.5266276 , 0.5312728 ])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicttest = loaded_model.predict(X_)\n",
    "predicttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['predicttest']=pd.DataFrame(predicttest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=df3.drop(['score','features'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>emotion</th>\n",
       "      <th>predicttest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21257</td>\n",
       "      <td>[#matthew, ncould, somebody, shoot, #video, it...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.561639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21258</td>\n",
       "      <td>[really, sucks, typing, mobile, device, always...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.513063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21259</td>\n",
       "      <td>[#afraid, #quiet, ones, ones, actually, #think]</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.491158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21260</td>\n",
       "      <td>[hes, horrible, person, gag, see, people, quote]</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.482655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21261</td>\n",
       "      <td>[fear, usually, need, tim, ferriss, #inspiring...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.507996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>22247</td>\n",
       "      <td>[vs, atlanta, yr, vs, rockies, dbacks, yr, tha...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.451459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>22248</td>\n",
       "      <td>[im, shaking]</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.507103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>22249</td>\n",
       "      <td>[guys, dating, yet, #trans, #nervous, #blowjob...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.591805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>22250</td>\n",
       "      <td>[listening, eurythmicsnme, polish, gothic, met...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.526628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>22251</td>\n",
       "      <td>[claimed, things, could, scare, usnbut, tough,...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.531273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>995 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id                                              tweet emotion  \\\n",
       "0    21257  [#matthew, ncould, somebody, shoot, #video, it...    fear   \n",
       "1    21258  [really, sucks, typing, mobile, device, always...    fear   \n",
       "2    21259    [#afraid, #quiet, ones, ones, actually, #think]    fear   \n",
       "3    21260   [hes, horrible, person, gag, see, people, quote]    fear   \n",
       "4    21261  [fear, usually, need, tim, ferriss, #inspiring...    fear   \n",
       "..     ...                                                ...     ...   \n",
       "990  22247  [vs, atlanta, yr, vs, rockies, dbacks, yr, tha...    fear   \n",
       "991  22248                                      [im, shaking]    fear   \n",
       "992  22249  [guys, dating, yet, #trans, #nervous, #blowjob...    fear   \n",
       "993  22250  [listening, eurythmicsnme, polish, gothic, met...    fear   \n",
       "994  22251  [claimed, things, could, scare, usnbut, tough,...    fear   \n",
       "\n",
       "     predicttest  \n",
       "0       0.561639  \n",
       "1       0.513063  \n",
       "2       0.491158  \n",
       "3       0.482655  \n",
       "4       0.507996  \n",
       "..           ...  \n",
       "990     0.451459  \n",
       "991     0.507103  \n",
       "992     0.591805  \n",
       "993     0.526628  \n",
       "994     0.531273  \n",
       "\n",
       "[995 rows x 4 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
